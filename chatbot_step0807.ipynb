{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가상환경설정\n",
    "    아나콘다 가상환경 설정하기\n",
    "  - conda create -n med_chatbot python=3.9\n",
    "  - conda activate  med_chatbot\n",
    "  - pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 (cpu만 사용할때)\n",
    "  - pip install python-mecab-ko\n",
    "  - pip install sentence-transformers\n",
    "  - pip install pandas\n",
    "  - pip install matplotlib\n",
    "  - pip install numpy==1.26.4 (제일 중요!(numpy가 2.0.1로 깔려있지만, 원활한 함수 사용을 위해 1.26.4로 다운그레이드 해줘야함))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import util\n",
    "from mecab import MeCab\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 데이터 불러와 데이터 프레임 만들기\n",
    " 1. glob으로 해당 폴더 모든 json 파일경로를 list로 불러옴\n",
    " 2. json.load를 이용해 질문, 답변, 의도를 각각 리스트에 모두 담는다\n",
    " 3. 질문, 답변, 의도에 대한 데이터 프레임을 만들고 concat으로 하나의 데이터프레임으로 합친다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(path1, path2):\n",
    "    question_path = path1\n",
    "    answer_path = path2\n",
    "\n",
    "    return glob(question_path + '/*/*.json'), glob(answer_path + '/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data, a_data = all_data('./training/원천데이터/질문/치매', './training/원천데이터/답변/치매') # 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4293, 10553)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 치매 데이터 개수 확인 (질문, 답변)\n",
    "len(q_data), len(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fileName': 'HC-Q-0318081', 'participantsInfo': {'participantID': 'QC088', 'gender': '남성', 'age': '20대', 'occupation': '학생', 'history': False, 'rPlace': '서울/경기/인천'}, 'disease_category': '뇌신경정신질환', 'disease_name': {'kor': '치매', 'eng': 'Dementia'}, 'intention': '약물', 'question': '약물치료를 통해 조부모님의 치매 치료 가능성을 알고 싶어요.', 'entities': [{'id': 0, 'text': '치매', 'entity': '질환명', 'position': 15}], 'num_of_words': 8}\n"
     ]
    }
   ],
   "source": [
    "with open(q_data[1000], 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list = []\n",
    "for i in range(len(q_data)):\n",
    "    with open(q_data[i], 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        q_list.append(json_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(len(q_data)):\n",
    "    with open(a_data[i], 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        sentence = \"\"\n",
    "        for key in json_data['answer']:\n",
    "            sentence += json_data['answer'][key]\n",
    "        a_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_list=[]\n",
    "for i in range(len(q_data)):\n",
    "    with open(q_data[i],'r',encoding='utf-8') as file:\n",
    "        json_data=json.load(file)\n",
    "        sentence = \"\"\n",
    "        sentence += json_data['intention']\n",
    "        i_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 만들기\n",
    "q_df = pd.DataFrame(q_list) # 질문 데이터 프레임\n",
    "a_df = pd.DataFrame(a_list) # 답변 데이터 프레임\n",
    "i_df = pd.DataFrame(i_list) # 의도 데이터 프레임\n",
    "qa_df = pd.concat((q_df, a_df), axis=1) # 질문-의도 데이터 프레임\n",
    "qa_df.columns=['question', 'answer']\n",
    "qi_df = pd.concat((q_df, i_df), axis=1) # 질문-답변 데이터 프레임\n",
    "qi_df.columns=['question', 'intention']\n",
    "df = pd.concat((q_df, i_df, a_df), axis=1) # 질문-의도-답변 데이터 프레임\n",
    "df.columns=['question', 'intention', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일로 저장\n",
    "df.to_csv(\"dementia_qia.csv\", sep=',') #질문, 의도, 답변 포함\n",
    "qa_df.to_csv(\"dementia_qa.csv\", sep=',') #질문, 답변\n",
    "qi_df.to_csv(\"dementia_qi.csv\", sep=',') #질문, 의도 -> 나중에 의도 분류 모델에 활용할수도 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4293"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석 불러오기\n",
    "from mecab import MeCab\n",
    "m = MeCab()\n",
    "\n",
    "# 각 질문마다 형태소 분석을 통해 질문당 몇 개의 단어 토큰이 들어갔는지 count리스트에 담음\n",
    "count = []\n",
    "for q in q_data:\n",
    "    with open(q, 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        count_num = len(m.morphs(json_data[\"question\"]))\n",
    "        count.append(count_num)\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       병원에서 치매 진단을 위해 다양한 검사가 수행됩니다. 치매가 의심될 경우, 환자들은...\n",
       "1       치매 진단은 전문가가 환자의 상황을 조사하고 병력 및 인지 기능 테스트, 신경 심리...\n",
       "2       병원에서 치매의 진단은 단계별로 이루어지며, 다음과 같은 과정을 거칩니다.먼저, 환...\n",
       "3       치매 진단을 위해 여러 가지 검사가 실시됩니다.먼저, 환자의 병력을 청취하고 문진을...\n",
       "4       병원에서는 다양한 진단 절차를 거쳐 치매를 진단합니다.먼저, 치매의 증상을 정확하게...\n",
       "                              ...                        \n",
       "4288    알츠하이머병은 현재까지 치료 옵션이 제한적이기 때문에, 예방이 중요합니다.알츠하이머...\n",
       "4289    알츠하이머병은 뇌의 인지 기능 저하를 일으키는 질병으로, 원인은 다양한 원인의 복합...\n",
       "4290    치매는 노화로 인해 발생하는 인지기능의 점진적인 저하를 의미하며, 다양한 원인에 의...\n",
       "4291    치매는 많은 사람들에게 영향을 미치는 질병입니다. 알츠하이머병은 노인성 치매의 흔한...\n",
       "4292    치매의 원인은 다양한 요인들에 의해 발생할 수 있습니다.치매의 원인 중 하나는 뇌 ...\n",
       "Name: answer, Length: 4293, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df['question']\n",
    "answer = df['answer']\n",
    "intention = df['intention']\n",
    "df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10553"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 질문마다 형태소 분석을 통해 답변당 몇 개의 단어 토큰이 들어갔는지 count1 리스트에 담음\n",
    "\n",
    "count1 = []\n",
    "for a in a_data:\n",
    "    with open(a, 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        sentence = \"\"\n",
    "        for key in json_data[\"answer\"].keys():\n",
    "            sentence += json_data[\"answer\"][key]\n",
    "            count_num = len(m.morphs(sentence))\n",
    "        count1.append(count_num)\n",
    "len(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7. 16. 19. 23. 25. 30.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 질문에 사용된 단어 개수\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(count)\n",
    "point_6 = np.percentile(count, q=[0, 50, 75, 90, 95, 99]) # 상위 0%, 50%, 75%, 90%, 95%, 99% 구간으로 나눠서 분포 그리기\n",
    "print(point_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63.   183.   213.   246.   273.   329.48]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 답변에 사용된 단어 개수 ()\n",
    "\n",
    "plt.hist(count1)\n",
    "point1_6 = np.percentile(count1, q=[0, 50, 75, 90, 95, 99]) # 상위 0%, 50%, 75%, 90%, 95%, 99% 구간으로 나눠서 분포 그리기\n",
    "print(point1_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[^ ?,.!A-Za-z0-9가-힣+]', re.UNICODE)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 한글, 영어, 숫자, 공백, ?!.,을 제외한 나머지 문자 제거\n",
    "korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "\n",
    "# 패턴 컴파일\n",
    "normalizer = re.compile(korean_pattern)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수정 전: 치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?\n",
      "수정 후: 치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?\n",
      "수정 전: 치매를 진단하기 위해 다양한 방법과 검사가 사용됩니다.병원에 가면 먼저 전문의와의 상세한 면담과 신체 진찰이 이루어집니다. 전문의는 인지 기능의 변화를 확인하기 위해 환자의 기억력, 집중력, 사고력 등을 평가합니다. 혈액검사를 통해 혈액 내의 알츠하이머병 위험 인자를 확인할 수 있습니다. 신경심리검사를 통해 인지 영역의 문제가 의심될 경우 치매 여부를 확인하기 위해 혈액검사, 뇌 영상 검사, 핵의학 검사 등을 시행합니다. 아포지질단백질 유전자 검사와 비타민 및 갑상선 기능 검사도 알츠하이머병 조기 진단을 위해 실시될 수 있습니다. 혈액검사에는 알츠하이머병 위험 인자가 있는 경우 아밀로이드 양전자 방출 단층촬영이나 양전자방출단층촬영이 가능합니다. 알츠하이머병의 조기 진단을 위해 아밀로이드와 관련된 유전자 검사와 비타민 A 부족에 따른 뇌 변화를 확인하는 검사도 있습니다. 최근에는 알츠하이머병을 조기에 진단하기 위해 뇌 자기공명영상을 통해 아밀로이드 단백질과 타우 단백질의 이상을 확인할 수 있는 다양한 연구가 진행 중입니다.치매를 정확히 진단하기 위해서는 다양한 검사가 필요하며, 각 검사에 따른 적절한 조치와 치료가 필요합니다.\n",
      "수정 후: 치매를 진단하기 위해 다양한 방법과 검사가 사용됩니다.병원에 가면 먼저 전문의와의 상세한 면담과 신체 진찰이 이루어집니다. 전문의는 인지 기능의 변화를 확인하기 위해 환자의 기억력, 집중력, 사고력 등을 평가합니다. 혈액검사를 통해 혈액 내의 알츠하이머병 위험 인자를 확인할 수 있습니다. 신경심리검사를 통해 인지 영역의 문제가 의심될 경우 치매 여부를 확인하기 위해 혈액검사, 뇌 영상 검사, 핵의학 검사 등을 시행합니다. 아포지질단백질 유전자 검사와 비타민 및 갑상선 기능 검사도 알츠하이머병 조기 진단을 위해 실시될 수 있습니다. 혈액검사에는 알츠하이머병 위험 인자가 있는 경우 아밀로이드 양전자 방출 단층촬영이나 양전자방출단층촬영이 가능합니다. 알츠하이머병의 조기 진단을 위해 아밀로이드와 관련된 유전자 검사와 비타민 A 부족에 따른 뇌 변화를 확인하는 검사도 있습니다. 최근에는 알츠하이머병을 조기에 진단하기 위해 뇌 자기공명영상을 통해 아밀로이드 단백질과 타우 단백질의 이상을 확인할 수 있는 다양한 연구가 진행 중입니다.치매를 정확히 진단하기 위해서는 다양한 검사가 필요하며, 각 검사에 따른 적절한 조치와 치료가 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리 (그런데 이미 불용어 처리된 데이터라 변화가 거의 없음)\n",
    "print(f'수정 전: {question[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", question[20])}')\n",
    "print(f'수정 전: {answer[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", answer[20])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)\n",
    "\n",
    "normalize(question[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['치매',\n",
       " '를',\n",
       " '진단',\n",
       " '하',\n",
       " '기',\n",
       " '위해',\n",
       " '어떤',\n",
       " '인지',\n",
       " '평가',\n",
       " '나',\n",
       " '뇌',\n",
       " '영상',\n",
       " '검사',\n",
       " '가',\n",
       " '사용',\n",
       " '되',\n",
       " '는지',\n",
       " '알려',\n",
       " '주',\n",
       " '세요',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석 돌려보기\n",
    "mecab = MeCab()\n",
    "mecab.morphs(normalize(question[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 전처리를 함수화\n",
    "def clean_text(sentence, mecab):\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'치매 를 진단 하 기 위해 다양 한 방법 과 검사 가 사용 됩니다 . 병원 에 가 면 먼저 전문의 와 의 상세 한 면담 과 신체 진찰 이 이루어집니다 . 전문의 는 인지 기능 의 변화 를 확인 하 기 위해 환자 의 기억력 , 집중력 , 사고력 등 을 평가 합니다 . 혈액 검사 를 통해 혈액 내 의 알츠하이머병 위험 인자 를 확인 할 수 있 습니다 . 신경심리검사 를 통해 인지 영역 의 문제 가 의심 될 경우 치매 여부 를 확인 하 기 위해 혈액 검사 , 뇌 영상 검사 , 핵의학 검사 등 을 시행 합니다 . 아포 지질단백질 유전자 검사 와 비타민 및 갑상선 기능 검사 도 알츠하이머병 조기 진단 을 위해 실시 될 수 있 습니다 . 혈액 검사 에 는 알츠하이머병 위험 인자 가 있 는 경우 아밀로이드 양전자 방출 단층 촬영 이나 양전자 방출 단층 촬영 이 가능 합니다 . 알츠하이머병 의 조기 진단 을 위해 아밀로이드 와 관련 된 유전자 검사 와 비타민 a 부족 에 따른 뇌 변화 를 확인 하 는 검사 도 있 습니다 . 최근 에 는 알츠하이머병 을 조기 에 진단 하 기 위해 뇌 자기공명영상 을 통해 아밀로이드 단백질 과 타우 단백질 의 이상 을 확인 할 수 있 는 다양 한 연구 가 진행 중 입니다 . 치매 를 정확히 진단 하 기 위해서 는 다양 한 검사 가 필요 하 며 , 각 검사 에 따른 적절 한 조치 와 치료 가 필요 합니다 .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글\n",
    "clean_text(question[20], mecab)\n",
    "clean_text(answer[20], mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변을 형태소 분석한 결과를 각각 리스트에 모두 담음\n",
    "questions = [clean_text(sent, mecab) for sent in question.values[:len(question)]]\n",
    "answers = [clean_text(sent, mecab) for sent in answer.values[:len(question)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cdr 에 비해 cdr 외 에 치매 진단 에 다른 검사 방법 이 있 는지 알 고 싶 습니다 .',\n",
       " '치매 검진 을 받 기 위해 어떤 검사 를 받 아야 할까요 ?',\n",
       " '치매 의 검진 을 위해 어떤 종류 의 검사 가 주로 시행 되 나요 ?',\n",
       " '치매 검사 를 어디 에서 받 을 수 있 는지 알 고 싶 어요 .',\n",
       " '치매 검진 을 위해 어느 의사 를 찾아가 야 하 나요 ?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['병원 에서 치매 진단 을 위해 다양 한 검사 가 수행 됩니다 . 치매 가 의심 될 경우 , 환자 들 은 여러 가지 신 경학 적 검사 와 상담 을 받 게 됩니다 . 의사 는 면담 을 통해 환자 의 증상 과 인지 기능 의 저하 정도 를 평가 합니다 . 안저 검사 , 신 경학 적 검사 , 혈액 검사 , 소변 검사 등 이 포함 될 수 있 습니다 . 뇌 영상 검사 에서 는 mri , ct , pet 등 이 사용 됩니다 . 알츠하이머병 진단 에 유용 하 며 , 양전자 방출 단층 촬영 이나 단일 광자 방출 컴퓨터 단층 촬영 도 고려 됩니다 . 이러 한 검사 는 뇌 의 기능 적 평가 에 도움 을 줍니다 . 또한 , 양전자 방출 단층 촬영 과 단일 광자 방출 컴퓨터 단층 촬영 은 알츠하이머병 치료 의 일환 으로 사용 되 기 도 합니다 . 치매 진단 을 위해서 는 환자 의 증상 과 다양 한 신 경학 적 검사 가 필요 하 며 , 정확 한 진단 을 위해 다양 한 검사 가 고려 될 수 있 습니다 .',\n",
       " '치매 진단 은 전문가 가 환자 의 상황 을 조사 하 고 병력 및 인지 기능 테스트 , 신경 심리 검사 등 의 검사 를 통해 수행 됩니다 . 전문가 는 환자 의 증상 , 물리 적 검사 , 신경 심리 검사 등 을 평가 하 여 치매 를 진단 합니다 . 일반 적 으로 신경 심리 검사 를 통해 인지 기능 을 평가 하 고 , 혈액 검사 를 통해 알츠하이머병 의 위험 요인 이 있 는지 확인 할 수 있 습니다 . 뇌 영상 검사 를 통해 뇌 의 이상 여부 를 확인 하 고 , 뇌파 검사 , ct , mri , 단일 광자 방출 컴퓨터 단층 촬영 mri 등 의 추가 검사 를 시행 하 기 도 합니다 . 진단 은 치매 의 원인 을 파악 하 고 , 적절 한 치료 계획 을 수립 하 기 위해 중요 한 과정 입니다 . 따라서 전문가 의 정확 한 검사 와 다양 한 검사 방법 을 통해 환자 의 상황 을 정확히 평가 하 고 , 이 에 맞 는 적절 한 치료 방법 을 선택 하 는 것 이 필요 합니다 .',\n",
       " '병원 에서 치매 의 진단 은 단계 별 로 이루어지 며 , 다음 과 같 은 과정 을 거칩니다 . 먼저 , 환자 를 진찰 하 여 초기 증상 을 확인 합니다 . 이 를 위해 기억력 , 언어 능력 , 주의력 , 문제 해결 능력 , 시공간 인식 능력 등 을 검사 합니다 . 이후 , 인지 기능 의 문제 가 있 는지 확인 하 기 위해 신경 심리 검사 가 실시 됩니다 . 이 를 통해 알츠하이머병 과 같 은 치매 의 유형 을 확인 하 고 , 치료 및 예방 을 위한 적절 한 방법 을 모색 할 수 있 습니다 . 혈액 검사 를 통해 알츠하이머병 의 고위험 군 인지 확인 하 고 , 뇌 영상 검사 를 통해 치매 의 원인 질환 을 확인 할 수 있 습니다 . 또한 핵의학 검사 를 통해 치매 의 원인 을 확인 할 수 있 습니다 . 치매 의 진단 은 단계 별 로 진행 되 며 , 신경 심리 검사 와 혈액 검사 , 뇌 영상 검사 , 핵의학 검사 등 의 다양 한 검사 가 사용 됩니다 . 이러 한 검사 결과 를 종합 하 여 치매 의 정확 한 진단 을 내릴 수 있 습니다 .',\n",
       " '치매 진단 을 위해 여러 가지 검사 가 실시 됩니다 . 먼저 , 환자 의 병력 을 청취 하 고 문진 을 진행 합니다 . 이러 한 진단 과정 은 치매 의 종류 와 발생 원인 을 파악 하 는 데 중요 한 역할 을 합니다 . 다음 으로 신경 심리 검사 를 통해 인지 기능 에 문제 가 있 는지 파악 합니다 . 이 검사 는 기억력 과 다른 인지 영역 의 문제 를 평가 하 는 것 으로 , 치매 와 관련 된 인지 기능 의 변화 를 확인 하 는 데 도움 이 됩니다 . 뇌 영상 검사 도 진단 의 중요 한 도구 입니다 . 뇌 의 구조 적 이상 이나 뇌전증 의 발생 여부 를 확인 하 기 위해 뇌 촬영 이나 자기공명영상 검사 를 사용 할 수 있 습니다 . 또한 , 혈액 검사 와 뇌척수액 검사 등 의 검사 를 통해 치매 와 관련 된 다른 질환 의 가능 성 도 배제 합니다 . 정확 한 치매 진단 을 위해 다양 한 검사 가 사용 되 며 , 이 를 통해 정확 한 원인 파악 과 치료 계획 이 수립 됩니다 .',\n",
       " '병원 에서 는 다양 한 진단 절차 를 거쳐 치매 를 진단 합니다 . 먼저 , 치매 의 증상 을 정확 하 게 판단 하 기 위해 면담 과 신체 진찰 이 진행 됩니다 . 의사 가 면담 을 통해 인지 기능 의 이상 이 있 는지 확인 하 면 신경 심리 검사 를 실시 하 여 다른 인지 영역 의 문제 가 있 는지 알아봅니다 . 신경 심리 검사 에서 알츠하이머병 과 다른 치료법 으로 치료 가능 한 치매 인지 확인 하 기 위해 혈액 검사 , 뇌 영상 검사 ct 또는 mri , 핵의학 검사 , 단일 광자 방출 컴퓨터 단층 촬영 spect , 양전자 방출 단층 촬영 pet 등 의 검사 가 실시 됩니다 . 이외 에 도 뇌척수액 검사 , 일반 혈액 검사 , 뇌파 검사 , 혈당 검사 등 의 추가 검사 가 필요 한 경우 도 있 을 수 있 습니다 . 정확 한 진단 을 위해 이러 한 다양 한 검사 가 사용 됩니다 . 이러 한 검사 들 은 치매 의 원인 을 정확 하 게 파악 하 고 적절 한 치료 를 시작 하 는 데 에 도움 이 됩니다 .']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 모델 만들어보기\n",
    " 1. 질문과 답변 내용을 모두 형태소 분석하여 큰 단어사전을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0 # 빈공간 채워주는 토큰\n",
    "SOS_TOKEN = 1 # 문장의 시작점을 표시하는 토큰\n",
    "EOS_TOKEN = 2 # 문장의 끝을 표시하는 토큰\n",
    "\n",
    "# 단어사전 클래스\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2index = {\n",
    "            '<PAD>': PAD_TOKEN,\n",
    "            '<SOS>': SOS_TOKEN, \n",
    "            '<EOS>': EOS_TOKEN,\n",
    "        }\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_TOKEN: '<PAD>', \n",
    "            SOS_TOKEN: '<SOS>', \n",
    "            EOS_TOKEN: '<EOS>'\n",
    "        }\n",
    "        \n",
    "        self.n_words = 3  # PAD, SOS, EOS 포함\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \n",
    "        # for word in sentence.split(' '):\n",
    "        for word in mecab.morphs(sentence): # 문장을 형태소 분석 함수에 돌리면 각 형태소가 담긴 리스트가 나오기 때문에 바로 반복문으로 하나씩 단어사전에 추가해준다.\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word): # word2index : 단어를 번호로 바꿔주는 딕셔너리, word2count : 해당 단어가 몇번 쓰였는지 횟수를 나타내는 딕셔너리, index2word : 번호를 단어로 바꿔주는 딕셔너리\n",
    "        if word not in self.word2index: # 해당 단어가 단어사전에 없는 경우 번호를 매겨주고, 단어사전에 추가한다.\n",
    "            self.word2index[word] = self.n_words \n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1 \n",
    "        else:\n",
    "            self.word2count[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 질문과 답변을 lang1이란 단어사전 클래스를 만들어서 넣어보기\n",
    "lang1 = WordVocab()\n",
    "for q in question:\n",
    "    lang1.add_sentence(q)\n",
    "for a in answer:\n",
    "    lang1.add_sentence(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " 'CDR': 3,\n",
       " '에': 4,\n",
       " '비해': 5,\n",
       " '외': 6,\n",
       " '치매': 7,\n",
       " '진단': 8,\n",
       " '다른': 9,\n",
       " '검사': 10,\n",
       " '방법': 11,\n",
       " '이': 12,\n",
       " '있': 13,\n",
       " '는지': 14,\n",
       " '알': 15,\n",
       " '고': 16,\n",
       " '싶': 17,\n",
       " '습니다': 18,\n",
       " '.': 19,\n",
       " '검진': 20,\n",
       " '을': 21,\n",
       " '받': 22,\n",
       " '기': 23,\n",
       " '위해': 24,\n",
       " '어떤': 25,\n",
       " '를': 26,\n",
       " '아야': 27,\n",
       " '할까요': 28,\n",
       " '?': 29,\n",
       " '의': 30,\n",
       " '종류': 31,\n",
       " '가': 32,\n",
       " '주로': 33,\n",
       " '시행': 34,\n",
       " '되': 35,\n",
       " '나요': 36,\n",
       " '어디': 37,\n",
       " '에서': 38,\n",
       " '수': 39,\n",
       " '어요': 40,\n",
       " '어느': 41,\n",
       " '의사': 42,\n",
       " '찾아가': 43,\n",
       " '야': 44,\n",
       " '하': 45,\n",
       " '는': 46,\n",
       " '병원': 47,\n",
       " '이나': 48,\n",
       " '의료': 49,\n",
       " '기관': 50,\n",
       " '알려': 51,\n",
       " '주': 52,\n",
       " '세요': 53,\n",
       " '연로': 54,\n",
       " '신데': 55,\n",
       " '최근': 56,\n",
       " '기억력': 57,\n",
       " ',': 58,\n",
       " '언어': 59,\n",
       " '력': 60,\n",
       " '판단력': 61,\n",
       " '떨어지': 62,\n",
       " '셔서': 63,\n",
       " '진료': 64,\n",
       " '는데': 65,\n",
       " '로': 66,\n",
       " '가장': 67,\n",
       " '효과': 68,\n",
       " '적': 69,\n",
       " '인': 70,\n",
       " '은': 71,\n",
       " '무엇': 72,\n",
       " '인가요': 73,\n",
       " '위한': 74,\n",
       " '인지': 75,\n",
       " '평가': 76,\n",
       " '나': 77,\n",
       " '뇌': 78,\n",
       " '영상': 79,\n",
       " '어떻게': 80,\n",
       " '진행': 81,\n",
       " '예방': 82,\n",
       " '가족': 83,\n",
       " '협력': 84,\n",
       " '해야': 85,\n",
       " '사용': 86,\n",
       " '핵심': 87,\n",
       " '포인트': 88,\n",
       " '와': 89,\n",
       " '대해': 90,\n",
       " '주요': 91,\n",
       " '절차': 92,\n",
       " '알츠하이머': 93,\n",
       " '질병': 94,\n",
       " '할': 95,\n",
       " '찾': 96,\n",
       " '노년기': 97,\n",
       " '원인': 98,\n",
       " '으로': 99,\n",
       " '뇌손상': 100,\n",
       " '이외': 101,\n",
       " '요인': 102,\n",
       " '증상': 103,\n",
       " '의심': 104,\n",
       " '될': 105,\n",
       " '은데': 106,\n",
       " '전문의': 107,\n",
       " '얼마나': 108,\n",
       " '자주': 109,\n",
       " '다면': 110,\n",
       " '소요': 111,\n",
       " '시간': 112,\n",
       " '과': 113,\n",
       " '노력': 114,\n",
       " '될까요': 115,\n",
       " '노인': 116,\n",
       " '성': 117,\n",
       " '방문': 118,\n",
       " '항목': 119,\n",
       " '들': 120,\n",
       " '확인': 121,\n",
       " '숫자': 122,\n",
       " '그림': 123,\n",
       " '예시': 124,\n",
       " '결과': 125,\n",
       " '따라야': 126,\n",
       " '전문': 127,\n",
       " '클리닉': 128,\n",
       " '추천': 129,\n",
       " '해': 130,\n",
       " '얼마': 131,\n",
       " '동안': 132,\n",
       " '건가요': 133,\n",
       " '알츠하이머병': 134,\n",
       " '초기': 135,\n",
       " '때': 136,\n",
       " '언제': 137,\n",
       " '쯤': 138,\n",
       " '필요': 139,\n",
       " '한가요': 140,\n",
       " '과정': 141,\n",
       " '거쳐야': 142,\n",
       " '다를까요': 143,\n",
       " '예': 144,\n",
       " '어': 145,\n",
       " '도움': 146,\n",
       " '줄': 147,\n",
       " '을까요': 148,\n",
       " '의심증': 149,\n",
       " '상': 150,\n",
       " '유형': 151,\n",
       " '미리': 152,\n",
       " '으려면': 153,\n",
       " '준비': 154,\n",
       " '특화': 155,\n",
       " '된': 156,\n",
       " '일반': 157,\n",
       " '한지': 158,\n",
       " '곳': 159,\n",
       " '더': 160,\n",
       " '자세': 161,\n",
       " '한': 162,\n",
       " '내용': 163,\n",
       " '알려져': 164,\n",
       " '지만': 165,\n",
       " '그': 166,\n",
       " '도': 167,\n",
       " '만': 168,\n",
       " '발견': 169,\n",
       " '면': 170,\n",
       " '통해': 171,\n",
       " '려면': 172,\n",
       " '위해서': 173,\n",
       " '나타났': 174,\n",
       " '포함': 175,\n",
       " '많': 176,\n",
       " '조기': 177,\n",
       " '필수': 178,\n",
       " '요소': 179,\n",
       " '대한': 180,\n",
       " '정확': 181,\n",
       " '관련': 182,\n",
       " '건강': 183,\n",
       " '보험': 184,\n",
       " '공단': 185,\n",
       " '이루어지': 186,\n",
       " '라는': 187,\n",
       " '정의': 188,\n",
       " '특징': 189,\n",
       " '문제': 190,\n",
       " '해결': 191,\n",
       " '것': 192,\n",
       " '좋': 193,\n",
       " '설명': 194,\n",
       " '실': 195,\n",
       " '나타나': 196,\n",
       " '어머니': 197,\n",
       " '정보': 198,\n",
       " '자료': 199,\n",
       " '적합': 200,\n",
       " '경우': 201,\n",
       " '나오': 202,\n",
       " '시기': 203,\n",
       " '함께': 204,\n",
       " '중요': 205,\n",
       " '이유': 206,\n",
       " '아버님': 207,\n",
       " '일까요': 208,\n",
       " '예약': 209,\n",
       " '단계': 210,\n",
       " '자세히': 211,\n",
       " '나빠': 212,\n",
       " '지': 213,\n",
       " '주의': 214,\n",
       " '사항': 215,\n",
       " '중': 216,\n",
       " '대해서': 217,\n",
       " '으면': 218,\n",
       " '게': 219,\n",
       " '이루': 220,\n",
       " '모시': 221,\n",
       " '같': 222,\n",
       " '흔한': 223,\n",
       " '하나': 224,\n",
       " '주기': 225,\n",
       " '얻': 226,\n",
       " '역할': 227,\n",
       " '그렇': 228,\n",
       " '가족력': 229,\n",
       " '걱정': 230,\n",
       " '기본': 231,\n",
       " '동행': 232,\n",
       " '인간': 233,\n",
       " '수명': 234,\n",
       " '아닌': 235,\n",
       " '건망증': 236,\n",
       " '증세': 237,\n",
       " '심하': 238,\n",
       " '전문가': 239,\n",
       " '서': 240,\n",
       " '거치': 241,\n",
       " '인해': 242,\n",
       " '에게': 243,\n",
       " '파악': 244,\n",
       " '혼동': 245,\n",
       " '않': 246,\n",
       " '도록': 247,\n",
       " '점': 248,\n",
       " '나타날': 249,\n",
       " '여러': 250,\n",
       " '가지': 251,\n",
       " '지표': 252,\n",
       " '상담': 253,\n",
       " '꼭': 254,\n",
       " '정확히': 255,\n",
       " '검': 256,\n",
       " '진': 257,\n",
       " '여부': 258,\n",
       " '문의': 259,\n",
       " '장소': 260,\n",
       " '검색': 261,\n",
       " '이용': 262,\n",
       " '질문': 263,\n",
       " '제공': 264,\n",
       " '특별': 265,\n",
       " '비용': 266,\n",
       " '정도': 267,\n",
       " '셨': 268,\n",
       " '했': 269,\n",
       " '적절': 270,\n",
       " '치료': 271,\n",
       " '의학': 272,\n",
       " '동반': 273,\n",
       " '알려진': 274,\n",
       " '참여': 275,\n",
       " '신속': 276,\n",
       " '담당': 277,\n",
       " '란': 278,\n",
       " '병': 279,\n",
       " '며': 280,\n",
       " '다양': 281,\n",
       " '권장': 282,\n",
       " '시': 283,\n",
       " '의료진': 284,\n",
       " '기기': 285,\n",
       " '장비': 286,\n",
       " '위치': 287,\n",
       " '체크': 288,\n",
       " '확진': 289,\n",
       " '용어': 290,\n",
       " '관계': 291,\n",
       " '목적': 292,\n",
       " '고려': 293,\n",
       " '당뇨병성': 294,\n",
       " '말초': 295,\n",
       " '신경': 296,\n",
       " '병증': 297,\n",
       " '협조': 298,\n",
       " '왜': 299,\n",
       " '대상': 300,\n",
       " '지요': 301,\n",
       " '없': 302,\n",
       " '어도': 303,\n",
       " '궁금': 304,\n",
       " '합니다': 305,\n",
       " '다는': 306,\n",
       " '신뢰': 307,\n",
       " '방식': 308,\n",
       " '도구': 309,\n",
       " '어서': 310,\n",
       " '원리': 311,\n",
       " '참고': 312,\n",
       " '시설': 313,\n",
       " '까요': 314,\n",
       " '실시': 315,\n",
       " '가능': 316,\n",
       " '데': 317,\n",
       " '가까운': 318,\n",
       " '기간': 319,\n",
       " '상태': 320,\n",
       " '겠': 321,\n",
       " '심해진': 322,\n",
       " '일': 323,\n",
       " '없이': 324,\n",
       " '신': 325,\n",
       " '아': 326,\n",
       " '으려': 327,\n",
       " '분야': 328,\n",
       " '해당': 329,\n",
       " '일상': 330,\n",
       " '생활': 331,\n",
       " '걸리': 332,\n",
       " '보': 333,\n",
       " '기억': 334,\n",
       " '좀': 335,\n",
       " '잘': 336,\n",
       " '흔히': 337,\n",
       " '할지': 338,\n",
       " '보인다면': 339,\n",
       " '따르': 340,\n",
       " '써야': 341,\n",
       " '법': 342,\n",
       " '볼': 343,\n",
       " '존재': 344,\n",
       " '한다면': 345,\n",
       " '수행': 346,\n",
       " '흐려져': 347,\n",
       " '식이': 348,\n",
       " '요법': 349,\n",
       " '습관': 350,\n",
       " '인한': 351,\n",
       " '저하': 352,\n",
       " '개선': 353,\n",
       " '체조': 354,\n",
       " '환자': 355,\n",
       " '식': 356,\n",
       " '의해': 357,\n",
       " '영향': 358,\n",
       " '손상': 359,\n",
       " '관리': 360,\n",
       " '미치': 361,\n",
       " '및': 362,\n",
       " '조언': 363,\n",
       " '알코올': 364,\n",
       " '중독': 365,\n",
       " '조절': 366,\n",
       " '상실': 367,\n",
       " '완화': 368,\n",
       " '시키': 369,\n",
       " '식단': 370,\n",
       " '흡인': 371,\n",
       " '폐렴': 372,\n",
       " '위험': 373,\n",
       " '사전': 374,\n",
       " '줄일': 375,\n",
       " '당뇨병': 376,\n",
       " '음식': 377,\n",
       " '제한': 378,\n",
       " '을지': 379,\n",
       " '간단': 380,\n",
       " '운동': 381,\n",
       " '중증': 382,\n",
       " '화': 383,\n",
       " '막': 384,\n",
       " '이점': 385,\n",
       " '(': 386,\n",
       " '영양': 387,\n",
       " ')': 388,\n",
       " '함': 389,\n",
       " '으로써': 390,\n",
       " '깜빡깜빡': 391,\n",
       " '조심': 392,\n",
       " '식습관': 393,\n",
       " '방침': 394,\n",
       " '키': 395,\n",
       " '토식': 396,\n",
       " '자폐증': 397,\n",
       " '부작용': 398,\n",
       " '최소': 399,\n",
       " '향상': 400,\n",
       " '구성': 401,\n",
       " '어야': 402,\n",
       " '먹': 403,\n",
       " '성분': 404,\n",
       " '여': 405,\n",
       " '앓': 406,\n",
       " '영양소': 407,\n",
       " '섭취': 408,\n",
       " '악화': 409,\n",
       " '연관': 410,\n",
       " '가질': 411,\n",
       " '발생': 412,\n",
       " '가져올': 413,\n",
       " '알콜': 414,\n",
       " '음': 415,\n",
       " '주량': 416,\n",
       " '줄이': 417,\n",
       " '할머니': 418,\n",
       " '유의': 419,\n",
       " '상세히': 420,\n",
       " '가진': 421,\n",
       " '돌보': 422,\n",
       " '기여': 423,\n",
       " '거나': 424,\n",
       " '적용': 425,\n",
       " '갖': 426,\n",
       " '유지': 427,\n",
       " '미칠': 428,\n",
       " '시킬': 429,\n",
       " '식사': 430,\n",
       " '불편': 431,\n",
       " '적절히': 432,\n",
       " '특히': 433,\n",
       " '식품': 434,\n",
       " '커피': 435,\n",
       " '마실': 436,\n",
       " '어려움': 437,\n",
       " '깜박': 438,\n",
       " '깜빡하': 439,\n",
       " '피해야': 440,\n",
       " '가져야': 441,\n",
       " '실천': 442,\n",
       " '작': 443,\n",
       " '연구': 444,\n",
       " '케톤': 445,\n",
       " '산증': 446,\n",
       " '걸린': 447,\n",
       " '사람': 448,\n",
       " '방지': 449,\n",
       " '너무': 450,\n",
       " '안': 451,\n",
       " '단': 452,\n",
       " '짠': 453,\n",
       " '마시': 454,\n",
       " '손실': 455,\n",
       " '약물': 456,\n",
       " '많이': 457,\n",
       " '증가': 458,\n",
       " '시도': 459,\n",
       " '높일': 460,\n",
       " '선택': 461,\n",
       " '감소': 462,\n",
       " '기능': 463,\n",
       " '지연': 464,\n",
       " '줄여': 465,\n",
       " '나갈': 466,\n",
       " '바꾸': 467,\n",
       " '특별히': 468,\n",
       " '어떻': 469,\n",
       " '부정': 470,\n",
       " '꾸준히': 471,\n",
       " '주목': 472,\n",
       " '즐겨': 473,\n",
       " '돕': 474,\n",
       " '조치': 475,\n",
       " '취할': 476,\n",
       " '낮출': 477,\n",
       " '의한': 478,\n",
       " '취해야': 479,\n",
       " '로부터': 480,\n",
       " '어르신': 481,\n",
       " '보호': 482,\n",
       " '술': 483,\n",
       " '좋아하': 484,\n",
       " '높': 485,\n",
       " '질': 486,\n",
       " '결핍': 487,\n",
       " '미칠까요': 488,\n",
       " '발병': 489,\n",
       " '낮': 490,\n",
       " '진다는': 491,\n",
       " '말': 492,\n",
       " '근거': 493,\n",
       " '사이': 494,\n",
       " '다고': 495,\n",
       " '친척': 496,\n",
       " '으셨다면': 497,\n",
       " '장기간': 498,\n",
       " '기대': 499,\n",
       " '즐기': 500,\n",
       " '동향': 501,\n",
       " '논문': 502,\n",
       " '늦추': 503,\n",
       " '팁': 504,\n",
       " '계신가요': 505,\n",
       " '후': 506,\n",
       " '재료': 507,\n",
       " '속': 508,\n",
       " '경감': 509,\n",
       " '해석': 510,\n",
       " '낮추': 511,\n",
       " '환경': 512,\n",
       " '가이드': 513,\n",
       " '가져다': 514,\n",
       " '줄까요': 515,\n",
       " '현황': 516,\n",
       " '음주': 517,\n",
       " '질까': 518,\n",
       " '요': 519,\n",
       " '활동': 520,\n",
       " '이것': 521,\n",
       " '악영향': 522,\n",
       " '외상': 523,\n",
       " '혈관': 524,\n",
       " '행동': 525,\n",
       " '통한': 526,\n",
       " '조부모': 527,\n",
       " '님': 528,\n",
       " '미간': 529,\n",
       " '증후군': 530,\n",
       " '강한': 531,\n",
       " '지적': 532,\n",
       " '장애': 533,\n",
       " '젊': 534,\n",
       " '동일': 535,\n",
       " '주변': 536,\n",
       " '권하': 537,\n",
       " '동시': 538,\n",
       " '동등': 539,\n",
       " '서로': 540,\n",
       " '전하': 541,\n",
       " '입증': 542,\n",
       " '개발': 543,\n",
       " '그리고': 544,\n",
       " '약': 545,\n",
       " '복용': 546,\n",
       " '어려워': 547,\n",
       " '올': 548,\n",
       " '억제': 549,\n",
       " '따른': 550,\n",
       " '차이': 551,\n",
       " '완전히': 552,\n",
       " '항우울제': 553,\n",
       " '심리': 554,\n",
       " '실제로': 555,\n",
       " '한데': 556,\n",
       " '겪': 557,\n",
       " '일으키': 558,\n",
       " '으면서': 559,\n",
       " '현재': 560,\n",
       " '작용': 561,\n",
       " '큰': 562,\n",
       " '가격': 563,\n",
       " '명칭': 564,\n",
       " '어려울': 565,\n",
       " '일상생활': 566,\n",
       " '해소': 567,\n",
       " '생각': 568,\n",
       " '안전': 569,\n",
       " '더디': 570,\n",
       " '늦출': 571,\n",
       " '효능': 572,\n",
       " '사례': 573,\n",
       " '제어': 574,\n",
       " '변화': 575,\n",
       " '가져다주': 576,\n",
       " '편': 577,\n",
       " '회복': 578,\n",
       " '해요': 579,\n",
       " '동작': 580,\n",
       " '처방': 581,\n",
       " '극복': 582,\n",
       " '정신': 583,\n",
       " '메커니즘': 584,\n",
       " '이름': 585,\n",
       " '치료법': 586,\n",
       " '속도': 587,\n",
       " '일부': 588,\n",
       " '불': 589,\n",
       " '용량': 590,\n",
       " '높이': 591,\n",
       " '병행': 592,\n",
       " '없앨': 593,\n",
       " '걸렸': 594,\n",
       " '지장': 595,\n",
       " '장단점': 596,\n",
       " '완치': 597,\n",
       " '권고': 598,\n",
       " '성공': 599,\n",
       " '대체': 600,\n",
       " '보조': 601,\n",
       " '심리치료': 602,\n",
       " '의미': 603,\n",
       " '치료제': 604,\n",
       " '은지': 605,\n",
       " '은가요': 606,\n",
       " '었': 607,\n",
       " '힘들': 608,\n",
       " '최신': 609,\n",
       " '기술': 610,\n",
       " '공통': 611,\n",
       " '예후': 612,\n",
       " '아버지': 613,\n",
       " '판단': 614,\n",
       " '막아주': 615,\n",
       " '시작': 616,\n",
       " '듣': 617,\n",
       " '부터': 618,\n",
       " '걷': 619,\n",
       " '영양제': 620,\n",
       " '보충': 621,\n",
       " '제': 622,\n",
       " '조정': 623,\n",
       " '나이': 624,\n",
       " '면서': 625,\n",
       " '나타난다면': 626,\n",
       " '단기': 627,\n",
       " '사회': 628,\n",
       " '고령자': 629,\n",
       " '예방법': 630,\n",
       " '장점': 631,\n",
       " '혈압': 632,\n",
       " '신체': 633,\n",
       " '지켜야': 634,\n",
       " '감정': 635,\n",
       " '규칙': 636,\n",
       " '형': 637,\n",
       " '특정': 638,\n",
       " '노화': 639,\n",
       " '부모': 640,\n",
       " '접종': 641,\n",
       " '걷기': 642,\n",
       " '친구': 643,\n",
       " '교육': 644,\n",
       " '쓰': 645,\n",
       " '심해': 646,\n",
       " '전략': 647,\n",
       " '적극': 648,\n",
       " '유용': 649,\n",
       " '부분': 650,\n",
       " '전': 651,\n",
       " '지침': 652,\n",
       " '새로운': 653,\n",
       " '배움': 654,\n",
       " '정기': 655,\n",
       " '등': 656,\n",
       " '스트레칭': 657,\n",
       " '담배': 658,\n",
       " '만들': 659,\n",
       " '심해질': 660,\n",
       " '예방책': 661,\n",
       " '도와': 662,\n",
       " '한다고': 663,\n",
       " '대책': 664,\n",
       " '리스트': 665,\n",
       " '학습': 666,\n",
       " '주사': 667,\n",
       " '성격': 668,\n",
       " '고령': 669,\n",
       " '층': 670,\n",
       " '수단': 671,\n",
       " '지인': 672,\n",
       " '스트레스': 673,\n",
       " '매일': 674,\n",
       " '일기': 675,\n",
       " '어릴': 676,\n",
       " '실수': 677,\n",
       " '지원': 678,\n",
       " '아이': 679,\n",
       " '쉽': 680,\n",
       " '기울여야': 681,\n",
       " '훈련': 682,\n",
       " '던': 683,\n",
       " '사건': 684,\n",
       " '봐야': 685,\n",
       " '취미': 686,\n",
       " '생길': 687,\n",
       " '우리': 688,\n",
       " '평소': 689,\n",
       " '아도': 690,\n",
       " '사소': 691,\n",
       " '보조제': 692,\n",
       " '안정': 693,\n",
       " '감': 694,\n",
       " '온다는': 695,\n",
       " '맞': 696,\n",
       " '그런': 697,\n",
       " '프로그램': 698,\n",
       " '70': 699,\n",
       " '세': 700,\n",
       " '이상': 701,\n",
       " '노년': 702,\n",
       " '모친': 703,\n",
       " '집': 704,\n",
       " '소개': 705,\n",
       " '뒤': 706,\n",
       " '지남력': 707,\n",
       " '감퇴': 708,\n",
       " '손가락': 709,\n",
       " '자': 710,\n",
       " '관리법': 711,\n",
       " '강도': 712,\n",
       " '빈도': 713,\n",
       " '호전': 714,\n",
       " '분': 715,\n",
       " '가져올까요': 716,\n",
       " '재활': 717,\n",
       " '손': 718,\n",
       " '강화': 719,\n",
       " '사실': 720,\n",
       " '능력': 721,\n",
       " '부족': 722,\n",
       " '질환': 723,\n",
       " '어머님': 724,\n",
       " '유산소': 725,\n",
       " '움직이': 726,\n",
       " '치': 727,\n",
       " '매': 728,\n",
       " '유발': 729,\n",
       " '도울': 730,\n",
       " '삶': 731,\n",
       " '지속': 732,\n",
       " '가벼운': 733,\n",
       " '거동': 734,\n",
       " '보이': 735,\n",
       " '경험': 736,\n",
       " '가져다줄': 737,\n",
       " '지키': 738,\n",
       " '넘': 739,\n",
       " '으신': 740,\n",
       " '촉진': 741,\n",
       " '할아버지': 742,\n",
       " '지력': 743,\n",
       " '테스트': 744,\n",
       " '라는데': 745,\n",
       " '았': 746,\n",
       " '꾸준': 747,\n",
       " '걸릴': 748,\n",
       " '상세': 749,\n",
       " '심한': 750,\n",
       " '갑자기': 751,\n",
       " '소비': 752,\n",
       " '생기': 753,\n",
       " '유전자': 754,\n",
       " '변이': 755,\n",
       " '어떠': 756,\n",
       " '취약': 757,\n",
       " '40': 758,\n",
       " '대': 759,\n",
       " '휴대폰': 760,\n",
       " '퇴행': 761,\n",
       " '과도': 762,\n",
       " '깜빡': 763,\n",
       " '잊어버리': 764,\n",
       " '잊': 765,\n",
       " '다가': 766,\n",
       " '간': 767,\n",
       " '인과': 768,\n",
       " '약해진': 769,\n",
       " '관찰': 770,\n",
       " '관여': 771,\n",
       " '변실금': 772,\n",
       " '유전': 773,\n",
       " '언급': 774,\n",
       " '종종': 775,\n",
       " '더욱': 776,\n",
       " '잦아지': 777,\n",
       " '60': 778,\n",
       " '가설': 779,\n",
       " '식별': 780,\n",
       " '께서': 781,\n",
       " '조사': 782,\n",
       " '연결': 783,\n",
       " '이론': 784,\n",
       " '마저': 785,\n",
       " '일으킬': 786,\n",
       " '상관': 787,\n",
       " '이해': 788,\n",
       " '규명': 789,\n",
       " '못하': 790,\n",
       " '요즘': 791,\n",
       " '섭': 792,\n",
       " '취량': 793,\n",
       " '잃': 794,\n",
       " '버리': 795,\n",
       " '드': 796,\n",
       " '갑작스러운': 797,\n",
       " '잦': 798,\n",
       " '일어나': 799,\n",
       " '심해질까': 800,\n",
       " '피하': 801,\n",
       " '약화': 802,\n",
       " '차이점': 803,\n",
       " '발병률': 804,\n",
       " '밝혀졌': 805,\n",
       " '추가': 806,\n",
       " '실증': 807,\n",
       " '이러': 808,\n",
       " '주된': 809,\n",
       " '마신': 810,\n",
       " '계속': 811,\n",
       " '으셨는데': 812,\n",
       " '특성': 813,\n",
       " '걸까요': 814,\n",
       " '가요': 815,\n",
       " '나라': 816,\n",
       " '몇': 817,\n",
       " '퍼센트': 818,\n",
       " '률': 819,\n",
       " '병인': 820,\n",
       " '구분': 821,\n",
       " '헌팅': 822,\n",
       " '톤': 823,\n",
       " '분류': 824,\n",
       " '관한': 825,\n",
       " '구체': 826,\n",
       " '상황': 827,\n",
       " '아니면': 828,\n",
       " '징후': 829,\n",
       " '라고': 830,\n",
       " '부탁드립니다': 831,\n",
       " '비교': 832,\n",
       " '다른가요': 833,\n",
       " '유사': 834,\n",
       " '다른지': 835,\n",
       " '이런': 836,\n",
       " '기준': 837,\n",
       " '구별': 838,\n",
       " '관해서': 839,\n",
       " '개념': 840,\n",
       " '측면': 841,\n",
       " '과거': 842,\n",
       " '간주': 843,\n",
       " '이란': 844,\n",
       " '신뢰도': 845,\n",
       " '계산': 846,\n",
       " '버거워': 847,\n",
       " '연령': 848,\n",
       " '혈액': 849,\n",
       " '뜻': 850,\n",
       " '기억나': 851,\n",
       " '바': 852,\n",
       " '확률': 853,\n",
       " '그런지': 854,\n",
       " '세포': 855,\n",
       " '얽힌': 856,\n",
       " '가리키': 857,\n",
       " '현상': 858,\n",
       " '생각나': 859,\n",
       " '여야': 860,\n",
       " '스스로': 861,\n",
       " '강하': 862,\n",
       " '남': 863,\n",
       " '물건': 864,\n",
       " '두': 865,\n",
       " '배회': 866,\n",
       " '심각': 867,\n",
       " '파키': 868,\n",
       " '슨': 869,\n",
       " '파킨슨병': 870,\n",
       " '못': 871,\n",
       " '대표': 872,\n",
       " '현관': 873,\n",
       " '비밀': 874,\n",
       " '번호': 875,\n",
       " '어려운': 876,\n",
       " '전혀': 877,\n",
       " '려는': 878,\n",
       " '순간': 879,\n",
       " '힘': 880,\n",
       " '해서': 881,\n",
       " '옷': 882,\n",
       " '입': 883,\n",
       " '밖': 884,\n",
       " '나가': 885,\n",
       " '려고': 886,\n",
       " '는지요': 887,\n",
       " '반복': 888,\n",
       " '밤': 889,\n",
       " '내': 890,\n",
       " '초래': 891,\n",
       " '나타날까요': 892,\n",
       " '통제': 893,\n",
       " '몸': 894,\n",
       " '점점': 895,\n",
       " '설문': 896,\n",
       " '순서': 897,\n",
       " '일시': 898,\n",
       " '또': 899,\n",
       " '자신': 900,\n",
       " '제대로': 901,\n",
       " '빈번': 902,\n",
       " '외출': 903,\n",
       " '폭력': 904,\n",
       " '욕설': 905,\n",
       " '전형': 906,\n",
       " '인구': 907,\n",
       " '급증': 908,\n",
       " '비슷': 909,\n",
       " '깜빡이': 910,\n",
       " '해지지': 911,\n",
       " '따라': 912,\n",
       " '냄': 913,\n",
       " '두드러지': 914,\n",
       " '공통점': 915,\n",
       " '마다': 916,\n",
       " '사고': 917,\n",
       " '는가요': 918,\n",
       " '심화': 919,\n",
       " '가끔': 920,\n",
       " '깜빡거리': 921,\n",
       " '다시': 922,\n",
       " '바람직': 923,\n",
       " '해리': 924,\n",
       " '참': 925,\n",
       " '아빠': 926,\n",
       " '오': 927,\n",
       " '됩니다': 928,\n",
       " '제출': 929,\n",
       " '주장': 930,\n",
       " '자꾸': 931,\n",
       " '신경과': 932,\n",
       " '방향': 933,\n",
       " '누구': 934,\n",
       " '은방': 935,\n",
       " '증과': 936,\n",
       " '간단히': 937,\n",
       " '나타내': 938,\n",
       " '길': 939,\n",
       " '헤매': 940,\n",
       " '연락처': 941,\n",
       " '기분': 942,\n",
       " '년': 943,\n",
       " '이뤄': 944,\n",
       " '일치': 945,\n",
       " '임': 946,\n",
       " '움직임': 947,\n",
       " '심해서': 948,\n",
       " '말씀': 949,\n",
       " '알려야': 950,\n",
       " '잊어버렸': 951,\n",
       " '으로서': 952,\n",
       " '내원': 953,\n",
       " '아서': 954,\n",
       " '노령': 955,\n",
       " '된다면': 956,\n",
       " '확실': 957,\n",
       " '그것': 958,\n",
       " '접근': 959,\n",
       " '현대': 960,\n",
       " '비': 961,\n",
       " '사고력': 962,\n",
       " '대안': 963,\n",
       " '완전': 964,\n",
       " '보통': 965,\n",
       " '지역': 966,\n",
       " '최대한': 967,\n",
       " '자연': 968,\n",
       " '멈추': 969,\n",
       " '성공률': 970,\n",
       " '사이트': 971,\n",
       " '작업': 972,\n",
       " '목표': 973,\n",
       " '포기': 974,\n",
       " '옵션': 975,\n",
       " '전화': 976,\n",
       " '센터': 977,\n",
       " '방치': 978,\n",
       " '예상': 979,\n",
       " '민간': 980,\n",
       " '전부': 981,\n",
       " '치료사': 982,\n",
       " '개인': 983,\n",
       " '가치': 984,\n",
       " '경학': 985,\n",
       " '면담': 986,\n",
       " '안저': 987,\n",
       " '소변': 988,\n",
       " 'MRI': 989,\n",
       " 'CT': 990,\n",
       " 'PET': 991,\n",
       " '양전자': 992,\n",
       " '방출': 993,\n",
       " '단층': 994,\n",
       " '촬영': 995,\n",
       " '단일': 996,\n",
       " '광자': 997,\n",
       " '컴퓨터': 998,\n",
       " '줍니다': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: 치매 환자 를 위한 식단 조절 의 중요 성 과 이점 은 무엇 인가요 ?\n",
      "==============================\n",
      "[단어사전]\n",
      "******************************\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '치매': 3, '환자': 4, '를': 5, '위한': 6, '식단': 7, '조절': 8, '의': 9, '중요': 10, '성': 11, '과': 12, '이점': 13, '은': 14, '무엇': 15, '인가요': 16, '?': 17}\n"
     ]
    }
   ],
   "source": [
    "# 문장 하나 가져와서 단어사전에 추가해서 확인 (테스트용 코드)\n",
    "print(f'원문: {questions[550]}')\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(questions[550])\n",
    "print('==='*10)\n",
    "print('[단어사전]')\n",
    "print('***'*10)\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: [27, 15, 81, 35, 12, 96, 85, 50, 92, 94, 7, 56, 95, 9, 4, 34, 53, 71, 84, 16, 47, 82, 17, 79, 62, 46, 14, 78, 63, 86]\n",
      "Output: [27, 15, 81, 35, 12, 96, 85, 50, 92, 94, 7, 56, 95, 9, 4, 34, 53, 71, 84, 16, 47, 82, 17, 79, 62, 46, 14, 78, 63, 86, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total Length: 50\n"
     ]
    }
   ],
   "source": [
    "# 문장 생성 테스트(나중에 데이터셋 만들때 필요한 과정)\n",
    "\n",
    "max_length = 50 # 문장을 담을 길이를 정하기\n",
    "sentence_length = 30 # 입력할 문장길이\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,)) # 랜덤으로 단어 뽑아오기(3번~100까지)\n",
    "sentence_tokens = sentence_tokens.tolist() \n",
    "print(f'Generated Sentence: {sentence_tokens}')\n",
    "\n",
    "sentence_tokens = sentence_tokens[:(max_length-1)]\n",
    "\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "# 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "sentence_tokens.append(2)\n",
    "\n",
    "for i in range(token_length, max_length-1):\n",
    "    # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "    sentence_tokens.append(0)\n",
    "\n",
    "print(f'Output: {sentence_tokens}')\n",
    "print(f'Total Length: {len(sentence_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 데이터 만들기(여기가 중요!)\n",
    " 1. csv파일에서 질문, 답변 데이터를 읽어와 데이터 프레임을 만듬\n",
    " 2. 질문, 답변내용에서 형태소 분석(mecab 사용)을 이용해 단어사전을 만듬\n",
    " 3. 질문내용은 30개 토큰, 답변 내용은 300개 토큰을 사용 (q_max_length, a_max_length로 조절할 수 있음)\n",
    " 4. 입력 문장을 단어사전을 이용해 숫자로 변환한 후 리스트로 만듬 ex) \"치매에 좋은 운동은 뭐가 있나요?\" -> [치매에, 좋은, 운동, 은, 뭐가, 있나요?] -> [2, 4, 5, 6, 7, 8, 0.....,0](길이가 30인 숫자 리스트로 변환)\n",
    " 5. 답변 문장도 같은 형식으로 길이가 300인 리스트로 변환\n",
    " 6. 의도는 각 단어마다 번호를 매겨 하나의 숫자로 변환\n",
    " 7. 출력은 {\"answer\" : 답변내용문자열, \"intention\" : 의도에 해당하는 숫자} 형태의 딕셔너리로 만들었음\n",
    " 8. 그리고 각 input값과 출력값은 학습시키려면 tensor로 변환시켜줘야하기 때문에, torch.tensor()함수로 감싸서 텐서형태로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, min_length=3, max_length1=50, q_max_length=30, a_max_length=300):\n",
    "        super(TextDataset, self).__init__()\n",
    "        # data_dir = 'data'\n",
    "        \n",
    "        # TOKEN 정의\n",
    "        self.PAD_TOKEN = 0 # Padding 토큰\n",
    "        self.SOS_TOKEN = 1 # SOS 토큰\n",
    "        self.EOS_TOKEN = 2 # EOS 토큰\n",
    "        \n",
    "        self.tagger = MeCab()   # 형태소 분석기\n",
    "        self.max_length1 = max_length # 한 문장의 최대 길이 지정\n",
    "        self.q_max_length = q_max_length # 질문 길이 최대 지정\n",
    "        self.a_max_length = a_max_length # 답변 길이 최대 지정\n",
    "        \n",
    "        # CSV 데이터 로드\n",
    "        # df = pd.read_csv(os.path.join(data_dir, csv_path))\n",
    "        df=pd.read_csv('dementia1.csv') # 질문, 답변, 의도가 저장된 csv파일\n",
    "        # 한글 정규화\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "        \n",
    "        # src: 질의, itn: 의도 tgt: 답변\n",
    "        src_clean = []\n",
    "        itn_clean = [] \n",
    "        tgt_clean = []\n",
    "        \n",
    "        # 단어 사전 생성\n",
    "        wordvocab = WordVocab()\n",
    "        itn_label = {\"검진\" : 0, \"식이, 생활\" : 1, \"약물\" : 2, \"예방\" : 3, \"운동\" : 4, \"원인\" : 5, \"정의\" : 6, \"증상\" : 7, \"진단\" : 8, \"치료\" : 9}\n",
    "        for _, row in df.iterrows():\n",
    "            src = row['question']\n",
    "            itn = row['intention']\n",
    "            tgt = row['answer']\n",
    "            \n",
    "            # 한글 전처리\n",
    "            src = self.clean_text(src)\n",
    "            tgt = self.clean_text(tgt)\n",
    "            \n",
    "            if len(src.split()) > min_length and len(tgt.split()) > min_length:\n",
    "                # 최소 길이를 넘어가는 문장의 단어만 추가\n",
    "                wordvocab.add_sentence(src)\n",
    "                wordvocab.add_sentence(tgt)\n",
    "                src_clean.append(src)          \n",
    "                tgt_clean.append(tgt)\n",
    "            itn_clean.append(itn_label[itn])\n",
    "        \n",
    "        self.srcs = src_clean\n",
    "        self.itns = itn_clean\n",
    "        self.tgts = tgt_clean\n",
    "        self.wordvocab = wordvocab\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        # 정규표현식에 따른 한글 정규화\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        # 한글 정규화\n",
    "        sentence = self.normalize(sentence)\n",
    "        # 형태소 처리\n",
    "        sentence = self.tagger.morphs(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "    \n",
    "    def texts_to_sequences(self, sentence):\n",
    "        # 문장 -> 시퀀스로 변환\n",
    "        return [self.wordvocab.word2index[w] for w in mecab.morphs(sentence)]\n",
    "    \n",
    "        # return [self.wordvocab.word2index[w] for w in sentence.split()]\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens, max_length):\n",
    "        # 문장의 맨 끝 토큰은 제거\n",
    "        sentence_tokens = sentence_tokens[:(max_length-1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "\n",
    "        # 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "\n",
    "        for i in range(token_length, (max_length-1)):\n",
    "            # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "        return sentence_tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터프레임 구조\n",
    "        #------------------------------------\n",
    "        # 1 | 질문내용 | 의도 | 답변내용 \n",
    "        # 2 | 질문내용 | 의도 | 답변내용 \n",
    "        # ...\n",
    "        # 마지막 idx | 질문내용 | 의도 | 답변내용 \n",
    "        #-------------------------------------\n",
    "        # 여기서 한줄씩 뽑아서 학습용 데이터셋을 구성\n",
    "        # inputs = self.srcs[idx]\n",
    "        \n",
    "        # 입력형태 만들기\n",
    "        inputs_sequences = self.texts_to_sequences(self.srcs[idx])\n",
    "        inputs_padded = self.pad_sequence(inputs_sequences, self.q_max_length)\n",
    "        \n",
    "        # outputs = self.tgts[idx]\n",
    "        # 출력 형태 만들기\n",
    "        outputs = {}\n",
    "        outputs_sequences = self.texts_to_sequences(self.tgts[idx])\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences, self.a_max_length)\n",
    "        outputs['answer'] = torch.tensor(outputs_padded)\n",
    "        outputs['intention'] = torch.tensor(self.itns[idx])\n",
    "        \n",
    "        return torch.tensor(inputs_padded), outputs\n",
    "        # return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장의 최대 단어길이를 300로 설정\n",
    "MAX_LENGTH = 300 # 안써도 되는 파라미터\n",
    "Q_MAX_LENGTH = 30 # 질문 문장 최대 30개 토큰 사용\n",
    "A_MAX_LENGTH = 300 # 답변 문장 최대 300개 토큰 사용\n",
    "\n",
    "dataset = TextDataset('dementia1.csv', min_length=3, max_length1=MAX_LENGTH, q_max_length=Q_MAX_LENGTH, a_max_length=A_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3471"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어사전 등록된 단어 개수\n",
    "dataset.wordvocab.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  4,  5,  3,  6,  4,  7,  8,  9,  4, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " {'answer': tensor([21, 22, 23,  9, 24, 25, 26, 27, 11, 28, 29, 30, 20, 23, 28, 31, 32, 33,\n",
       "          34, 35, 36, 37, 38, 39, 40, 41, 42, 11, 43, 44, 24, 45, 46, 30, 20, 47,\n",
       "          48, 49, 24, 50, 35, 51, 52, 53, 54, 55, 51, 56, 57, 58, 59, 60, 20, 61,\n",
       "          11, 34, 40, 41, 42, 11, 34, 62, 11, 34, 63, 11, 64, 13, 65, 32, 66, 14,\n",
       "          19, 20, 67, 68, 11, 22, 48, 69, 34, 70, 34, 71, 64, 13, 72, 30, 20, 73,\n",
       "           9,  4, 74, 75, 76, 34, 77, 78, 79, 80, 81, 82, 83, 78, 84, 79, 80, 85,\n",
       "          86, 30, 20, 87, 27, 11, 48, 67, 51, 55, 42, 59,  4, 88, 24, 89, 20, 90,\n",
       "          34, 77, 78, 79, 80, 53, 82, 83, 78, 84, 79, 80, 37, 73, 91, 51, 92, 93,\n",
       "          72, 94, 95, 85, 60, 20, 23,  9, 24, 96, 48, 35, 51, 52, 53, 26, 27, 40,\n",
       "          41, 42, 11, 28, 97, 75, 76, 34, 98, 27,  9, 24, 25, 26, 27, 11, 28, 86,\n",
       "          32, 66, 14, 19, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       "  'intention': tensor(0)})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋으로 변환한 결과 확인\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 데이터 임의 추출\n",
    "x, y = dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 23,  58,   9,  75,  95,  25, 100,  54,  59, 259,  67,  68,  11,  28,\n",
       "         72,  94,  15, 209, 210, 211,  20,   2,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([30])\n",
      "tensor([ 23,  58,   9,  75,  95,  25, 100,  54,  59, 259,  67,  68,  11,  28,\n",
      "         72,  94,  15, 209, 210, 211,  20,   2,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "y shape: torch.Size([300])\n",
      "{'answer': tensor([ 23,  58,   9,  75,  95,  96,  48,  38,  39,  11,  28,  97,  60,  20,\n",
      "        176,  53, 198,  11,  58,  50,  23,  51, 189, 190,  24,  59,  75,  46,\n",
      "         30,  20,  13,   4, 123, 143, 110, 111,  11,  58,  50,  54,  55,  24,\n",
      "         59,  75,  46,  30,  20, 110, 111,  11,  48,  23,  51,  54,  55,  56,\n",
      "         57,  58,  98,  75,  46, 197,  75,  48, 178,  72,  30,  20,  90,  62,\n",
      "         11,  34,  67,  68,  11,  34, 168,  11,  34, 188,  11,  64,  51,  11,\n",
      "         85, 160,  94, 290,  23,  51, 125,  24, 291,  75,  95,  25,  72,  30,\n",
      "         20, 218, 292, 220,  11,  34, 221,  11,  34, 223,  55,  11,  34, 293,\n",
      "         11,  64,  85,   9,   4,  88,  13,  30,  20,  87,  27,  26,  27,  11,\n",
      "         58,  50,  47,  48,  23,  51, 125,  24, 126,  75,  17, 127,  27,  91,\n",
      "         43, 162,  24, 160, 119,  66,  14,  19,  20,  98,  27,  23,   9,  24,\n",
      "         96,  48,  26,  27,  11,  28,  97,  75,  76,  34,  97,  27,  33,  10,\n",
      "         11,  85, 123,  42,  93, 160,  75, 114,  98,  27,  23,   9,  24, 294,\n",
      "        275,  60,  20,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0]), 'intention': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.shape}')\n",
    "print(x)\n",
    "\n",
    "print(f'y shape: {y[\"answer\"].shape}')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80%의 데이터를 train에 할당합니다.\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나머지 20% 데이터를 test에 할당합니다.\n",
    "test_size = len(dataset) - train_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 랜덤 스플릿으로 분할을 완료합니다.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# 배치사이즈 : 16 (16개씩 묶음)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=16, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 23, 162,  24, 163, 297, 514,  81, 433, 241,  12,   4, 263, 209, 210,\n",
       "         211,  20,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0]),\n",
       " {'answer': tensor([  23,  162,   37,  825,   28,  296,   22,  833,  119,   66,   14,   48,\n",
       "            26,   27,   12,   24,   50,  236,   20,  148,   34,  433,   27,  811,\n",
       "            24,  555,  275,   60,   20,   26,   27, 1780,   43, 1137,   24, 1040,\n",
       "            75,  114,  221,   53, 1426,   34, 2020,   64,  900,   58, 1120,  841,\n",
       "            75,   48,  137,   13,  130,   60,   20,   90,   34,  127,   27,  708,\n",
       "            24, 1072,   75,   48,  137,   13,  570,   19,   20,  198,   42,  419,\n",
       "            37,   67,  419,   24,  888,   75,   17,   23,   51,  116,   24,  832,\n",
       "            66,   14,   19,   20,  422,  405,   42,  217,  433,   99,   53,   47,\n",
       "            43,   51, 1041,   27,   44,   85,   23,  162,    4,  464,   88,   13,\n",
       "            30,   20,   23,  162,   24,   25,  296,   22,  808,   42,   93,  119,\n",
       "            66,   14,   48,  419,   53, 1091,  297,   24,   39,   48,  137,   13,\n",
       "           570,   19,   20,  440,   58,   36,  290,   34,  819,  826,   95,   34,\n",
       "          1037,   34,  393, 1830,   95,   34, 1883, 1884,   64,   24,   50,   67,\n",
       "            58,  820,   75,   17,  820,   24,  304,  119,   66,   14,   19,   20,\n",
       "            90,   34,  433,   27,  811,   24,  555,   75,   48,  137,   85,  130,\n",
       "            60,   20, 1132, 1134,   43, 1530,   28, 1811,   27, 1424,   24, 1040,\n",
       "            75,  114,  900,   58, 1706,   75,   17,   34,  849,   53, 1827,   85,\n",
       "            23,  162,    4,   88,   13,   30,   20,  630,   93,   34,  808,   42,\n",
       "           217,  708,   24,   50, 1972,   24,  555,   75,   17,  261,   42,  433,\n",
       "            24,  307,  713,   48,  137,   85,   23,  162,    4,  130,   60,   20,\n",
       "           296,  297,   22,   87,   27,  419,   36,   24,   50,  825,   51,   67,\n",
       "           419,   24,  820,   75,   17,  151,   24,  307,  713,   48,  178,   88,\n",
       "            13,   30,   20,   23,  162,   24,   25,  405,   42,  217,  433,   99,\n",
       "            53,   47,   43,   51, 1041,   27,   44,   13,   97,   60,   20,   90,\n",
       "            34,   23,  162,   24,   25,  296,   42,  217,  419,   53, 1091,    2]),\n",
       "  'intention': tensor(3)})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치 데이터를 추출합니다.\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  23,   91,    4,   88,   13,   94,   48,  600,   37,  100,  137,   36,\n",
       "            13,   14,  442,  103,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,   91,    4,  100,  138,   51,  600,   13,   72,   94,   15,  209,\n",
       "           210,  211,   20,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [1454,   43,  111,   91,  375,  100,  137,   13,   23,  181, 2245,  475,\n",
       "            24,  490,   75,   48,  178,    4,  492,  243,   42,  628,  103,    2,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,    4, 1690,   35,   28,  708,   24,   50,  244,   24,  272,   24,\n",
       "            66,   14,   15,   16,   17,   18,  174,   20,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [1601, 2266,  190,   23,   91,    4,   72,   94,   48,  600,  375,  100,\n",
       "           137,   36,   13,  243,   42,   54,   16,   17,   18,  174,   20,    2,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 926,   28,   23,  550,   53,  100,  181,   13,   14,   15,  209,  210,\n",
       "           211,   20,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 329,  167,   51,  177,  125,   53,  181,  182,  493,   27,  494,   24,\n",
       "            16,   17,   18,  174,   20,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,  162,   24,   25,  816,   81,  947,   34,  708,   64,   24,  260,\n",
       "          1494,  275,  102,  103,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 329,   43,  181,  182,  708,  574,   13,   14,  360,  100,  137,   24,\n",
       "           398,  399,  210,  447,  446,  174,  103,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,   99,   37,  433,  560,  561,   22,  100,  195,  143,  144,   15,\n",
       "           264,  209,  210,  211,   20,    2,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 329,   99,   24,  112,   93,   75,   48,   21,   13,  173,   54,  209,\n",
       "           210,  211,   20,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,   51,   52,  375,  548,  143, 3060, 3061, 3062, 1003,   13,  259,\n",
       "           299,  737,   48,  137,   13,   65,   94,   15,  443,   60,   20,    2,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,   58,  391,  713,   95,   25,  100, 1132, 1133,   24,  398,   75,\n",
       "           447,   15,  209,  210,  211,   20,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 346,  190,   23,   99,   24,   45,   95,   25,  100,   11,   28,   97,\n",
       "           420,  103,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [  23,   91,    4,   14,  575,  816,   81,  297,  514,    4,  270,  266,\n",
       "            13,   14,   15,  209,  210,  211,   20,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 329,  679,   53,   23,   51,  566,   43, 1747,    4,  263,  264,   16,\n",
       "            17,   18,  174,   20,  329,  679,   51,   98,   27,  347,   43,   52,\n",
       "            24,  209,  210,  211,   20,    2]]),\n",
       " {'answer': tensor([[ 23,  58, 162,  ...,   0,   0,   0],\n",
       "          [ 73,  37,  67,  ...,   0,   0,   0],\n",
       "          [ 23, 162,  24,  ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [ 23,  48,  26,  ...,   0,   0,   0],\n",
       "          [ 23,  48, 316,  ...,   0,   0,   0],\n",
       "          [ 23,  35, 256,  ...,   0,   0,   0]]),\n",
       "  'intention': tensor([2, 9, 2, 4, 2, 5, 5, 3, 4, 0, 0, 7, 1, 0, 9, 6])})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 30]), torch.Size([16, 300]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: (batch_size, sequence_length)\n",
    "x.shape, y[\"answer\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 만들기\n",
    " - 기본적으로 seq-to-seq 는 encoder - decoder 구조로 되어있음\n",
    " - 하나의 단어를 임베딩을 통해 벡터로 만들어줌\n",
    " - 단어사전의 있는 모든 단어를 임베딩을 해줌\n",
    " - 배치사이즈가 16개이므로, 질문입력 사이즈는(16x30), 답변 입력사이즈는(16x300)\n",
    " - 여기에 임베딩 차원이 64이므로(단어 하나를 64개의 무언가로 표현) 각각 인코더를 통과하면 (16x30x64),(16x300x64)의 사이즈가 된다.\n",
    " - gru의 히든 레이어 사이즈가 32이므로, 임베딩을 통과한 데이터가 64에서 32로 줄어듬 (16x30x32), (16x300x32)\n",
    " - -------여기부터는 더 공부해가지고 올게요--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 단어 사전의 개수 지정\n",
    "        self.num_vocabs = num_vocabs\n",
    "        # 임베딩 레이어 정의 (number of vocabs, embedding dimension)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # GRU (embedding dimension)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# Embedding Layer의 입/출력 shape에 대한 이해\n",
    "\n",
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "# x의 shape을 변경합니다.\n",
    "# (batch_size, sequence_length) => (sequence_length, batch_size)\n",
    "embedded = embedding(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(embedded.shape)\n",
    "# input:  (sequence_length, batch_size)\n",
    "# output: (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "embedded = embedded.permute(1, 0, 2)\n",
    "print(embedded.shape)\n",
    "# (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32   \n",
    "\n",
    "gru = nn.GRU(embedding_dim,      # embedding 차원\n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False)\n",
    "\n",
    "# input       : (sequence_length, batch_size, embedding_dim)\n",
    "# h0          : (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "o, h = gru(embedded, None)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabs: 3471\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "print(f'number of vocabs: {NUM_VOCABS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 정의\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "# Encoder에 x 통과 후 output, hidden_size 의 shape 확인\n",
    "# input(x)    : (batch_size, sequence_length)\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 단어사전 개수\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # 최종 출력은 단어사전의 개수\n",
    "        self.fc = nn.Linear(hidden_size, num_vocabs)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0) # (1, batch_size) 로 변환\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        output = self.fc(output.squeeze(0)) # (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding Layer의 입/출력 shape\n",
    "x = torch.abs(torch.randn(size=(1, 16)).long())\n",
    "print(x)\n",
    "x.shape\n",
    "# batch_size = 16 이라 가정했을 때,\n",
    "# (1, batch_size)\n",
    "# 여기서 batch_size => (1, batch_size) 로 shape 변환을 선행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    " #GRU Layer의 입/출력 shape에 대한 이해\n",
    "hidden_size = 32\n",
    "\n",
    "gru = nn.GRU(embedding_dim, \n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False, \n",
    "             batch_first=False, # batch_first=False로 지정\n",
    "            )\n",
    "\n",
    "o, h = gru(embedded)\n",
    "\n",
    "print(o.shape)\n",
    "# output shape: (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state shape: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([16, 3471])\n"
     ]
    }
   ],
   "source": [
    "# 최종 출력층(FC) shape에 대한 이해\n",
    "fc = nn.Linear(32, NUM_VOCABS) # 출력은 단어사전의 개수로 가정\n",
    "\n",
    "output = fc(o[0])\n",
    "\n",
    "print(o[0].shape)\n",
    "print(output.shape)\n",
    "# input : (batch_size, output from GRU)\n",
    "# output: (batch_size, output dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더 -> 디코더 입출력 shape\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape, h.shape)\n",
    "# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))\n",
    "# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***************\n",
    "x = torch.abs(torch.full(size=(16,), fill_value=SOS_TOKEN, dtype=torch.long))\n",
    "print(x)\n",
    "x.shape\n",
    "\n",
    "# batch_size = 16 이라 가정(16개의 SOS 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3471]), torch.Size([1, 16, 32]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output, decoder_hidden = decoder(x, h)\n",
    "decoder_output.shape, decoder_hidden.shape\n",
    "# (batch_size, num_vocabs), (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        # inputs : (batch_size, sequence_length)\n",
    "        # outputs: (batch_size, sequence_length)\n",
    "        \n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "        \n",
    "        # 리턴할 예측된 outputs를 저장할 임시 변수\n",
    "        # (sequence_length, batch_size, num_vocabs)\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "        \n",
    "        # 인코더에 입력 데이터 주입, encoder_output은 버리고 hidden_state 만 살립니다. \n",
    "        # 여기서 hidden_state가 디코더에 주입할 context vector 입니다.\n",
    "        # (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # (batch_size) shape의 SOS TOKEN으로 채워진 디코더 입력 생성********************\n",
    "        decoder_input = torch.full((batch_size,), SOS_TOKEN, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 순회하면서 출력 단어를 생성합니다.\n",
    "        # 0번째는 SOS TOKEN이 위치하므로, 1번째 인덱스부터 순회합니다.\n",
    "        for t in range(0, output_length):\n",
    "            # decoder_input : 디코더 입력 (batch_size) 형태의 SOS TOKEN로 채워진 입력\n",
    "            # decoder_output: (batch_size, num_vocabs)\n",
    "            # decoder_hidden: (Bidirectional(1) x number of layers(1), batch_size, hidden_size), context vector와 동일 shape\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # t번째 단어에 디코더의 output 저장\n",
    "            predicted_outputs[t] = decoder_output\n",
    "            \n",
    "            # teacher forcing 적용 여부 확률로 결정\n",
    "            # teacher forcing 이란: 정답치를 다음 RNN Cell의 입력으로 넣어주는 경우. 수렴속도가 빠를 수 있으나, 불안정할 수 있음\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # top1 단어 토큰 예측\n",
    "            top1 = decoder_output.argmax(1) \n",
    "            \n",
    "            # teacher forcing 인 경우 ground truth 값을, 그렇지 않은 경우, 예측 값을 다음 input으로 지정\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "        \n",
    "        return predicted_outputs.permute(1, 0, 2) # (batch_size, sequence_length, num_vocabs)로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq 입출력 확인\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Seq2Seq 정의\n",
    "seq2seq = Seq2Seq(encoder, decoder, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "# print(x.shape, y.shape)\n",
    "# (batch_size, sequence_length), (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  23, 1047,  100,  615,    4,  177,   75,   48,  330,  245,  103,  602,\n",
       "           43,  181,   75,  114,  265,  399,  210,  211,   20,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,   91,   58,   25,  100,   91,   12,   13,  115,   42,   93,   72,\n",
       "           94,  140,  103,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [1860,   28, 2398,    4, 2157,   48, 1634,   24, 2835,   48,   52,   13,\n",
       "           14,   24,  413,   34,  100,  207,  208,   22,    9,   11,   58,   45,\n",
       "           24,   66,   14,  442,  103,    2],\n",
       "        [ 600,   91,   51,  243,   58,  609,   95,   25,  601,   12,   13,   10,\n",
       "           33,   28,   14,  140,  103,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [2211,  687,   28,   23,    4, 1675,   46,   94,   48,  318,  125,   36,\n",
       "           37,  244,  245,  103,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,   35,   36,   13,  354,  453,   48,  318,   27,   52,   36,   24,\n",
       "          209,  210,  211,   20,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,  143,  389,  151,  301,   24,  241,   75,   95,  163, 1750,   13,\n",
       "           43,  297,  514,   37,  100,  137,   36,   13,   14,  442,  103,    2,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 607,   24, 1054,   95,   25,  100,  816,   81,  708,   24,  398,   75,\n",
       "          447,   15,  209,  210,  211,   20,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,  162,   24,   25, 2246,  575,  858,  119,   66,   14,   48,  499,\n",
       "          643,   13,   14,  442,  103,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 329,  330,   24,  490,  713,   95,   25,   72,   94,   48,  600,    4,\n",
       "           48,  100,  243,   28,   14,   15,  209,  210,  211,   20,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 329,    9,   24,   45,   24,   66,   14,   48,   21,   81,   21,   13,\n",
       "           14,  442,  103,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,   99,   37,  100,  138,   51,   11,   58,   45,   24,   66,   14,\n",
       "           15,  209,  210,  211,   20,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  73,  162,   24,   25,  100,  138,   51,  708,   13,   88,   13,  367,\n",
       "          103,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 715,  289,   23,   11,   48,  173,   22,   45,  101,   75,  140,  103,\n",
       "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  23,  143,  436,  880,   24, 1635,  290, 2840,   17, 1045, 2980,  246,\n",
       "           48,  137,   13, 1938,   94,  199,  100,   52,   13,  334,   66,   14,\n",
       "           15,  209,  210,  211,   20,    2],\n",
       "        [  23,  143,  436,   52,   13,  472,   95, 1058,    4,  162,   75,  599,\n",
       "          241,  119,   66,   14,   48,   12,   13,   14,   15,  209,  210,  211,\n",
       "           20,    2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': tensor([[ 23,  48, 804,  ...,   0,   0,   0],\n",
       "         [ 23,  48, 346,  ...,   0,   0,   0],\n",
       "         [ 23,  35, 256,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 23,  48, 316,  ...,   0,   0,   0],\n",
       "         [ 23,  35,  28,  ...,   0,   0,   0],\n",
       "         [ 23,  35,  58,  ...,   0,   0,   0]]),\n",
       " 'intention': tensor([6, 9, 8, 2, 5, 7, 1, 3, 3, 2, 8, 0, 4, 9, 7, 7])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "output = seq2seq(x, y['answer'])\n",
    "# print(output.shape)\n",
    "# (batch_size, sequence_length, num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vocabs: 3471\n",
      "======================\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(3471, 256)\n",
      "    (gru): GRU(256, 512)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3471, 256)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(256, 512)\n",
      "    (fc): Linear(in_features=512, out_features=3471, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDIMG_DIM = 256\n",
    "\n",
    "print(f'num_vocabs: {NUM_VOCABS}\\n======================')\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "# Seq2Seq 생성\n",
    "# encoder, decoder를 device 모두 지정\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(3471, 256)\n",
       "  (gru): GRU(256, 512)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 및 모델 튜닝\n",
    " - 학습시키면서 성능 향상을 위해 하이퍼 파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련에 적용할 하이퍼파라미터 설정\n",
    "\n",
    "LR = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "es = EarlyStopping(patience=5, \n",
    "                   delta=0.001, \n",
    "                   mode='min', \n",
    "                   verbose=True\n",
    "                  )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=2,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수 정의\n",
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y['answer'].to(device) # tensor로만 학습이 되기때문에, 딕셔너리안에 있는 답변에 해당하는 값을 가져와야함\n",
    "        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # 초기화\n",
    "        \n",
    "        # output: (batch_size, sequence_length, num_vocabs)\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "        \n",
    "        # 1번 index 부터 슬라이싱한 이유는 0번 index가 SOS TOKEN 이기 때문\n",
    "        # (batch_size*sequence_length, num_vocabs) 로 변경\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        \n",
    "        # (batch_size*sequence_length) 로 변경\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation 함수 정의\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y['answer'].to(device)\n",
    "            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)\n",
    "            output = model(x, y)\n",
    "            output_dim = output.size(2)\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            eval_loss += loss.item() * x.size(0)\n",
    "            \n",
    "    return eval_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 샘플링 후 결과 추론\n",
    "def sequence_to_sentence(sequences, index2word):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "\n",
    "        word = index2word[p]\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence를 다시 문장으로 바꾸어 문장 형식으로 출력하기 위한 함수\n",
    "\n",
    "def random_evaluation(model, dataset, index2word, device, n=10):\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)      # Shuffle\n",
    "    sampled_indices = indices[:n]   # Sampling N indices\n",
    "    \n",
    "    # 샘플링한 데이터를 기반으로 DataLoader 생성\n",
    "    sampler = SubsetRandomSampler(sampled_indices)\n",
    "    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in sampled_dataloader:\n",
    "            x, y = x.to(device), y['answer'].to(device)      \n",
    "            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  \n",
    "            output = model(x, y, teacher_forcing_ratio=0)\n",
    "            # output: (number of samples, sequence_length, num_vocabs)\n",
    "            \n",
    "            preds = output.detach().cpu().numpy()\n",
    "            x = x.detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(n):\n",
    "                print(f'질문   : {sequence_to_sentence(x[i], index2word)}')\n",
    "                print(f'답변   : {sequence_to_sentence(y[i], index2word)}')\n",
    "                print(f'예측답변: {sequence_to_sentence(preds[i].argmax(1), index2word)}')\n",
    "                print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 45.9442, val_loss: 37.2873\n",
      "[EarlyStopping] (Update) Best Score: 45.94422\n",
      "[EarlyStopping] (Update) Best Score: 35.74989\n",
      "[EarlyStopping] (Update) Best Score: 33.07088\n",
      "[EarlyStopping] (Update) Best Score: 31.55208\n",
      "[EarlyStopping] (Update) Best Score: 30.73681\n"
     ]
    }
   ],
   "source": [
    "#  훈련 시작\n",
    "NUM_EPOCHS = 5\n",
    "STATEDICT_PATH = 'seq2seq-chatbot-kor.pt'\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    \n",
    "    val_loss = evaluate(model, test_loader, loss_fn, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), STATEDICT_PATH)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early Stop\n",
    "    es(loss)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "                   \n",
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "torch.save(model.state_dict(), f'seq2seq-chatbot-kor-{best_loss:.4f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 입력받아 답을 출력하는 함수\n",
    "\n",
    "def predict(model, sentence, index2word, device, n=10):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tokens = dataset.texts_to_sequences(dataset.clean_text(sentence))\n",
    "        input_padded = dataset.pad_sequence(input_tokens, dataset.q_max_length)\n",
    "    \n",
    "        # 입력 데이터를 텐서로 변환\n",
    "        input_tensor = torch.tensor(input_padded).unsqueeze(0).to(device)  # 배치 차원을 추가하고 텐서로 변환\n",
    "        output_tensor =  torch.tensor([0 for i in range(300)]).unsqueeze(0).to(device)\n",
    "        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  \n",
    "        output = model(input_tensor, output_tensor, teacher_forcing_ratio=0)\n",
    "        # output: (number of samples, sequence_length, num_vocabs)\n",
    "        \n",
    "        # preds = output.detach().cpu().numpy()\n",
    "        # x = x.detach().cpu().numpy()\n",
    "        # y = y.detach().cpu().numpy()\n",
    "        \n",
    "        output_tokens = output.detach().squeeze(0).cpu().numpy()  # 배치 차원을 제거하고 넘파이 배열로 변환\n",
    "        predicted_tokens = np.argmax(output_tokens, axis=1)  # 각 시퀀스의 최대 확률 토큰을 선택\n",
    "        \n",
    "        response_sentence = sequence_to_sentence(predicted_tokens, index2word)  # 토큰 시퀀스를 문자열로 변환\n",
    "        \n",
    "        return response_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 응답: 치매 환자 에게 운동 은 다양 한 이점 을 제공 합니다 . 치매 환자 에게 는 운동 은 인지 기능 을 개선 하 고 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 됩니다 . 치매 환자 에게 는 운동 운동 은 신체 운동 , 인지 기능 저하 와 같 은 운동 을 을 하 는 데 도움 이 됩니다 . 치매 환자 에게 는 운동 운동 은 신체 운동 , 인지 기능 저하 와 의 위험 을 감소 시키 는 데 도움 이 됩니다 . 치매 환자 에게 는 운동 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 에게 는 운동 운동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 데 도움 이 될 수 있 습니다 .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"치매의 증상 중 하나로 현관 비밀 번호 생각이 안 나는 것이 포함되는 것이 있는지 알려주세요\"\n",
    "response = predict(model, sentence, dataset.wordvocab.index2word, device)\n",
    "print(f'챗봇 응답: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([5017, 256]) from checkpoint, the shape in current model is torch.Size([3471, 256]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([5017, 256]) from checkpoint, the shape in current model is torch.Size([3471, 256]).\n\tsize mismatch for decoder.fc.weight: copying a param with shape torch.Size([5017, 512]) from checkpoint, the shape in current model is torch.Size([3471, 512]).\n\tsize mismatch for decoder.fc.bias: copying a param with shape torch.Size([5017]) from checkpoint, the shape in current model is torch.Size([3471]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m STATEDICT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2seq-chatbot-kor11.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTATEDICT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m random_evaluation(model, test_dataset, dataset\u001b[38;5;241m.\u001b[39mwordvocab\u001b[38;5;241m.\u001b[39mindex2word, device)\n",
      "File \u001b[1;32mc:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([5017, 256]) from checkpoint, the shape in current model is torch.Size([3471, 256]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([5017, 256]) from checkpoint, the shape in current model is torch.Size([3471, 256]).\n\tsize mismatch for decoder.fc.weight: copying a param with shape torch.Size([5017, 512]) from checkpoint, the shape in current model is torch.Size([3471, 512]).\n\tsize mismatch for decoder.fc.bias: copying a param with shape torch.Size([5017]) from checkpoint, the shape in current model is torch.Size([3471])."
     ]
    }
   ],
   "source": [
    "STATEDICT_PATH = 'seq2seq-chatbot-kor11.pt'\n",
    "model.load_state_dict(torch.load(STATEDICT_PATH, map_location=device))\n",
    "random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
