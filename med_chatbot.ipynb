{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가상환경설정\n",
    "    아나콘다 가상환경 설정하기\n",
    "  - conda create -n med_chatbot python=3.9\n",
    "  - conda activate  med_chatbot\n",
    "  - pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 (cpu만 사용할때)\n",
    "  - pip install python-mecab-ko\n",
    "  - pip install sentence-transformers\n",
    "  - pip install pandas\n",
    "  - pip install matplotlib\n",
    "  - pip install numpy==1.26.4 (제일 중요!(numpy가 2.0.1로 깔려있지만, 원활한 함수 사용을 위해 1.26.4로 다운그레이드 해줘야함))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import util\n",
    "from mecab import MeCab\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json파일에서 필요 데이터 추출하고, 데이터프레임 만들기, csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(path1, path2):\n",
    "    question_path = path1\n",
    "    answer_path = path2\n",
    "\n",
    "    return glob(question_path + '/*/*.json'), glob(answer_path + '/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data, a_data = all_data('./training/원천데이터/질문/치매', './training/원천데이터/답변/치매')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4293, 10553)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 치매 데이터\n",
    "len(q_data), len(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fileName': 'HC-Q-0318081', 'participantsInfo': {'participantID': 'QC088', 'gender': '남성', 'age': '20대', 'occupation': '학생', 'history': False, 'rPlace': '서울/경기/인천'}, 'disease_category': '뇌신경정신질환', 'disease_name': {'kor': '치매', 'eng': 'Dementia'}, 'intention': '약물', 'question': '약물치료를 통해 조부모님의 치매 치료 가능성을 알고 싶어요.', 'entities': [{'id': 0, 'text': '치매', 'entity': '질환명', 'position': 15}], 'num_of_words': 8}\n"
     ]
    }
   ],
   "source": [
    "with open(q_data[1000], 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list = []\n",
    "for i in range(len(q_data)):\n",
    "    with open(q_data[i], 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        q_list.append(json_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(len(q_data)):\n",
    "    with open(a_data[i], 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        sentence = \"\"\n",
    "        for key in json_data['answer']:\n",
    "            sentence += json_data['answer'][key]\n",
    "        a_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_list=[]\n",
    "for i in range(len(q_data)):\n",
    "    with open(q_data[i],'r',encoding='utf-8') as file:\n",
    "        json_data=json.load(file)\n",
    "        sentence = \"\"\n",
    "        sentence += json_data['intention']\n",
    "        i_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.DataFrame(q_list)\n",
    "a_df = pd.DataFrame(a_list)\n",
    "i_df = pd.DataFrame(i_list)\n",
    "df = pd.concat((q_df, i_df, a_df), axis=1)\n",
    "df.columns=['question', 'intention', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dementia1.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 질문당, 몇 개의 단어가 사용 되었는지 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4293"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mecab import MeCab\n",
    "m = MeCab()\n",
    "count = []\n",
    "for question1 in q_data:\n",
    "    with open(question1, 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        count_num = len(m.morphs(json_data[\"question\"]))\n",
    "        count.append(count_num)\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       병원에서 치매 진단을 위해 다양한 검사가 수행됩니다. 치매가 의심될 경우, 환자들은...\n",
       "1       치매 진단은 전문가가 환자의 상황을 조사하고 병력 및 인지 기능 테스트, 신경 심리...\n",
       "2       병원에서 치매의 진단은 단계별로 이루어지며, 다음과 같은 과정을 거칩니다.먼저, 환...\n",
       "3       치매 진단을 위해 여러 가지 검사가 실시됩니다.먼저, 환자의 병력을 청취하고 문진을...\n",
       "4       병원에서는 다양한 진단 절차를 거쳐 치매를 진단합니다.먼저, 치매의 증상을 정확하게...\n",
       "                              ...                        \n",
       "4288    알츠하이머병은 현재까지 치료 옵션이 제한적이기 때문에, 예방이 중요합니다.알츠하이머...\n",
       "4289    알츠하이머병은 뇌의 인지 기능 저하를 일으키는 질병으로, 원인은 다양한 원인의 복합...\n",
       "4290    치매는 노화로 인해 발생하는 인지기능의 점진적인 저하를 의미하며, 다양한 원인에 의...\n",
       "4291    치매는 많은 사람들에게 영향을 미치는 질병입니다. 알츠하이머병은 노인성 치매의 흔한...\n",
       "4292    치매의 원인은 다양한 요인들에 의해 발생할 수 있습니다.치매의 원인 중 하나는 뇌 ...\n",
       "Name: answer, Length: 4293, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df['question']\n",
    "answer = df['answer']\n",
    "df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7. 16. 19. 23. 25. 30.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(count)\n",
    "point_6 = np.percentile(count, q=[0, 50, 75, 90, 95, 99]) # 상위 0%, 50%, 75%, 90%, 95%, 99% 구간으로 나눠서 분포 그리기\n",
    "print(point_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[^ ?,.!A-Za-z0-9가-힣+]', re.UNICODE)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 한글, 영어, 숫자, 공백, ?!.,을 제외한 나머지 문자 제거\n",
    "korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "\n",
    "# 패턴 컴파일\n",
    "normalizer = re.compile(korean_pattern)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수정 전: 치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?\n",
      "수정 후: 치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?\n",
      "수정 전: 치매를 진단하기 위해 다양한 방법과 검사가 사용됩니다.병원에 가면 먼저 전문의와의 상세한 면담과 신체 진찰이 이루어집니다. 전문의는 인지 기능의 변화를 확인하기 위해 환자의 기억력, 집중력, 사고력 등을 평가합니다. 혈액검사를 통해 혈액 내의 알츠하이머병 위험 인자를 확인할 수 있습니다. 신경심리검사를 통해 인지 영역의 문제가 의심될 경우 치매 여부를 확인하기 위해 혈액검사, 뇌 영상 검사, 핵의학 검사 등을 시행합니다. 아포지질단백질 유전자 검사와 비타민 및 갑상선 기능 검사도 알츠하이머병 조기 진단을 위해 실시될 수 있습니다. 혈액검사에는 알츠하이머병 위험 인자가 있는 경우 아밀로이드 양전자 방출 단층촬영이나 양전자방출단층촬영이 가능합니다. 알츠하이머병의 조기 진단을 위해 아밀로이드와 관련된 유전자 검사와 비타민 A 부족에 따른 뇌 변화를 확인하는 검사도 있습니다. 최근에는 알츠하이머병을 조기에 진단하기 위해 뇌 자기공명영상을 통해 아밀로이드 단백질과 타우 단백질의 이상을 확인할 수 있는 다양한 연구가 진행 중입니다.치매를 정확히 진단하기 위해서는 다양한 검사가 필요하며, 각 검사에 따른 적절한 조치와 치료가 필요합니다.\n",
      "수정 후: 치매를 진단하기 위해 다양한 방법과 검사가 사용됩니다.병원에 가면 먼저 전문의와의 상세한 면담과 신체 진찰이 이루어집니다. 전문의는 인지 기능의 변화를 확인하기 위해 환자의 기억력, 집중력, 사고력 등을 평가합니다. 혈액검사를 통해 혈액 내의 알츠하이머병 위험 인자를 확인할 수 있습니다. 신경심리검사를 통해 인지 영역의 문제가 의심될 경우 치매 여부를 확인하기 위해 혈액검사, 뇌 영상 검사, 핵의학 검사 등을 시행합니다. 아포지질단백질 유전자 검사와 비타민 및 갑상선 기능 검사도 알츠하이머병 조기 진단을 위해 실시될 수 있습니다. 혈액검사에는 알츠하이머병 위험 인자가 있는 경우 아밀로이드 양전자 방출 단층촬영이나 양전자방출단층촬영이 가능합니다. 알츠하이머병의 조기 진단을 위해 아밀로이드와 관련된 유전자 검사와 비타민 A 부족에 따른 뇌 변화를 확인하는 검사도 있습니다. 최근에는 알츠하이머병을 조기에 진단하기 위해 뇌 자기공명영상을 통해 아밀로이드 단백질과 타우 단백질의 이상을 확인할 수 있는 다양한 연구가 진행 중입니다.치매를 정확히 진단하기 위해서는 다양한 검사가 필요하며, 각 검사에 따른 적절한 조치와 치료가 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(f'수정 전: {question[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", question[20])}')\n",
    "print(f'수정 전: {answer[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", answer[20])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'치매 검진을 받는데 소요되는 시간과 노력은 얼마나 될까요?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)\n",
    "\n",
    "normalize(question[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['치매',\n",
       " '를',\n",
       " '진단',\n",
       " '하',\n",
       " '기',\n",
       " '위해',\n",
       " '어떤',\n",
       " '인지',\n",
       " '평가',\n",
       " '나',\n",
       " '뇌',\n",
       " '영상',\n",
       " '검사',\n",
       " '가',\n",
       " '사용',\n",
       " '되',\n",
       " '는지',\n",
       " '알려',\n",
       " '주',\n",
       " '세요',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = MeCab()\n",
    "mecab.morphs(normalize(question[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 전처리를 함수화\n",
    "def clean_text(sentence, mecab):\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'치매 를 진단 하 기 위해 다양 한 방법 과 검사 가 사용 됩니다 . 병원 에 가 면 먼저 전문의 와 의 상세 한 면담 과 신체 진찰 이 이루어집니다 . 전문의 는 인지 기능 의 변화 를 확인 하 기 위해 환자 의 기억력 , 집중력 , 사고력 등 을 평가 합니다 . 혈액 검사 를 통해 혈액 내 의 알츠하이머병 위험 인자 를 확인 할 수 있 습니다 . 신경심리검사 를 통해 인지 영역 의 문제 가 의심 될 경우 치매 여부 를 확인 하 기 위해 혈액 검사 , 뇌 영상 검사 , 핵의학 검사 등 을 시행 합니다 . 아포 지질단백질 유전자 검사 와 비타민 및 갑상선 기능 검사 도 알츠하이머병 조기 진단 을 위해 실시 될 수 있 습니다 . 혈액 검사 에 는 알츠하이머병 위험 인자 가 있 는 경우 아밀로이드 양전자 방출 단층 촬영 이나 양전자 방출 단층 촬영 이 가능 합니다 . 알츠하이머병 의 조기 진단 을 위해 아밀로이드 와 관련 된 유전자 검사 와 비타민 a 부족 에 따른 뇌 변화 를 확인 하 는 검사 도 있 습니다 . 최근 에 는 알츠하이머병 을 조기 에 진단 하 기 위해 뇌 자기공명영상 을 통해 아밀로이드 단백질 과 타우 단백질 의 이상 을 확인 할 수 있 는 다양 한 연구 가 진행 중 입니다 . 치매 를 정확히 진단 하 기 위해서 는 다양 한 검사 가 필요 하 며 , 각 검사 에 따른 적절 한 조치 와 치료 가 필요 합니다 .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글\n",
    "clean_text(question[20], mecab)\n",
    "clean_text(answer[20], mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [clean_text(sent, mecab) for sent in question.values[:len(question)]]\n",
    "answers = [clean_text(sent, mecab) for sent in answer.values[:len(question)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cdr 에 비해 cdr 외 에 치매 진단 에 다른 검사 방법 이 있 는지 알 고 싶 습니다 .',\n",
       " '치매 검진 을 받 기 위해 어떤 검사 를 받 아야 할까요 ?',\n",
       " '치매 의 검진 을 위해 어떤 종류 의 검사 가 주로 시행 되 나요 ?',\n",
       " '치매 검사 를 어디 에서 받 을 수 있 는지 알 고 싶 어요 .',\n",
       " '치매 검진 을 위해 어느 의사 를 찾아가 야 하 나요 ?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['병원 에서 치매 진단 을 위해 다양 한 검사 가 수행 됩니다 . 치매 가 의심 될 경우 , 환자 들 은 여러 가지 신 경학 적 검사 와 상담 을 받 게 됩니다 . 의사 는 면담 을 통해 환자 의 증상 과 인지 기능 의 저하 정도 를 평가 합니다 . 안저 검사 , 신 경학 적 검사 , 혈액 검사 , 소변 검사 등 이 포함 될 수 있 습니다 . 뇌 영상 검사 에서 는 mri , ct , pet 등 이 사용 됩니다 . 알츠하이머병 진단 에 유용 하 며 , 양전자 방출 단층 촬영 이나 단일 광자 방출 컴퓨터 단층 촬영 도 고려 됩니다 . 이러 한 검사 는 뇌 의 기능 적 평가 에 도움 을 줍니다 . 또한 , 양전자 방출 단층 촬영 과 단일 광자 방출 컴퓨터 단층 촬영 은 알츠하이머병 치료 의 일환 으로 사용 되 기 도 합니다 . 치매 진단 을 위해서 는 환자 의 증상 과 다양 한 신 경학 적 검사 가 필요 하 며 , 정확 한 진단 을 위해 다양 한 검사 가 고려 될 수 있 습니다 .',\n",
       " '치매 진단 은 전문가 가 환자 의 상황 을 조사 하 고 병력 및 인지 기능 테스트 , 신경 심리 검사 등 의 검사 를 통해 수행 됩니다 . 전문가 는 환자 의 증상 , 물리 적 검사 , 신경 심리 검사 등 을 평가 하 여 치매 를 진단 합니다 . 일반 적 으로 신경 심리 검사 를 통해 인지 기능 을 평가 하 고 , 혈액 검사 를 통해 알츠하이머병 의 위험 요인 이 있 는지 확인 할 수 있 습니다 . 뇌 영상 검사 를 통해 뇌 의 이상 여부 를 확인 하 고 , 뇌파 검사 , ct , mri , 단일 광자 방출 컴퓨터 단층 촬영 mri 등 의 추가 검사 를 시행 하 기 도 합니다 . 진단 은 치매 의 원인 을 파악 하 고 , 적절 한 치료 계획 을 수립 하 기 위해 중요 한 과정 입니다 . 따라서 전문가 의 정확 한 검사 와 다양 한 검사 방법 을 통해 환자 의 상황 을 정확히 평가 하 고 , 이 에 맞 는 적절 한 치료 방법 을 선택 하 는 것 이 필요 합니다 .',\n",
       " '병원 에서 치매 의 진단 은 단계 별 로 이루어지 며 , 다음 과 같 은 과정 을 거칩니다 . 먼저 , 환자 를 진찰 하 여 초기 증상 을 확인 합니다 . 이 를 위해 기억력 , 언어 능력 , 주의력 , 문제 해결 능력 , 시공간 인식 능력 등 을 검사 합니다 . 이후 , 인지 기능 의 문제 가 있 는지 확인 하 기 위해 신경 심리 검사 가 실시 됩니다 . 이 를 통해 알츠하이머병 과 같 은 치매 의 유형 을 확인 하 고 , 치료 및 예방 을 위한 적절 한 방법 을 모색 할 수 있 습니다 . 혈액 검사 를 통해 알츠하이머병 의 고위험 군 인지 확인 하 고 , 뇌 영상 검사 를 통해 치매 의 원인 질환 을 확인 할 수 있 습니다 . 또한 핵의학 검사 를 통해 치매 의 원인 을 확인 할 수 있 습니다 . 치매 의 진단 은 단계 별 로 진행 되 며 , 신경 심리 검사 와 혈액 검사 , 뇌 영상 검사 , 핵의학 검사 등 의 다양 한 검사 가 사용 됩니다 . 이러 한 검사 결과 를 종합 하 여 치매 의 정확 한 진단 을 내릴 수 있 습니다 .',\n",
       " '치매 진단 을 위해 여러 가지 검사 가 실시 됩니다 . 먼저 , 환자 의 병력 을 청취 하 고 문진 을 진행 합니다 . 이러 한 진단 과정 은 치매 의 종류 와 발생 원인 을 파악 하 는 데 중요 한 역할 을 합니다 . 다음 으로 신경 심리 검사 를 통해 인지 기능 에 문제 가 있 는지 파악 합니다 . 이 검사 는 기억력 과 다른 인지 영역 의 문제 를 평가 하 는 것 으로 , 치매 와 관련 된 인지 기능 의 변화 를 확인 하 는 데 도움 이 됩니다 . 뇌 영상 검사 도 진단 의 중요 한 도구 입니다 . 뇌 의 구조 적 이상 이나 뇌전증 의 발생 여부 를 확인 하 기 위해 뇌 촬영 이나 자기공명영상 검사 를 사용 할 수 있 습니다 . 또한 , 혈액 검사 와 뇌척수액 검사 등 의 검사 를 통해 치매 와 관련 된 다른 질환 의 가능 성 도 배제 합니다 . 정확 한 치매 진단 을 위해 다양 한 검사 가 사용 되 며 , 이 를 통해 정확 한 원인 파악 과 치료 계획 이 수립 됩니다 .',\n",
       " '병원 에서 는 다양 한 진단 절차 를 거쳐 치매 를 진단 합니다 . 먼저 , 치매 의 증상 을 정확 하 게 판단 하 기 위해 면담 과 신체 진찰 이 진행 됩니다 . 의사 가 면담 을 통해 인지 기능 의 이상 이 있 는지 확인 하 면 신경 심리 검사 를 실시 하 여 다른 인지 영역 의 문제 가 있 는지 알아봅니다 . 신경 심리 검사 에서 알츠하이머병 과 다른 치료법 으로 치료 가능 한 치매 인지 확인 하 기 위해 혈액 검사 , 뇌 영상 검사 ct 또는 mri , 핵의학 검사 , 단일 광자 방출 컴퓨터 단층 촬영 spect , 양전자 방출 단층 촬영 pet 등 의 검사 가 실시 됩니다 . 이외 에 도 뇌척수액 검사 , 일반 혈액 검사 , 뇌파 검사 , 혈당 검사 등 의 추가 검사 가 필요 한 경우 도 있 을 수 있 습니다 . 정확 한 진단 을 위해 이러 한 다양 한 검사 가 사용 됩니다 . 이러 한 검사 들 은 치매 의 원인 을 정확 하 게 파악 하 고 적절 한 치료 를 시작 하 는 데 에 도움 이 됩니다 .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2index = {\n",
    "            '<PAD>': PAD_TOKEN,\n",
    "            '<SOS>': SOS_TOKEN, \n",
    "            '<EOS>': EOS_TOKEN,\n",
    "        }\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_TOKEN: '<PAD>', \n",
    "            SOS_TOKEN: '<SOS>', \n",
    "            EOS_TOKEN: '<EOS>'\n",
    "        }\n",
    "        \n",
    "        self.n_words = 3  # PAD, SOS, EOS 포함\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: 치매 환자 를 위한 식단 조절 의 중요 성 과 이점 은 무엇 인가요 ?\n",
      "==============================\n",
      "[단어사전]\n",
      "******************************\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '치매': 3, '환자': 4, '를': 5, '위한': 6, '식단': 7, '조절': 8, '의': 9, '중요': 10, '성': 11, '과': 12, '이점': 13, '은': 14, '무엇': 15, '인가요': 16, '?': 17}\n"
     ]
    }
   ],
   "source": [
    "print(f'원문: {questions[550]}')\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(questions[550])\n",
    "print('==='*10)\n",
    "print('[단어사전]')\n",
    "print('***'*10)\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: [55, 27, 52, 56, 38, 65]\n",
      "Output: [55, 27, 52, 56, 38, 65, 2, 0, 0, 0]\n",
      "Total Length: 10\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "sentence_length = 6\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,))\n",
    "sentence_tokens = sentence_tokens.tolist()\n",
    "print(f'Generated Sentence: {sentence_tokens}')\n",
    "\n",
    "sentence_tokens = sentence_tokens[:(max_length-1)]\n",
    "\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "# 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "sentence_tokens.append(2)\n",
    "\n",
    "for i in range(token_length, max_length-1):\n",
    "    # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "    sentence_tokens.append(0)\n",
    "\n",
    "print(f'Output: {sentence_tokens}')\n",
    "print(f'Total Length: {len(sentence_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, min_length=3, max_length=32):\n",
    "        super(TextDataset, self).__init__()\n",
    "        # data_dir = 'data'\n",
    "        \n",
    "        # TOKEN 정의\n",
    "        self.PAD_TOKEN = 0 # Padding 토큰\n",
    "        self.SOS_TOKEN = 1 # SOS 토큰\n",
    "        self.EOS_TOKEN = 2 # EOS 토큰\n",
    "        \n",
    "        self.tagger = MeCab()   # 형태소 분석기\n",
    "        self.max_length = max_length # 한 문장의 최대 길이 지정\n",
    "        \n",
    "        # CSV 데이터 로드\n",
    "        # df = pd.read_csv(os.path.join(data_dir, csv_path))\n",
    "        df=pd.read_csv('dementia.csv')\n",
    "        # 한글 정규화\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "        \n",
    "        # src: 질의, tgt: 답변\n",
    "        src_clean = []\n",
    "        tgt_clean = []\n",
    "        \n",
    "        # 단어 사전 생성\n",
    "        wordvocab = WordVocab()\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            src = row['question']\n",
    "            tgt = row['answer']\n",
    "            \n",
    "            # 한글 전처리\n",
    "            src = self.clean_text(src)\n",
    "            tgt = self.clean_text(tgt)\n",
    "            \n",
    "            if len(src.split()) > min_length and len(tgt.split()) > min_length:\n",
    "                # 최소 길이를 넘어가는 문장의 단어만 추가\n",
    "                wordvocab.add_sentence(src)\n",
    "                wordvocab.add_sentence(tgt)\n",
    "                src_clean.append(src)\n",
    "                tgt_clean.append(tgt)            \n",
    "        \n",
    "        self.srcs = src_clean\n",
    "        self.tgts = tgt_clean\n",
    "        self.wordvocab = wordvocab\n",
    "\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        # 정규표현식에 따른 한글 정규화\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        # 한글 정규화\n",
    "        sentence = self.normalize(sentence)\n",
    "        # 형태소 처리\n",
    "        sentence = self.tagger.morphs(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "    \n",
    "    def texts_to_sequences(self, sentence):\n",
    "        # 문장 -> 시퀀스로 변환\n",
    "        return [self.wordvocab.word2index[w] for w in sentence.split()]\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens):\n",
    "        # 문장의 맨 끝 토큰은 제거\n",
    "        sentence_tokens = sentence_tokens[:(self.max_length-1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "\n",
    "        # 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "\n",
    "        for i in range(token_length, (self.max_length-1)):\n",
    "            # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "        return sentence_tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.srcs[idx]\n",
    "        inputs_sequences = self.texts_to_sequences(inputs)\n",
    "        inputs_padded = self.pad_sequence(inputs_sequences)\n",
    "        \n",
    "        outputs = self.tgts[idx]\n",
    "        outputs_sequences = self.texts_to_sequences(outputs)\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences)\n",
    "        \n",
    "        return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 한 문장의 최대 단어길이를 30로 설정\u001b[39;00m\n\u001b[0;32m      2\u001b[0m MAX_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdementia.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 36\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, csv_path, min_length, max_length)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 한글 전처리\u001b[39;00m\n\u001b[0;32m     35\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_text(src)\n\u001b[1;32m---> 36\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(src\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m>\u001b[39m min_length \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tgt\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m>\u001b[39m min_length:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# 최소 길이를 넘어가는 문장의 단어만 추가\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     wordvocab\u001b[38;5;241m.\u001b[39madd_sentence(src)\n",
      "Cell \u001b[1;32mIn[84], line 56\u001b[0m, in \u001b[0;36mTextDataset.clean_text\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# 한글 정규화\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# 형태소 처리\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger\u001b[38;5;241m.\u001b[39mmorphs(sentence)\n",
      "Cell \u001b[1;32mIn[84], line 52\u001b[0m, in \u001b[0;36mTextDataset.normalize\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# 정규표현식에 따른 한글 정규화\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# 한 문장의 최대 단어길이를 30로 설정\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "dataset = TextDataset('dementia.csv', min_length=3, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 데이터 임의 추출\n",
    "x, y = dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([30])\n",
      "tensor([  7,  56,   8,  73,  93,  23,  98,  52,  57, 257,  65,  66,  10,  26,\n",
      "         70,  92,  14, 206, 207, 208,  19,   2,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "y shape: torch.Size([30])\n",
      "tensor([  7,  56,   8,  73,  93,  94,  46,  36,  37,  10,  26,  95,  58,  19,\n",
      "        173,  51, 195,  10,  56,  48,   7,  49, 186, 187,  22,  57,  73,  44,\n",
      "         28,   2])\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.shape}')\n",
    "print(x)\n",
    "\n",
    "print(f'y shape: {y.shape}')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80%의 데이터를 train에 할당합니다.\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나머지 20% 데이터를 test에 할당합니다.\n",
    "test_size = len(dataset) - train_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 랜덤 스플릿으로 분할을 완료합니다.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=16, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치 데이터를 추출합니다.\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 30]), torch.Size([16, 30]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: (batch_size, sequence_length)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 단어 사전의 개수 지정\n",
    "        self.num_vocabs = num_vocabs\n",
    "        # 임베딩 레이어 정의 (number of vocabs, embedding dimension)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # GRU (embedding dimension)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# Embedding Layer의 입/출력 shape에 대한 이해\n",
    "\n",
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "# x의 shape을 변경합니다.\n",
    "# (batch_size, sequence_length) => (sequence_length, batch_size)\n",
    "embedded = embedding(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(embedded.shape)\n",
    "# input:  (sequence_length, batch_size)\n",
    "# output: (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "embedded = embedded.permute(1, 0, 2)\n",
    "print(embedded.shape)\n",
    "# (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32   \n",
    "\n",
    "gru = nn.GRU(embedding_dim,      # embedding 차원\n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False)\n",
    "\n",
    "# input       : (sequence_length, batch_size, embedding_dim)\n",
    "# h0          : (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "o, h = gru(embedded, None)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabs: 3477\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "print(f'number of vocabs: {NUM_VOCABS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 정의\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "# Encoder에 x 통과 후 output, hidden_size 의 shape 확인\n",
    "# input(x)    : (batch_size, sequence_length)\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 단어사전 개수\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # 최종 출력은 단어사전의 개수\n",
    "        self.fc = nn.Linear(hidden_size, num_vocabs)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0) # (1, batch_size) 로 변환\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        output = self.fc(output.squeeze(0)) # (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding Layer의 입/출력 shape\n",
    "x = torch.abs(torch.randn(size=(1, 16)).long())\n",
    "print(x)\n",
    "x.shape\n",
    "# batch_size = 16 이라 가정했을 때,\n",
    "# (1, batch_size)\n",
    "# 여기서 batch_size => (1, batch_size) 로 shape 변환을 선행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    " #GRU Layer의 입/출력 shape에 대한 이해\n",
    "hidden_size = 32\n",
    "\n",
    "gru = nn.GRU(embedding_dim, \n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False, \n",
    "             batch_first=False, # batch_first=False로 지정\n",
    "            )\n",
    "\n",
    "o, h = gru(embedded)\n",
    "\n",
    "print(o.shape)\n",
    "# output shape: (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state shape: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([16, 3477])\n"
     ]
    }
   ],
   "source": [
    "# 최종 출력층(FC) shape에 대한 이해\n",
    "fc = nn.Linear(32, NUM_VOCABS) # 출력은 단어사전의 개수로 가정\n",
    "\n",
    "output = fc(o[0])\n",
    "\n",
    "print(o[0].shape)\n",
    "print(output.shape)\n",
    "# input : (batch_size, output from GRU)\n",
    "# output: (batch_size, output dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더 -> 디코더 입출력 shape\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape, h.shape)\n",
    "# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))\n",
    "# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***************\n",
    "x = torch.abs(torch.full(size=(16,), fill_value=SOS_TOKEN, dtype=torch.long))\n",
    "print(x)\n",
    "x.shape\n",
    "\n",
    "# batch_size = 16 이라 가정(16개의 SOS 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3477]), torch.Size([1, 16, 32]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output, decoder_hidden = decoder(x, h)\n",
    "decoder_output.shape, decoder_hidden.shape\n",
    "# (batch_size, num_vocabs), (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        # inputs : (batch_size, sequence_length)\n",
    "        # outputs: (batch_size, sequence_length)\n",
    "        \n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "        \n",
    "        # 리턴할 예측된 outputs를 저장할 임시 변수\n",
    "        # (sequence_length, batch_size, num_vocabs)\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "        \n",
    "        # 인코더에 입력 데이터 주입, encoder_output은 버리고 hidden_state 만 살립니다. \n",
    "        # 여기서 hidden_state가 디코더에 주입할 context vector 입니다.\n",
    "        # (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # (batch_size) shape의 SOS TOKEN으로 채워진 디코더 입력 생성********************\n",
    "        decoder_input = torch.full((batch_size,), SOS_TOKEN, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 순회하면서 출력 단어를 생성합니다.\n",
    "        # 0번째는 SOS TOKEN이 위치하므로, 1번째 인덱스부터 순회합니다.\n",
    "        for t in range(0, output_length):\n",
    "            # decoder_input : 디코더 입력 (batch_size) 형태의 SOS TOKEN로 채워진 입력\n",
    "            # decoder_output: (batch_size, num_vocabs)\n",
    "            # decoder_hidden: (Bidirectional(1) x number of layers(1), batch_size, hidden_size), context vector와 동일 shape\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # t번째 단어에 디코더의 output 저장\n",
    "            predicted_outputs[t] = decoder_output\n",
    "            \n",
    "            # teacher forcing 적용 여부 확률로 결정\n",
    "            # teacher forcing 이란: 정답치를 다음 RNN Cell의 입력으로 넣어주는 경우. 수렴속도가 빠를 수 있으나, 불안정할 수 있음\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # top1 단어 토큰 예측\n",
    "            top1 = decoder_output.argmax(1) \n",
    "            \n",
    "            # teacher forcing 인 경우 ground truth 값을, 그렇지 않은 경우, 예측 값을 다음 input으로 지정\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "        \n",
    "        return predicted_outputs.permute(1, 0, 2) # (batch_size, sequence_length, num_vocabs)로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq 입출력 확인\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Seq2Seq 정의\n",
    "seq2seq = Seq2Seq(encoder, decoder, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30]) torch.Size([16, 30])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape, y.shape)\n",
    "# (batch_size, sequence_length), (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30, 3477])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "output = seq2seq(x, y)\n",
    "print(output.shape)\n",
    "# (batch_size, sequence_length, num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vocabs: 3477\n",
      "======================\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(3477, 256)\n",
      "    (gru): GRU(256, 512)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3477, 256)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(256, 512)\n",
      "    (fc): Linear(in_features=512, out_features=3477, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDIMG_DIM = 256\n",
    "\n",
    "print(f'num_vocabs: {NUM_VOCABS}\\n======================')\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "# Seq2Seq 생성\n",
    "# encoder, decoder를 device 모두 지정\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련에 적용할 하이퍼파라미터 설정\n",
    "\n",
    "LR = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "es = EarlyStopping(patience=5, \n",
    "                   delta=0.001, \n",
    "                   mode='min', \n",
    "                   verbose=True\n",
    "                  )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=2,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수 정의\n",
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output: (batch_size, sequence_length, num_vocabs)\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "        \n",
    "        # 1번 index 부터 슬라이싱한 이유는 0번 index가 SOS TOKEN 이기 때문\n",
    "        # (batch_size*sequence_length, num_vocabs) 로 변경\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        \n",
    "        # (batch_size*sequence_length) 로 변경\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation 함수 정의\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x, y)\n",
    "            output_dim = output.size(2)\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            eval_loss += loss.item() * x.size(0)\n",
    "            \n",
    "    return eval_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 샘플링 후 결과 추론\n",
    "def sequence_to_sentence(sequences, index2word):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "\n",
    "        word = index2word[p]\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence를 다시 문장으로 바꾸어 문장 형식으로 출력하기 위한 함수\n",
    "\n",
    "def random_evaluation(model, dataset, index2word, device, n=10):\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)      # Shuffle\n",
    "    sampled_indices = indices[:n]   # Sampling N indices\n",
    "    \n",
    "    # 샘플링한 데이터를 기반으로 DataLoader 생성\n",
    "    sampler = SubsetRandomSampler(sampled_indices)\n",
    "    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in sampled_dataloader:\n",
    "            x, y = x.to(device), y.to(device)        \n",
    "            output = model(x, y, teacher_forcing_ratio=0)\n",
    "            # output: (number of samples, sequence_length, num_vocabs)\n",
    "            \n",
    "            preds = output.detach().cpu().numpy()\n",
    "            x = x.detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(n):\n",
    "                print(f'질문   : {sequence_to_sentence(x[i], index2word)}')\n",
    "                print(f'답변   : {sequence_to_sentence(y[i], index2word)}')\n",
    "                print(f'예측답변: {sequence_to_sentence(preds[i].argmax(1), index2word)}')\n",
    "                print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train output :  tensor([[[ -9.1213, -10.0441,  -5.8223,  ...,  -9.4564,  -9.2038,  -9.3820],\n",
      "         [-11.3635, -12.7712,  -4.9944,  ..., -11.3816, -11.9248, -11.3855],\n",
      "         [-10.9816, -12.6387,  -5.2636,  ..., -11.1212, -11.7909, -11.7844],\n",
      "         ...,\n",
      "         [-10.9478, -11.6780,   5.4317,  ..., -10.9040, -11.2415, -11.0675],\n",
      "         [-11.7038, -12.3056,   7.2792,  ..., -11.5999, -11.6906, -11.9033],\n",
      "         [-10.8046, -11.5619,   8.5538,  ..., -10.6359, -11.2309, -11.4497]],\n",
      "\n",
      "        [[ -9.5156,  -9.9462,  -6.1687,  ...,  -9.3022,  -9.4643, -10.0432],\n",
      "         [-11.0405, -11.9636,  -5.1192,  ..., -11.0330, -11.6773, -11.5758],\n",
      "         [-11.3658, -12.5191,  -4.7318,  ..., -11.7286, -12.1710, -12.6131],\n",
      "         ...,\n",
      "         [-10.5254, -10.7136,   3.6381,  ..., -10.3599, -10.4300, -10.7769],\n",
      "         [-10.1904, -10.4635,   5.7356,  ..., -10.6063, -10.4099, -11.1141],\n",
      "         [ -9.7106, -10.2150,   7.6394,  ...,  -9.8873, -10.0225, -10.6170]],\n",
      "\n",
      "        [[ -9.3869, -10.3802,  -6.2123,  ...,  -9.7590,  -9.6268, -10.1188],\n",
      "         [-11.7020, -13.1976,  -5.0333,  ..., -11.9642, -12.5791, -12.1258],\n",
      "         [-12.5959, -13.8985,  -4.7631,  ..., -12.5736, -12.9886, -13.2033],\n",
      "         ...,\n",
      "         [-11.9819, -12.2901,   6.6808,  ..., -11.8730, -11.7245, -12.2050],\n",
      "         [-11.1796, -12.1120,   7.4133,  ..., -11.1940, -11.1101, -11.5745],\n",
      "         [-12.3304, -13.0483,   9.0307,  ..., -12.1510, -12.1768, -12.6725]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.4618,  -9.8956,  -5.8031,  ...,  -9.4048,  -9.3139,  -9.5423],\n",
      "         [-11.8851, -12.9564,  -4.6384,  ..., -11.8797, -12.4005, -11.8793],\n",
      "         [-11.4685, -12.8127,  -5.0898,  ..., -11.6996, -12.2588, -12.2175],\n",
      "         ...,\n",
      "         [-10.8756, -11.3801,   6.1216,  ..., -11.0144, -11.2772, -11.3038],\n",
      "         [-11.3660, -11.9309,   7.5673,  ..., -11.4883, -11.8458, -12.0557],\n",
      "         [-10.8235, -11.5063,   8.9380,  ..., -10.9240, -11.0230, -11.5570]],\n",
      "\n",
      "        [[ -9.4881, -10.1836,  -6.3092,  ...,  -9.2997,  -9.2637,  -9.9307],\n",
      "         [-10.6452, -11.8799,  -4.7937,  ..., -10.7984, -11.4348, -11.0310],\n",
      "         [-11.2516, -12.6240,  -5.7741,  ..., -11.4183, -11.8455, -11.6886],\n",
      "         ...,\n",
      "         [-11.3247, -12.0239,   5.0188,  ..., -11.4818, -11.8150, -11.6802],\n",
      "         [-11.3390, -12.2541,   6.6101,  ..., -11.4006, -11.7009, -11.5025],\n",
      "         [-11.1694, -11.6304,   7.4343,  ..., -10.9531, -11.0852, -11.2564]],\n",
      "\n",
      "        [[ -8.9938, -10.0455,  -6.3573,  ...,  -9.5195,  -9.3996,  -9.6045],\n",
      "         [-11.7885, -13.4159,  -5.1071,  ..., -12.0525, -12.4299, -12.2383],\n",
      "         [-11.8234, -13.0227,  -4.9558,  ..., -11.9415, -12.1601, -12.3574],\n",
      "         ...,\n",
      "         [-11.4844, -12.1829,   4.3255,  ..., -11.5871, -11.6763, -11.8838],\n",
      "         [-11.3612, -12.2452,   6.1066,  ..., -11.2302, -11.3156, -11.9184],\n",
      "         [-10.3959, -11.5761,   7.2387,  ..., -10.7067, -10.4931, -11.1740]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.0720, -10.1057,  -6.3568,  ...,  -9.3119,  -9.4369,  -9.5684],\n",
      "         [-11.5859, -13.1208,  -5.1471,  ..., -11.8451, -12.4767, -11.8599],\n",
      "         [-11.3642, -12.9820,  -5.6011,  ..., -11.6730, -12.3489, -12.0752],\n",
      "         ...,\n",
      "         [-10.6092, -11.4857,   3.9076,  ..., -11.0026, -10.7802, -11.0374],\n",
      "         [-10.8999, -11.6132,   6.0173,  ..., -11.2950, -11.1766, -11.3739],\n",
      "         [-11.2111, -11.8875,   7.2900,  ..., -11.5399, -11.3174, -11.4871]],\n",
      "\n",
      "        [[ -9.1127,  -9.8533,  -6.2370,  ...,  -8.9356,  -9.6023,  -9.7581],\n",
      "         [-11.3931, -12.6265,  -4.9969,  ..., -11.2199, -12.1833, -11.6542],\n",
      "         [-11.9951, -13.5421,  -4.6442,  ..., -12.5651, -12.9299, -12.8796],\n",
      "         ...,\n",
      "         [-12.0636, -12.4379,   5.3163,  ..., -12.1530, -12.4257, -12.5452],\n",
      "         [-12.0151, -12.2025,   6.9500,  ..., -11.9978, -12.5741, -11.9674],\n",
      "         [-12.1374, -12.7608,   7.3691,  ..., -12.0225, -12.5152, -12.1860]],\n",
      "\n",
      "        [[ -8.8671,  -9.2542,  -6.5503,  ...,  -8.6150,  -9.3125,  -8.9905],\n",
      "         [-11.1977, -12.2261,  -5.2706,  ..., -11.1612, -12.1089, -11.1130],\n",
      "         [-10.2696, -11.0237,  -5.9480,  ..., -10.2533, -11.2949, -10.5201],\n",
      "         ...,\n",
      "         [-12.3378, -12.7617,   4.3048,  ..., -12.0833, -12.5427, -11.9107],\n",
      "         [-11.5551, -12.0795,   5.4556,  ..., -11.4415, -11.6683, -11.7160],\n",
      "         [-11.3058, -11.6125,   5.9970,  ..., -11.2291, -11.4467, -11.4174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.8428,  -9.5525,  -6.1244,  ...,  -9.0122,  -8.7804,  -9.1921],\n",
      "         [-10.0346, -11.3798,  -4.7307,  ..., -10.2942, -10.8244, -10.4149],\n",
      "         [-11.1307, -12.4993,  -5.3968,  ..., -11.3545, -11.4781, -11.5608],\n",
      "         ...,\n",
      "         [-11.3607, -11.9635,   4.7594,  ..., -11.5050, -11.5872, -11.7356],\n",
      "         [-10.3659, -11.2958,   6.3224,  ..., -10.8346, -10.9415, -10.9334],\n",
      "         [-10.4023, -11.1474,   6.2982,  ..., -10.7064, -10.8240, -10.8376]],\n",
      "\n",
      "        [[ -9.3379, -10.3217,  -6.1371,  ...,  -9.4520,  -9.4699,  -9.9045],\n",
      "         [-11.6781, -13.2522,  -4.9259,  ..., -11.9934, -12.4214, -12.0848],\n",
      "         [-10.9690, -12.5203,  -5.6977,  ..., -11.4216, -11.8752, -11.8415],\n",
      "         ...,\n",
      "         [-11.6939, -12.1952,   5.9836,  ..., -11.7252, -12.0066, -11.8793],\n",
      "         [-11.9553, -12.5995,   6.4578,  ..., -12.0436, -12.0101, -12.1438],\n",
      "         [-12.4671, -13.2485,   7.6651,  ..., -12.4486, -12.3641, -12.8854]],\n",
      "\n",
      "        [[ -9.5298, -10.0529,  -6.2909,  ...,  -9.5166,  -9.4691,  -9.9954],\n",
      "         [-10.7100, -11.9527,  -4.7963,  ..., -10.8777, -11.2780, -10.9686],\n",
      "         [-11.5515, -12.8383,  -5.6774,  ..., -11.7371, -11.9800, -11.8234],\n",
      "         ...,\n",
      "         [ -9.2189,  -9.6782,   3.6687,  ...,  -9.6406,  -9.7009,  -9.4080],\n",
      "         [ -9.3624,  -9.8844,   6.0040,  ...,  -9.7746,  -9.6949,  -9.8385],\n",
      "         [ -9.3596,  -9.9696,   7.7757,  ...,  -9.6060,  -9.6859,  -9.6002]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.1794,  -9.7414,  -6.2856,  ...,  -9.1626,  -9.2349,  -9.7995],\n",
      "         [-11.8266, -12.9525,  -5.1429,  ..., -11.8174, -12.4737, -12.2034],\n",
      "         [-12.3553, -13.3917,  -4.8755,  ..., -12.2785, -12.5559, -13.0384],\n",
      "         ...,\n",
      "         [-11.1385, -11.2791,   4.8606,  ..., -11.3330, -11.5735, -11.5008],\n",
      "         [-10.9566, -11.2843,   5.7585,  ..., -11.0150, -10.9780, -11.4929],\n",
      "         [-10.4847, -10.6781,   6.9480,  ..., -10.5751, -10.7156, -10.8492]],\n",
      "\n",
      "        [[ -9.4152, -10.1814,  -6.1260,  ...,  -9.3813,  -9.6539, -10.0156],\n",
      "         [-10.2148, -11.5429,  -4.6139,  ..., -10.4291, -11.0172, -10.5215],\n",
      "         [-11.5997, -12.8518,  -5.3675,  ..., -11.7234, -12.1451, -11.9426],\n",
      "         ...,\n",
      "         [-10.4109, -10.9614,   4.9490,  ..., -10.3392, -10.7153, -10.3388],\n",
      "         [-10.7211, -11.2079,   5.8754,  ..., -10.8403, -10.9891, -10.9444],\n",
      "         [-10.5916, -11.2384,   7.6014,  ..., -10.6971, -10.4307, -10.4472]],\n",
      "\n",
      "        [[ -9.0052,  -9.6659,  -6.2158,  ...,  -9.1551,  -8.8994,  -9.3274],\n",
      "         [-11.3541, -12.4384,  -5.1065,  ..., -11.4899, -11.7801, -11.4961],\n",
      "         [-10.8731, -12.2427,  -5.2867,  ..., -11.0562, -11.6025, -11.6746],\n",
      "         ...,\n",
      "         [-11.3139, -11.7156,   3.8473,  ..., -11.4712, -11.1289, -11.4134],\n",
      "         [-10.8118, -11.4136,   5.4653,  ..., -11.3537, -10.9457, -11.4850],\n",
      "         [-10.3077, -11.2182,   7.1324,  ..., -10.7812, -10.6713, -10.8137]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.2490,  -9.6898,  -6.3934,  ...,  -9.0001,  -9.6498,  -9.8068],\n",
      "         [-11.2931, -12.3166,  -5.1544,  ..., -11.3150, -12.1629, -11.6347],\n",
      "         [-12.4328, -13.5503,  -4.7689,  ..., -12.7122, -12.8298, -13.0938],\n",
      "         ...,\n",
      "         [-10.9949, -11.3724,   5.3600,  ..., -10.8688, -10.8477, -11.2724],\n",
      "         [-11.2503, -11.2803,   7.1533,  ..., -10.8293, -11.3384, -11.3250],\n",
      "         [-11.7750, -12.2148,   8.1596,  ..., -11.8570, -12.0981, -12.2551]],\n",
      "\n",
      "        [[ -9.1591,  -9.9699,  -6.2956,  ...,  -9.4344,  -9.5640,  -9.7746],\n",
      "         [-11.8040, -13.0218,  -5.0729,  ..., -11.8878, -12.5293, -11.8845],\n",
      "         [-11.2129, -11.8670,  -5.7648,  ..., -10.9226, -11.7591, -11.1372],\n",
      "         ...,\n",
      "         [-11.3060, -11.8977,   4.1239,  ..., -11.8135, -11.9992, -11.9543],\n",
      "         [-12.1190, -12.6075,   4.0124,  ..., -12.1962, -11.8727, -12.2175],\n",
      "         [-12.5475, -12.9469,   6.1797,  ..., -12.3924, -12.8353, -12.4236]],\n",
      "\n",
      "        [[ -9.2025,  -9.6319,  -6.5446,  ...,  -9.3213,  -9.7695,  -9.7849],\n",
      "         [-11.6856, -12.8472,  -5.0697,  ..., -11.7731, -12.4560, -12.0396],\n",
      "         [-12.3272, -13.7946,  -4.6689,  ..., -13.1026, -13.0777, -13.2613],\n",
      "         ...,\n",
      "         [-13.1451, -13.9161,   4.4310,  ..., -13.0275, -13.5266, -13.5525],\n",
      "         [-12.0229, -12.2467,   5.1947,  ..., -12.2484, -12.0717, -12.3312],\n",
      "         [-12.0219, -11.8231,   7.1625,  ..., -11.7364, -11.6544, -11.8177]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.5499, -10.2059,  -6.5528,  ...,  -9.6086, -10.1603, -10.2134],\n",
      "         [-11.7754, -13.0096,  -5.1586,  ..., -11.9169, -12.5779, -11.9374],\n",
      "         [-12.4107, -13.7871,  -4.7594,  ..., -13.1188, -13.1753, -13.3094],\n",
      "         ...,\n",
      "         [-11.4496, -12.1436,   4.7305,  ..., -11.6179, -11.9360, -12.0271],\n",
      "         [-12.5899, -13.1455,   5.9813,  ..., -12.9476, -13.0248, -12.9700],\n",
      "         [-12.5993, -13.4257,   7.4160,  ..., -12.4012, -12.6896, -12.7200]],\n",
      "\n",
      "        [[ -8.6066,  -9.4920,  -5.6963,  ...,  -8.6751,  -8.5110,  -8.9939],\n",
      "         [-10.8511, -12.0875,  -4.8034,  ..., -10.9395, -11.5258, -11.2181],\n",
      "         [-10.8769, -12.3851,  -4.9572,  ..., -11.0702, -11.7016, -11.7367],\n",
      "         ...,\n",
      "         [-11.1445, -11.7572,   5.2228,  ..., -11.1688, -11.4840, -12.1705],\n",
      "         [-11.2933, -11.8781,   6.7432,  ..., -11.1176, -11.2802, -12.0933],\n",
      "         [-11.1431, -11.7534,   8.5701,  ..., -11.1309, -11.0469, -11.6643]],\n",
      "\n",
      "        [[ -9.1791,  -9.6654,  -6.3827,  ...,  -9.0292,  -9.6065,  -9.7229],\n",
      "         [-11.7200, -12.6157,  -5.1113,  ..., -11.6529, -12.3899, -11.7444],\n",
      "         [-12.3146, -13.6121,  -4.8416,  ..., -12.8433, -13.0186, -13.0891],\n",
      "         ...,\n",
      "         [-11.1762, -11.5074,   4.6093,  ..., -11.1192, -11.4924, -11.5583],\n",
      "         [-10.9572, -11.1630,   5.9128,  ..., -10.9516, -11.2006, -11.1552],\n",
      "         [-10.4083, -10.9096,   8.5178,  ..., -10.3905, -10.8505, -10.7268]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.2076, -10.1600,  -6.3316,  ...,  -9.1470,  -9.9668,  -9.9650],\n",
      "         [-11.3180, -12.7340,  -5.0622,  ..., -11.3312, -12.3813, -11.7852],\n",
      "         [-12.2264, -13.9716,  -4.6706,  ..., -12.9521, -13.2994, -13.3602],\n",
      "         ...,\n",
      "         [-10.0790, -10.9630,   4.5832,  ..., -10.0476, -10.8868, -10.4259],\n",
      "         [-10.9461, -11.2057,   6.7958,  ..., -10.9236, -11.2932, -11.3894],\n",
      "         [-11.0169, -11.6457,   9.0572,  ..., -11.1564, -11.6096, -11.7499]],\n",
      "\n",
      "        [[ -8.9049,  -9.3050,  -6.5011,  ...,  -8.6346,  -9.5399,  -9.4737],\n",
      "         [-10.9303, -11.9117,  -5.3368,  ..., -10.9476, -11.9209, -11.1682],\n",
      "         [-12.2371, -13.4372,  -4.8150,  ..., -12.5839, -12.9899, -13.1327],\n",
      "         ...,\n",
      "         [-11.6622, -11.9637,   6.3013,  ..., -11.2572, -11.3921, -11.7575],\n",
      "         [-11.7376, -12.0664,   8.1636,  ..., -11.5426, -12.1794, -11.7688],\n",
      "         [-11.8727, -12.0185,   9.1792,  ..., -11.6481, -12.0713, -12.1472]],\n",
      "\n",
      "        [[ -9.3787,  -9.9483,  -5.8674,  ...,  -9.3316,  -9.3203,  -9.6564],\n",
      "         [-10.4043, -11.4934,  -4.2820,  ..., -10.5106, -11.1547, -10.8406],\n",
      "         [-11.4911, -12.6713,  -5.4036,  ..., -11.6071, -11.9861, -11.8477],\n",
      "         ...,\n",
      "         [-12.8420, -12.9631,   5.1522,  ..., -12.4798, -12.5821, -13.1426],\n",
      "         [-12.5506, -12.9877,   6.2665,  ..., -12.5069, -12.7595, -13.1044],\n",
      "         [-11.9503, -12.3461,   7.8562,  ..., -11.9267, -11.7983, -12.1526]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.6133,  -9.3867,  -5.9080,  ...,  -8.7009,  -8.6688,  -8.9182],\n",
      "         [-11.1422, -12.2404,  -4.9796,  ..., -11.1088, -11.7894, -11.1069],\n",
      "         [-10.7069, -11.9516,  -5.4178,  ..., -10.8686, -11.6085, -11.3645],\n",
      "         ...,\n",
      "         [-11.8179, -12.1535,   7.0197,  ..., -11.7093, -11.8037, -12.2714],\n",
      "         [-11.6840, -12.3375,   8.2378,  ..., -11.7875, -12.4282, -12.7800],\n",
      "         [-11.4399, -12.0388,   8.9182,  ..., -11.8135, -11.8530, -12.2736]],\n",
      "\n",
      "        [[ -9.4777, -10.1491,  -6.3655,  ...,  -9.6533,  -9.4761,  -9.9945],\n",
      "         [-12.1722, -13.4430,  -5.0470,  ..., -12.3502, -12.7084, -12.4542],\n",
      "         [-12.2387, -13.4179,  -5.0890,  ..., -12.1226, -12.4257, -12.9232],\n",
      "         ...,\n",
      "         [-10.1215, -10.4674,   5.5738,  ..., -10.1189,  -9.9992, -10.2384],\n",
      "         [ -9.9412, -10.7999,   7.5744,  ..., -10.4550, -10.2318, -10.8020],\n",
      "         [ -9.9231, -10.5258,   9.0909,  ..., -10.4842,  -9.9972, -10.6134]],\n",
      "\n",
      "        [[ -8.6422,  -9.4361,  -6.5903,  ...,  -8.6659,  -8.6707,  -9.0487],\n",
      "         [-10.5450, -11.7515,  -5.5197,  ..., -10.6942, -11.2879, -10.6499],\n",
      "         [-10.7077, -12.0399,  -5.9536,  ..., -10.8821, -11.6387, -11.3225],\n",
      "         ...,\n",
      "         [-10.2903, -10.8016,   4.5399,  ..., -10.4473, -10.5036, -10.6613],\n",
      "         [-10.5320, -11.1800,   6.2394,  ..., -10.7225, -10.7182, -10.8590],\n",
      "         [-11.1958, -11.1854,   6.6559,  ..., -10.9232, -11.1120, -11.0245]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.0282,  -9.4357,  -6.2884,  ...,  -8.8403,  -9.5925,  -9.6355],\n",
      "         [-11.4314, -12.2318,  -4.9810,  ..., -11.5424, -12.1875, -11.7697],\n",
      "         [-12.6601, -13.7762,  -4.4570,  ..., -12.9826, -13.1732, -13.3994],\n",
      "         ...,\n",
      "         [-10.5316, -10.8023,   4.3132,  ..., -10.4012, -10.8684, -10.4726],\n",
      "         [-10.2859, -10.8766,   6.7245,  ..., -10.4575, -10.5604, -10.6104],\n",
      "         [-10.9971, -11.3209,   7.2599,  ..., -10.8646, -11.2677, -11.2291]],\n",
      "\n",
      "        [[ -9.6987, -10.7005,  -5.6722,  ...,  -9.8981,  -9.8984, -10.0938],\n",
      "         [-12.1939, -13.7295,  -4.5229,  ..., -12.3319, -12.8234, -12.4186],\n",
      "         [-11.9789, -13.4853,  -5.0372,  ..., -12.3079, -12.8157, -12.6030],\n",
      "         ...,\n",
      "         [-11.5700, -12.1526,   6.1946,  ..., -11.8099, -11.6268, -11.8062],\n",
      "         [-11.1971, -11.5293,   7.0718,  ..., -11.0018, -11.2228, -10.9727],\n",
      "         [-11.4953, -11.7749,   8.5157,  ..., -11.4258, -11.6204, -11.3662]],\n",
      "\n",
      "        [[ -9.2636,  -9.6451,  -6.4513,  ...,  -8.9993,  -9.6461,  -9.7503],\n",
      "         [-11.4335, -12.4384,  -5.0022,  ..., -11.4325, -12.3027, -11.6953],\n",
      "         [-12.6472, -13.8292,  -4.6860,  ..., -13.0123, -13.3204, -13.5174],\n",
      "         ...,\n",
      "         [-10.1527, -10.2720,   3.4961,  ...,  -9.9066, -10.3574, -10.0086],\n",
      "         [-10.8011, -11.3124,   5.8902,  ..., -10.6692, -11.1933, -10.8028],\n",
      "         [-10.1187, -10.1887,   6.0297,  ...,  -9.9004, -10.3176,  -9.9198]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.7456, -10.3052,  -6.2212,  ...,  -9.5695,  -9.4569, -10.2625],\n",
      "         [-11.0229, -12.1276,  -4.4952,  ..., -11.2290, -11.6359, -11.3873],\n",
      "         [-11.8500, -13.1264,  -5.4964,  ..., -12.0621, -12.2750, -12.2873],\n",
      "         ...,\n",
      "         [-10.7194, -11.0818,   5.5590,  ..., -10.7215, -10.6372, -10.9727],\n",
      "         [-11.3342, -12.0280,   6.6258,  ..., -11.3755, -11.5180, -11.6363],\n",
      "         [-10.6466, -11.6017,   8.0160,  ..., -10.9407, -11.2045, -11.1889]],\n",
      "\n",
      "        [[ -9.0591,  -9.9725,  -6.1265,  ...,  -9.2471,  -9.1383,  -9.6401],\n",
      "         [ -9.8523, -10.7263,  -5.3748,  ..., -10.2737, -10.2458, -10.4033],\n",
      "         [-11.0994, -11.5484,  -5.7648,  ..., -11.0393, -11.3005, -11.1195],\n",
      "         ...,\n",
      "         [-11.4215, -12.0202,   5.3825,  ..., -11.5627, -11.4130, -12.0606],\n",
      "         [-12.4434, -13.0494,   7.7479,  ..., -12.3145, -12.4807, -12.7349],\n",
      "         [-11.6556, -12.4499,   8.8025,  ..., -11.7915, -11.5192, -12.1697]],\n",
      "\n",
      "        [[ -8.6659,  -9.5014,  -6.3621,  ...,  -8.8013,  -9.2925,  -9.5001],\n",
      "         [-11.3060, -12.7310,  -5.1593,  ..., -11.4933, -12.1307, -11.6190],\n",
      "         [-11.8473, -13.6468,  -4.8501,  ..., -12.5728, -12.6151, -12.8938],\n",
      "         ...,\n",
      "         [-10.9119, -11.5632,   5.0503,  ..., -11.0543, -11.4203, -11.4103],\n",
      "         [-10.1774, -10.5842,   6.4623,  ..., -10.0060, -10.5415, -10.3426],\n",
      "         [-10.6688, -11.3497,   7.8496,  ..., -10.8110, -10.9833, -10.7900]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.9576,  -9.4636,  -6.3196,  ...,  -8.9156,  -9.4711,  -9.5677],\n",
      "         [-11.2597, -12.2997,  -5.0163,  ..., -11.3186, -12.0549, -11.5928],\n",
      "         [-12.2089, -13.5931,  -4.6622,  ..., -12.7787, -12.8824, -13.1529],\n",
      "         ...,\n",
      "         [-11.5320, -11.9570,   6.3587,  ..., -11.3611, -11.4707, -11.8438],\n",
      "         [-11.9552, -12.3462,   8.3271,  ..., -11.6665, -11.8648, -11.9612],\n",
      "         [-11.2719, -11.6994,   8.0826,  ..., -10.9286, -11.0835, -10.8135]],\n",
      "\n",
      "        [[ -9.2574, -10.0290,  -6.4250,  ...,  -9.4136,  -9.4041,  -9.7350],\n",
      "         [-11.6865, -12.8290,  -5.2846,  ..., -11.6667, -12.2697, -11.7978],\n",
      "         [-12.2228, -13.2355,  -4.9840,  ..., -11.9617, -12.4004, -12.5999],\n",
      "         ...,\n",
      "         [-11.1186, -11.3812,   3.8883,  ..., -11.2367, -11.1369, -11.0685],\n",
      "         [-11.1317, -11.5874,   5.5969,  ..., -11.2390, -11.0720, -11.3455],\n",
      "         [ -9.7759, -10.2223,   7.4664,  ...,  -9.7409, -10.0363,  -9.6160]],\n",
      "\n",
      "        [[ -9.3971, -10.2129,  -6.2634,  ...,  -9.3440, -10.0780, -10.0647],\n",
      "         [-11.6621, -12.9979,  -4.7680,  ..., -11.7608, -12.5958, -11.9097],\n",
      "         [-12.7425, -14.1801,  -4.4286,  ..., -13.2767, -13.4494, -13.5859],\n",
      "         ...,\n",
      "         [-11.6042, -11.9866,   4.9357,  ..., -11.6580, -11.6857, -11.4251],\n",
      "         [-11.6199, -11.7816,   7.3755,  ..., -11.2118, -11.3537, -10.9628],\n",
      "         [-10.5989, -10.7540,   7.2989,  ..., -10.1684, -10.4995, -10.3072]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.6227, -10.0579,  -5.9366,  ...,  -9.5905,  -9.6347,  -9.7917],\n",
      "         [-10.0378, -11.0677,  -4.3004,  ..., -10.4549, -11.0448, -10.5161],\n",
      "         [-11.7088, -12.8650,  -5.1364,  ..., -11.8932, -12.2976, -12.0222],\n",
      "         ...,\n",
      "         [-12.4875, -12.6452,   4.7063,  ..., -12.2779, -12.3875, -12.6968],\n",
      "         [-11.5890, -11.8006,   5.8357,  ..., -11.5749, -11.4678, -11.8775],\n",
      "         [-11.8255, -12.0230,   7.0851,  ..., -11.6833, -11.7087, -12.0202]],\n",
      "\n",
      "        [[ -9.1358,  -9.7741,  -6.1274,  ...,  -9.4858,  -9.3531,  -9.4533],\n",
      "         [-11.9229, -13.0552,  -4.8644,  ..., -12.1813, -12.4121, -11.9321],\n",
      "         [-12.4400, -13.7575,  -4.5696,  ..., -13.1248, -13.0746, -13.2355],\n",
      "         ...,\n",
      "         [-11.2535, -11.4164,   5.6508,  ..., -11.4632, -11.2619, -11.3487],\n",
      "         [-11.3990, -11.4417,   7.6099,  ..., -11.1897, -11.3366, -11.2821],\n",
      "         [-11.0235, -11.4752,   8.1502,  ..., -11.3866, -11.1387, -11.0213]],\n",
      "\n",
      "        [[ -9.3544,  -9.7879,  -6.4339,  ...,  -9.3435,  -9.8226, -10.2274],\n",
      "         [-11.7769, -12.7699,  -5.1163,  ..., -11.8753, -12.5195, -12.1839],\n",
      "         [-12.5387, -13.7972,  -4.6679,  ..., -13.1175, -13.2094, -13.6055],\n",
      "         ...,\n",
      "         [-10.9991, -11.3679,   6.1899,  ..., -11.0537, -11.3286, -11.4967],\n",
      "         [-11.3522, -11.7250,   8.2060,  ..., -11.3152, -11.5928, -11.7580],\n",
      "         [-10.9809, -11.3407,   8.5585,  ..., -11.1235, -11.2492, -11.4023]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.6045,  -9.1950,  -6.5793,  ...,  -8.8129,  -9.1061,  -9.3250],\n",
      "         [-11.3930, -12.5724,  -5.1660,  ..., -11.5687, -12.2269, -11.6502],\n",
      "         [-12.1853, -13.6599,  -4.9092,  ..., -12.7949, -12.9941, -13.1877],\n",
      "         ...,\n",
      "         [-11.9138, -11.9325,   5.2208,  ..., -11.8713, -12.3329, -11.7515],\n",
      "         [-11.5344, -11.8583,   6.2334,  ..., -11.4073, -11.7354, -11.4668],\n",
      "         [-11.0210, -11.7725,   7.5924,  ..., -10.7588, -10.9658, -11.3104]],\n",
      "\n",
      "        [[ -8.7666,  -9.9831,  -6.2069,  ...,  -9.1754,  -9.1445,  -9.6514],\n",
      "         [-11.4208, -12.8977,  -4.9696,  ..., -11.6471, -12.0168, -11.7419],\n",
      "         [-11.9230, -13.1575,  -4.8608,  ..., -11.8764, -12.3138, -12.4724],\n",
      "         ...,\n",
      "         [-10.8600, -11.4174,   4.5317,  ..., -11.0110, -10.9291, -11.0250],\n",
      "         [-11.6613, -12.4336,   6.5275,  ..., -11.9423, -11.5054, -11.5581],\n",
      "         [-10.9858, -11.5662,   7.0537,  ..., -10.8791, -10.7142, -11.3696]],\n",
      "\n",
      "        [[ -8.9595,  -9.6807,  -6.0870,  ...,  -9.1230,  -8.7758,  -9.2342],\n",
      "         [-11.1240, -12.2065,  -5.0224,  ..., -11.1366, -11.5245, -11.1555],\n",
      "         [-11.0620, -12.3476,  -5.0164,  ..., -11.3124, -11.7887, -11.7983],\n",
      "         ...,\n",
      "         [-11.7189, -12.1755,   6.6506,  ..., -11.7609, -12.0925, -12.0770],\n",
      "         [-11.3566, -12.2099,   8.3785,  ..., -11.5540, -11.7177, -11.9814],\n",
      "         [-11.6500, -12.4933,   8.6002,  ..., -11.6898, -11.7835, -12.2192]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.3590,  -9.8505,  -5.8541,  ...,  -9.1721,  -9.1582,  -9.3716],\n",
      "         [-10.3555, -10.8052,  -4.5713,  ..., -10.2035, -10.7515, -10.2592],\n",
      "         [-11.0810, -11.8951,  -5.4225,  ..., -10.9813, -11.6586, -11.5470],\n",
      "         ...,\n",
      "         [-10.4844, -10.6527,   3.4624,  ..., -10.4126, -10.3579, -10.8649],\n",
      "         [-10.9912, -11.1994,   5.6687,  ..., -11.1484, -10.9327, -11.5030],\n",
      "         [-11.2764, -11.4406,   6.8387,  ..., -11.3899, -11.0690, -11.7604]],\n",
      "\n",
      "        [[ -9.1933,  -9.9273,  -6.4045,  ...,  -9.0520,  -9.6699,  -9.7682],\n",
      "         [-11.8128, -13.0722,  -5.0637,  ..., -11.9020, -12.5560, -12.0127],\n",
      "         [-12.5216, -13.9584,  -4.6710,  ..., -12.9741, -13.1394, -13.3923],\n",
      "         ...,\n",
      "         [-11.6118, -11.9622,   6.1581,  ..., -11.5726, -11.9143, -11.9126],\n",
      "         [-11.3890, -11.5438,   6.9588,  ..., -11.3239, -11.4207, -11.3943],\n",
      "         [-11.7812, -12.5675,   8.8901,  ..., -11.8523, -11.7510, -12.0275]],\n",
      "\n",
      "        [[ -9.0970,  -9.6905,  -6.3009,  ...,  -9.0286,  -9.6981,  -9.8252],\n",
      "         [-11.5031, -12.6656,  -5.1349,  ..., -11.4726, -12.2863, -11.7905],\n",
      "         [-12.3219, -13.7843,  -4.8371,  ..., -12.9292, -13.1335, -13.3068],\n",
      "         ...,\n",
      "         [-11.9056, -12.1935,   3.6506,  ..., -12.1080, -11.6519, -11.9241],\n",
      "         [-11.8026, -11.7708,   5.4131,  ..., -11.9986, -11.9829, -12.0743],\n",
      "         [-11.4844, -11.4725,   6.9790,  ..., -11.6209, -11.9098, -11.5481]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.8604,  -9.6803,  -6.2000,  ...,  -9.1899,  -8.9761,  -9.4184],\n",
      "         [-11.4208, -12.6330,  -5.2219,  ..., -11.6899, -11.9997, -11.6004],\n",
      "         [-10.9740, -11.8587,  -5.5621,  ..., -10.8236, -11.7094, -11.3653],\n",
      "         ...,\n",
      "         [-12.3426, -12.8556,   1.9579,  ..., -12.3229, -12.2315, -12.6816],\n",
      "         [-12.2973, -12.7532,   3.4054,  ..., -12.1490, -12.6607, -12.1550],\n",
      "         [-12.3945, -12.4236,   5.0690,  ..., -12.2514, -12.3266, -12.4268]],\n",
      "\n",
      "        [[ -8.9715,  -9.5631,  -6.6026,  ...,  -8.7351,  -9.4760,  -9.9022],\n",
      "         [-11.6668, -12.6892,  -5.2920,  ..., -11.4733, -12.3487, -11.9585],\n",
      "         [-12.4373, -13.7292,  -4.6383,  ..., -12.8620, -13.0554, -13.5314],\n",
      "         ...,\n",
      "         [-10.2949, -10.6804,   2.4173,  ..., -10.1273, -10.5211, -10.5281],\n",
      "         [-10.7638, -10.9116,   4.0294,  ..., -10.5813, -10.7823, -10.9927],\n",
      "         [-10.8858, -11.1151,   6.1801,  ..., -10.4212, -10.7457, -10.9306]],\n",
      "\n",
      "        [[ -9.0491,  -9.3598,  -6.4734,  ...,  -8.8930,  -9.5073,  -9.5630],\n",
      "         [-11.6366, -12.5590,  -5.1413,  ..., -11.6851, -12.3157, -11.8196],\n",
      "         [-11.0023, -11.8925,  -5.1754,  ..., -11.0616, -11.6102, -11.1654],\n",
      "         ...,\n",
      "         [-11.1375, -11.1130,   1.8256,  ..., -10.7413, -11.0611, -10.7554],\n",
      "         [-11.6009, -11.4446,   3.3498,  ..., -11.2715, -11.3842, -11.5394],\n",
      "         [-11.0831, -11.4393,   5.1001,  ..., -11.2277, -11.7893, -11.8789]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.0329,  -9.4757,  -6.3392,  ...,  -8.8220,  -9.5922,  -9.7254],\n",
      "         [-11.4540, -12.3829,  -4.9930,  ..., -11.4559, -12.2641, -11.6601],\n",
      "         [-12.4717, -13.5089,  -4.5754,  ..., -12.8424, -13.0707, -13.3503],\n",
      "         ...,\n",
      "         [-10.6576, -10.4956,   5.5648,  ..., -10.8235, -10.9552, -10.8865],\n",
      "         [-10.8365, -10.6668,   5.5855,  ..., -10.5323, -10.9875, -10.7766],\n",
      "         [-10.0257, -10.6291,   8.3761,  ..., -10.2919, -10.5926, -10.6563]],\n",
      "\n",
      "        [[ -9.0425,  -9.9794,  -6.3091,  ...,  -9.3534,  -9.1526,  -9.4717],\n",
      "         [-11.7663, -13.1030,  -5.0549,  ..., -11.9689, -12.3131, -11.9680],\n",
      "         [-11.8532, -12.8854,  -5.0597,  ..., -11.7876, -12.1714, -12.4342],\n",
      "         ...,\n",
      "         [ -9.6383,  -9.9550,   5.6552,  ..., -10.0468,  -9.6945, -10.2148],\n",
      "         [-11.1223, -11.6397,   7.0441,  ..., -11.1707, -10.8641, -11.2584],\n",
      "         [-11.5729, -11.7030,   7.9811,  ..., -11.3275, -11.6123, -11.1968]],\n",
      "\n",
      "        [[ -8.7371,  -9.5364,  -6.3684,  ...,  -8.8023,  -9.2325,  -9.1886],\n",
      "         [-11.6241, -13.0876,  -5.1656,  ..., -11.8499, -12.6097, -11.9137],\n",
      "         [-11.7658, -13.2614,  -4.8702,  ..., -11.8411, -12.6374, -12.4446],\n",
      "         ...,\n",
      "         [-12.2088, -12.6712,   6.0128,  ..., -11.8837, -12.3638, -12.2437],\n",
      "         [-11.4431, -11.7502,   7.1724,  ..., -11.3601, -11.6718, -11.4549],\n",
      "         [-11.0239, -11.7496,   7.9300,  ..., -10.9550, -11.3459, -11.5113]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.1468,  -9.7637,  -6.3810,  ...,  -9.3921,  -9.0417,  -9.4626],\n",
      "         [-11.5554, -12.5816,  -5.2773,  ..., -11.7417, -11.9478, -11.7227],\n",
      "         [-11.8457, -12.7205,  -5.9678,  ..., -11.7952, -12.2101, -12.0849],\n",
      "         ...,\n",
      "         [-10.8629, -11.3524,   3.7934,  ..., -11.3771, -10.6257, -11.0876],\n",
      "         [-11.4391, -11.4823,   4.5677,  ..., -11.5387, -10.8510, -11.5503],\n",
      "         [-11.3369, -11.6919,   6.8996,  ..., -11.1678, -10.8997, -11.4011]],\n",
      "\n",
      "        [[ -8.7345,  -9.6125,  -6.2481,  ...,  -8.9205,  -8.8560,  -9.2874],\n",
      "         [-11.4630, -12.9693,  -5.0745,  ..., -11.7857, -12.1993, -11.8654],\n",
      "         [-11.6754, -12.9587,  -5.8634,  ..., -11.9652, -12.2540, -12.1448],\n",
      "         ...,\n",
      "         [-11.7014, -12.1175,   3.8562,  ..., -11.8442, -11.5511, -12.1730],\n",
      "         [-11.6705, -12.3019,   6.1290,  ..., -11.8800, -11.5403, -12.1593],\n",
      "         [-12.2040, -12.9544,   7.7951,  ..., -12.0523, -11.8230, -12.5941]],\n",
      "\n",
      "        [[ -9.5284,  -9.9751,  -6.1030,  ...,  -9.4592,  -9.3930, -10.0048],\n",
      "         [-11.9707, -13.0467,  -4.7450,  ..., -12.1109, -12.5776, -12.2439],\n",
      "         [-12.3841, -13.4028,  -5.4516,  ..., -12.3808, -12.9607, -12.7688],\n",
      "         ...,\n",
      "         [-10.5517, -10.9733,   4.8503,  ..., -10.4811, -10.6112, -10.5817],\n",
      "         [-10.3256, -10.3690,   5.6287,  ...,  -9.9587, -10.3641,  -9.8533],\n",
      "         [-10.7446, -11.0780,   7.2679,  ..., -10.7844, -11.0303, -10.9390]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.9609,  -9.5161,  -6.3331,  ...,  -8.9934,  -9.5103,  -9.6708],\n",
      "         [-11.2926, -12.3422,  -5.0619,  ..., -11.5187, -11.9701, -11.6407],\n",
      "         [-12.4158, -13.8670,  -4.5286,  ..., -13.1880, -13.1039, -13.4990],\n",
      "         ...,\n",
      "         [-10.8821, -11.5118,   4.9170,  ..., -10.8959, -11.2750, -10.9669],\n",
      "         [-11.2236, -11.6406,   6.5831,  ..., -11.2764, -11.2333, -11.5743],\n",
      "         [-10.9694, -11.3923,   8.3862,  ..., -11.0109, -11.6195, -11.6795]],\n",
      "\n",
      "        [[ -9.0502,  -9.3964,  -6.4645,  ...,  -8.8091,  -9.5285,  -9.5711],\n",
      "         [-11.5273, -12.3189,  -4.9479,  ..., -11.6165, -12.2223, -11.7798],\n",
      "         [-12.5330, -13.6780,  -4.5285,  ..., -12.9746, -13.1852, -13.3365],\n",
      "         ...,\n",
      "         [-10.8310, -10.8472,   3.8809,  ..., -10.8741, -10.9590, -10.8427],\n",
      "         [-10.2783, -10.4394,   5.8914,  ..., -10.0725,  -9.9927, -10.2448],\n",
      "         [-10.4390, -10.5407,   7.7606,  ..., -10.0082,  -9.9449, -10.5849]],\n",
      "\n",
      "        [[ -9.1825,  -9.9586,  -6.4029,  ...,  -9.4678,  -9.2331,  -9.6514],\n",
      "         [-11.3819, -12.6754,  -5.3450,  ..., -11.7966, -12.1226, -11.7493],\n",
      "         [-11.8048, -12.9497,  -5.0761,  ..., -12.0373, -12.3929, -12.5900],\n",
      "         ...,\n",
      "         [-10.0259, -10.7137,   5.2782,  ..., -10.0709, -10.0138, -10.7117],\n",
      "         [ -9.7183, -10.3036,   6.5933,  ...,  -9.8085,  -9.7322, -10.2180],\n",
      "         [-10.6942, -11.4257,   8.3931,  ..., -10.5131, -10.8986, -11.4324]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.0212,  -9.4532,  -6.5798,  ...,  -8.7334,  -9.4345,  -9.5493],\n",
      "         [-11.5447, -12.5573,  -5.1875,  ..., -11.5438, -12.1438, -11.5790],\n",
      "         [-12.5545, -13.9232,  -4.7047,  ..., -13.0412, -13.0272, -13.3102],\n",
      "         ...,\n",
      "         [-11.0857, -11.3988,   4.5300,  ..., -10.8264, -11.0172, -11.3029],\n",
      "         [-10.6277, -11.2014,   6.8434,  ..., -11.0178, -10.8507, -11.0626],\n",
      "         [-11.3196, -11.4055,   7.1408,  ..., -11.1662, -11.1453, -11.3433]],\n",
      "\n",
      "        [[ -8.4810,  -9.3585,  -6.3339,  ...,  -8.6483,  -8.7440,  -9.1498],\n",
      "         [-10.9992, -12.2733,  -5.2309,  ..., -11.2322, -11.8177, -11.6095],\n",
      "         [-11.6444, -12.7754,  -4.8419,  ..., -11.6292, -12.1750, -12.2846],\n",
      "         ...,\n",
      "         [-10.1554, -11.0365,   5.0406,  ..., -10.3200, -10.5170, -10.8532],\n",
      "         [-10.2556, -10.7209,   4.9413,  ..., -10.2643, -10.0752, -10.4610],\n",
      "         [ -9.8608, -10.4095,   7.7777,  ..., -10.0373,  -9.9307, -10.3192]],\n",
      "\n",
      "        [[ -9.5539,  -9.9506,  -6.4755,  ...,  -9.2151,  -9.6641,  -9.9513],\n",
      "         [-12.2157, -13.1268,  -5.1184,  ..., -12.0850, -12.7179, -12.2439],\n",
      "         [-12.7950, -14.0145,  -4.5608,  ..., -13.3072, -13.4733, -13.5974],\n",
      "         ...,\n",
      "         [-12.2670, -12.6369,   5.8212,  ..., -12.1039, -12.2364, -12.6388],\n",
      "         [-11.5711, -11.9670,   7.3230,  ..., -11.4549, -11.3740, -11.7284],\n",
      "         [-11.3651, -11.7587,   9.0788,  ..., -11.3263, -11.6600, -11.3585]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.9041,  -9.5261,  -6.4164,  ...,  -8.7570,  -9.4348,  -9.3768],\n",
      "         [-11.3301, -12.3925,  -5.0577,  ..., -11.2012, -11.9860, -11.2565],\n",
      "         [-12.0541, -13.4496,  -4.7655,  ..., -12.3546, -12.7451, -12.7719],\n",
      "         ...,\n",
      "         [-12.2418, -12.7552,   4.1054,  ..., -12.5075, -12.2969, -12.6284],\n",
      "         [-11.9591, -11.8583,   4.6619,  ..., -12.0354, -11.6196, -11.8443],\n",
      "         [-11.6902, -11.7170,   5.5825,  ..., -11.7754, -11.6557, -12.3184]],\n",
      "\n",
      "        [[ -8.9653,  -9.7575,  -6.0200,  ...,  -9.1191,  -8.7369,  -9.4591],\n",
      "         [-11.4672, -12.6718,  -5.1310,  ..., -11.5086, -11.9244, -11.6511],\n",
      "         [-11.3231, -12.4694,  -5.5694,  ..., -11.5559, -11.9566, -12.0302],\n",
      "         ...,\n",
      "         [-10.0553, -10.5127,   4.2680,  ..., -10.3946,  -9.9913, -10.4642],\n",
      "         [-10.5327, -10.6876,   6.7203,  ..., -10.6145, -10.5308, -10.9401],\n",
      "         [-10.9142, -11.0678,   9.2644,  ..., -10.9250, -10.9268, -10.9959]],\n",
      "\n",
      "        [[ -9.1513, -10.1120,  -6.3386,  ...,  -9.1970,  -9.6436,  -9.8671],\n",
      "         [-11.7707, -13.0447,  -4.9920,  ..., -11.8147, -12.3190, -11.8828],\n",
      "         [-12.4959, -14.0490,  -4.6713,  ..., -13.0541, -13.1777, -13.3471],\n",
      "         ...,\n",
      "         [-11.3869, -12.1706,   4.7718,  ..., -11.2488, -11.6499, -11.7376],\n",
      "         [-11.3009, -11.6416,   6.0450,  ..., -11.1855, -11.1777, -11.1265],\n",
      "         [-11.3384, -11.5567,   8.5828,  ..., -11.1622, -11.3208, -11.1872]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.2181,  -9.8032,  -6.3373,  ...,  -8.8199,  -9.2324,  -9.6788],\n",
      "         [-11.4676, -12.5728,  -4.9910,  ..., -11.3848, -12.1922, -11.6585],\n",
      "         [-12.5693, -13.8525,  -4.5998,  ..., -12.9879, -13.1958, -13.3691],\n",
      "         ...,\n",
      "         [-10.8134, -10.8065,   4.4820,  ..., -10.4329, -10.9751, -10.4681],\n",
      "         [-10.6206, -10.7768,   7.2605,  ..., -10.4543, -10.6535, -10.4650],\n",
      "         [-11.0592, -11.0827,   8.6161,  ..., -10.8577, -10.7509, -10.7411]],\n",
      "\n",
      "        [[ -8.7705,  -9.1596,  -6.3983,  ...,  -8.4460,  -9.3807,  -9.4360],\n",
      "         [-11.5080, -12.3313,  -5.0036,  ..., -11.4641, -12.2649, -11.8448],\n",
      "         [-11.5626, -12.3110,  -5.0405,  ..., -11.6603, -12.2980, -12.1052],\n",
      "         ...,\n",
      "         [-11.0821, -11.2674,   6.3469,  ..., -11.0788, -11.3248, -11.2399],\n",
      "         [-12.3639, -12.4343,   8.4416,  ..., -11.8367, -12.1958, -12.4006],\n",
      "         [-12.0286, -12.1463,   9.1723,  ..., -11.9205, -12.3054, -12.4206]],\n",
      "\n",
      "        [[ -8.7436,  -9.7421,  -6.2529,  ...,  -8.9971,  -9.0258,  -9.2824],\n",
      "         [-10.1864, -11.3469,  -5.1798,  ..., -10.8008, -10.7195, -10.9545],\n",
      "         [-10.5115, -11.4290,  -5.6302,  ..., -10.7365, -11.1415, -10.7566],\n",
      "         ...,\n",
      "         [-12.2507, -13.1326,   5.9634,  ..., -12.1327, -12.1322, -13.1668],\n",
      "         [-11.7969, -12.6474,   7.2280,  ..., -11.5567, -11.4048, -12.2060],\n",
      "         [-11.4619, -12.5197,   8.8939,  ..., -11.6280, -12.1604, -12.0952]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.8583,  -9.5358,  -6.1060,  ...,  -8.9824,  -8.6838,  -9.1310],\n",
      "         [-11.1913, -12.1580,  -5.0183,  ..., -11.2187, -11.5329, -11.0562],\n",
      "         [-11.6177, -12.6228,  -5.5492,  ..., -11.6211, -12.0109, -11.9890],\n",
      "         ...,\n",
      "         [-10.2712, -11.1618,   4.2793,  ..., -10.9653, -10.5707, -10.7722],\n",
      "         [ -9.7085, -10.2086,   4.6742,  ...,  -9.9797,  -9.9337,  -9.8176],\n",
      "         [ -9.9406, -10.5705,   7.3410,  ..., -10.3799, -10.3373, -10.2623]],\n",
      "\n",
      "        [[ -9.2264,  -9.8368,  -6.3393,  ...,  -8.9693,  -9.9272,  -9.9517],\n",
      "         [-12.0728, -13.0621,  -4.9458,  ..., -12.0274, -12.8036, -12.2816],\n",
      "         [-11.8272, -12.8844,  -4.9288,  ..., -11.8696, -12.6136, -12.3846],\n",
      "         ...,\n",
      "         [-11.0080, -11.1104,   5.3284,  ..., -10.8855, -11.2120, -11.4747],\n",
      "         [-10.8569, -10.8717,   7.9477,  ..., -10.8598, -10.9616, -10.7156],\n",
      "         [-11.0544, -11.1331,   9.8283,  ..., -11.1377, -11.3486, -11.2886]],\n",
      "\n",
      "        [[ -8.9602,  -9.7753,  -5.9968,  ...,  -8.8112,  -9.5510,  -9.5725],\n",
      "         [-11.4078, -12.5801,  -4.8084,  ..., -11.4002, -12.2333, -11.7004],\n",
      "         [-11.1199, -12.1414,  -5.3824,  ..., -11.1312, -11.9630, -11.7806],\n",
      "         ...,\n",
      "         [-11.1096, -11.5488,   3.8228,  ..., -10.9288, -11.1628, -10.8636],\n",
      "         [-10.5280, -11.2384,   5.8623,  ..., -10.4898, -10.9038, -10.7074],\n",
      "         [-10.8146, -11.3385,   7.2649,  ..., -10.6979, -10.8004, -10.8774]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.3574, -10.2578,  -6.1371,  ...,  -9.1163,  -9.6717,  -9.8910],\n",
      "         [-11.6186, -12.8344,  -4.9702,  ..., -11.5539, -12.1116, -11.7165],\n",
      "         [-12.4517, -13.8931,  -4.6392,  ..., -13.0952, -13.1921, -13.3049],\n",
      "         ...,\n",
      "         [-10.8450, -11.2627,   5.1999,  ..., -10.8848, -10.6819, -11.3211],\n",
      "         [-11.5392, -11.7544,   7.3197,  ..., -11.2881, -11.5197, -11.9052],\n",
      "         [-11.2072, -11.7023,   8.5180,  ..., -11.0859, -11.7036, -12.2004]],\n",
      "\n",
      "        [[ -9.3534,  -9.9874,  -5.8949,  ...,  -9.2394,  -9.2994,  -9.5115],\n",
      "         [-10.3050, -10.7170,  -4.4632,  ..., -10.0389, -10.7258, -10.3290],\n",
      "         [-10.4846, -11.3032,  -4.9884,  ..., -10.2422, -10.6337, -10.8068],\n",
      "         ...,\n",
      "         [-11.3922, -11.8197,   3.9252,  ..., -11.0062, -11.3402, -11.6641],\n",
      "         [-12.3132, -11.8357,   4.8196,  ..., -11.8006, -12.0517, -12.2548],\n",
      "         [-11.4066, -11.4728,   7.2479,  ..., -11.0338, -11.3886, -11.8011]],\n",
      "\n",
      "        [[ -9.3204, -10.1714,  -6.3991,  ...,  -9.1975,  -9.9858, -10.0449],\n",
      "         [-11.7677, -12.9909,  -5.1649,  ..., -11.7393, -12.4906, -12.1684],\n",
      "         [-11.8377, -13.0060,  -4.7211,  ..., -11.6730, -12.2181, -12.0053],\n",
      "         ...,\n",
      "         [-11.6744, -12.4891,   4.8295,  ..., -11.9418, -11.8267, -11.6011],\n",
      "         [-11.7597, -12.6363,   6.7799,  ..., -12.0251, -11.8251, -12.1524],\n",
      "         [-11.2460, -11.9037,   7.2386,  ..., -11.0716, -11.4512, -11.5863]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.5722,  -9.2463,  -5.8952,  ...,  -8.6448,  -8.4362,  -8.6837],\n",
      "         [-10.8388, -11.9822,  -4.9767,  ..., -10.8967, -11.4873, -10.7378],\n",
      "         [-11.5186, -12.5062,  -4.4687,  ..., -11.3175, -11.9608, -11.9052],\n",
      "         ...,\n",
      "         [-10.4302, -11.0839,   5.6679,  ..., -10.8387, -11.1343, -11.1476],\n",
      "         [-10.9837, -11.5936,   6.5848,  ..., -11.1722, -11.4737, -11.6222],\n",
      "         [-10.8384, -11.3434,   7.8961,  ..., -10.9393, -11.1881, -11.2160]],\n",
      "\n",
      "        [[ -8.8229,  -9.2647,  -6.0865,  ...,  -8.7707,  -8.6920,  -9.0742],\n",
      "         [-10.0155, -10.9721,  -4.3035,  ..., -10.1837, -10.6718, -10.4458],\n",
      "         [-11.0743, -12.2005,  -5.2119,  ..., -11.2277, -11.6949, -11.6875],\n",
      "         ...,\n",
      "         [ -9.4651,  -9.3022,   3.7276,  ...,  -9.5467, -10.0100,  -9.6010],\n",
      "         [ -9.3136,  -9.5475,   4.7232,  ...,  -9.5200,  -9.7074,  -9.4057],\n",
      "         [ -9.4467,  -9.6481,   7.0651,  ...,  -9.6040,  -9.6164,  -9.4862]],\n",
      "\n",
      "        [[ -8.6542,  -8.9903,  -6.3696,  ...,  -8.3919,  -9.2523,  -9.1763],\n",
      "         [-11.2492, -12.0618,  -5.0144,  ..., -11.2869, -12.0277, -11.4532],\n",
      "         [-12.6849, -13.6287,  -4.3904,  ..., -13.0974, -13.2705, -13.3162],\n",
      "         ...,\n",
      "         [-12.7080, -12.5883,   4.9717,  ..., -12.3919, -12.2348, -12.7174],\n",
      "         [-12.1144, -12.4228,   6.2631,  ..., -11.9465, -11.8228, -12.3381],\n",
      "         [-11.9091, -11.8088,   8.3420,  ..., -11.4903, -11.9805, -11.8989]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.1683,  -9.5346,  -6.4946,  ...,  -8.8651,  -9.7521,  -9.8388],\n",
      "         [-10.8965, -11.8225,  -4.7456,  ..., -10.4190, -11.4818, -10.9768],\n",
      "         [-11.8177, -12.8194,  -4.7606,  ..., -11.4881, -12.3373, -12.4233],\n",
      "         ...,\n",
      "         [-10.9570, -10.9958,   4.3702,  ..., -10.8972, -11.3476, -11.3090],\n",
      "         [-11.9219, -12.2583,   5.8564,  ..., -11.8890, -12.3492, -11.9897],\n",
      "         [-11.5230, -11.4716,   7.0739,  ..., -11.2276, -11.6832, -11.2192]],\n",
      "\n",
      "        [[ -9.5179, -10.0149,  -5.9656,  ...,  -9.3689,  -9.3896, -10.0213],\n",
      "         [-12.0466, -13.0007,  -4.8077,  ..., -12.0217, -12.4153, -12.1401],\n",
      "         [-11.9243, -12.9884,  -5.0301,  ..., -12.2053, -12.5657, -12.5573],\n",
      "         ...,\n",
      "         [-10.9547, -11.3544,   5.8359,  ..., -11.2254, -11.2281, -11.2504],\n",
      "         [-10.8170, -11.5138,   6.9655,  ..., -10.8507, -11.0386, -11.0698],\n",
      "         [-12.1104, -12.1851,   8.7418,  ..., -12.2075, -11.9756, -12.5146]],\n",
      "\n",
      "        [[ -9.2991,  -9.7868,  -6.0926,  ...,  -9.1704,  -9.7571,  -9.8197],\n",
      "         [-11.7877, -12.6975,  -4.7841,  ..., -11.7737, -12.2853, -11.8496],\n",
      "         [-12.8711, -14.0028,  -4.3203,  ..., -13.2943, -13.3555, -13.5871],\n",
      "         ...,\n",
      "         [-11.6760, -11.8854,   5.1390,  ..., -11.8353, -11.9997, -12.0094],\n",
      "         [-11.7864, -11.8662,   7.5714,  ..., -11.8001, -12.1715, -11.5865],\n",
      "         [-11.9933, -12.3689,   8.1205,  ..., -11.8245, -12.3132, -11.8609]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.4292, -10.1189,  -5.9330,  ...,  -9.3638, -10.0158, -10.0658],\n",
      "         [-10.8697, -11.8678,  -4.4881,  ..., -10.5349, -11.6572, -11.1368],\n",
      "         [-11.2865, -12.1999,  -4.6424,  ..., -11.1334, -11.7765, -11.3750],\n",
      "         ...,\n",
      "         [-10.7490, -10.9873,   2.5927,  ..., -10.4339, -10.7131, -10.7841],\n",
      "         [-11.0312, -11.8382,   4.8441,  ..., -11.1375, -11.2190, -11.1295],\n",
      "         [-10.8249, -11.4430,   6.5404,  ..., -10.6113, -10.7464, -11.4385]],\n",
      "\n",
      "        [[ -8.9744,  -9.5560,  -6.5142,  ...,  -8.7602,  -9.6864,  -9.7658],\n",
      "         [-11.6592, -12.6460,  -5.1086,  ..., -11.6437, -12.5004, -11.9466],\n",
      "         [-11.5554, -12.5422,  -5.0158,  ..., -11.5874, -12.3590, -12.0514],\n",
      "         ...,\n",
      "         [-11.2244, -11.2897,   3.1125,  ..., -11.3431, -11.3985, -11.5625],\n",
      "         [-11.7745, -11.9024,   5.3055,  ..., -11.5686, -11.9254, -12.0097],\n",
      "         [-11.2766, -11.2164,   6.3045,  ..., -11.3295, -11.3923, -11.4023]],\n",
      "\n",
      "        [[ -9.3355,  -9.5557,  -6.3676,  ...,  -8.8570,  -9.1933,  -9.5291],\n",
      "         [-12.3159, -13.1468,  -4.8793,  ..., -12.2610, -12.6664, -12.2955],\n",
      "         [-11.7363, -12.5919,  -5.1350,  ..., -11.7514, -12.4529, -12.2718],\n",
      "         ...,\n",
      "         [-11.0909, -11.2892,   3.9720,  ..., -11.5143, -11.5002, -11.3134],\n",
      "         [-11.0843, -11.1051,   4.5106,  ..., -11.2146, -11.1245, -11.1106],\n",
      "         [-12.0712, -12.1363,   6.7384,  ..., -12.2754, -11.9625, -12.1029]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.8918,  -9.2226,  -6.3431,  ...,  -8.5570,  -9.4310,  -9.4719],\n",
      "         [-11.5786, -12.3314,  -4.9325,  ..., -11.3238, -12.2646, -11.7206],\n",
      "         [-12.7577, -13.7429,  -4.4284,  ..., -13.0143, -13.2818, -13.5499],\n",
      "         ...,\n",
      "         [ -9.9950, -10.0146,   4.2327,  ..., -10.1851, -10.6332, -10.0726],\n",
      "         [ -9.8484, -10.0689,   5.0525,  ...,  -9.8068, -10.3801, -10.0592],\n",
      "         [ -9.6653,  -9.9425,   7.0913,  ...,  -9.6724, -10.0868,  -9.6250]],\n",
      "\n",
      "        [[ -8.8054,  -9.1444,  -6.6436,  ...,  -8.6139,  -9.4097,  -9.2414],\n",
      "         [-11.6273, -12.4268,  -5.0274,  ..., -11.5599, -12.2243, -11.6244],\n",
      "         [-11.5144, -12.1815,  -5.1789,  ..., -11.4720, -12.0911, -11.7839],\n",
      "         ...,\n",
      "         [-12.0849, -11.7883,   4.5724,  ..., -12.0095, -11.9581, -12.0366],\n",
      "         [-10.9941, -11.0522,   5.6993,  ..., -10.9565, -11.0964, -10.9105],\n",
      "         [-12.0631, -11.9684,   7.7879,  ..., -11.6051, -11.7578, -11.8328]],\n",
      "\n",
      "        [[ -9.0447,  -9.5389,  -6.3402,  ...,  -8.9954,  -9.7659,  -9.8715],\n",
      "         [-10.8861, -11.7271,  -4.7697,  ..., -10.8956, -11.4849, -11.6154],\n",
      "         [-11.1505, -11.9995,  -4.8082,  ..., -11.0593, -11.4947, -11.5263],\n",
      "         ...,\n",
      "         [-10.5621, -10.9523,   4.3480,  ..., -10.7634, -10.7763, -10.7431],\n",
      "         [-10.1613, -10.3545,   4.1161,  ..., -10.1567, -10.4620, -10.3213],\n",
      "         [ -9.6654, -10.0816,   6.7182,  ...,  -9.8557,  -9.8668, -10.1573]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.7723,  -9.1576,  -6.3648,  ...,  -8.5180,  -9.4258,  -9.2986],\n",
      "         [-11.4035, -12.2045,  -4.9969,  ..., -11.3475, -12.1262, -11.5555],\n",
      "         [-12.8208, -13.8652,  -4.4260,  ..., -13.2383, -13.3360, -13.5502],\n",
      "         ...,\n",
      "         [-12.0996, -12.2799,   6.5578,  ..., -11.7904, -12.0374, -12.0442],\n",
      "         [-12.1371, -12.1322,   8.3389,  ..., -12.1005, -12.3249, -12.4394],\n",
      "         [-11.8179, -11.6844,   9.6508,  ..., -11.6119, -11.9843, -11.8045]],\n",
      "\n",
      "        [[ -9.2628,  -9.5886,  -6.1075,  ...,  -9.1458,  -8.9951,  -9.6340],\n",
      "         [-10.4388, -11.4177,  -4.7859,  ..., -10.7857, -11.0022, -10.8563],\n",
      "         [-10.6976, -11.5094,  -5.4524,  ..., -11.1138, -11.3458, -11.5479],\n",
      "         ...,\n",
      "         [ -9.6861, -10.0148,   4.3775,  ..., -10.0656,  -9.6975, -10.0237],\n",
      "         [ -9.8997, -10.2254,   6.3505,  ..., -10.3804,  -9.9705, -10.4368],\n",
      "         [-10.1962, -10.3956,   8.2308,  ..., -10.1690,  -9.6899, -10.3078]],\n",
      "\n",
      "        [[ -8.1830,  -8.9601,  -6.2210,  ...,  -7.9892,  -8.9694,  -8.6443],\n",
      "         [-11.1374, -12.3042,  -4.7896,  ..., -11.1304, -11.8937, -11.2559],\n",
      "         [-12.4145, -13.7191,  -4.4508,  ..., -12.8215, -13.0579, -13.1613],\n",
      "         ...,\n",
      "         [-11.1840, -11.2200,   5.4293,  ..., -10.7897, -10.4085, -10.7178],\n",
      "         [-11.2873, -11.4365,   7.3423,  ..., -10.8527, -11.4787, -11.3162],\n",
      "         [-11.2621, -11.2570,   8.0648,  ..., -10.6937, -11.0616, -10.9271]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.0692,  -9.2448,  -6.5631,  ...,  -8.9613,  -9.6418,  -9.7288],\n",
      "         [-11.7604, -12.4346,  -5.3320,  ..., -11.5689, -12.3162, -11.7013],\n",
      "         [-12.9251, -13.9355,  -4.6760,  ..., -13.3925, -13.3491, -13.7240],\n",
      "         ...,\n",
      "         [-11.3731, -11.4230,   7.6580,  ..., -11.1276, -11.3062, -11.7136],\n",
      "         [-11.8521, -11.8944,   9.5476,  ..., -11.8827, -11.6755, -12.0844],\n",
      "         [-11.6879, -12.0534,  10.0429,  ..., -11.8974, -11.6284, -12.0088]],\n",
      "\n",
      "        [[ -9.4855, -10.2760,  -6.0823,  ...,  -9.2251,  -9.9581,  -9.9736],\n",
      "         [-11.6871, -12.9136,  -5.0358,  ..., -11.6859, -12.3906, -11.8887],\n",
      "         [-12.9252, -14.2161,  -4.5109,  ..., -13.3147, -13.4997, -13.7007],\n",
      "         ...,\n",
      "         [-10.4546, -10.4315,   4.3740,  ..., -10.5793, -10.7564, -10.4850],\n",
      "         [-10.9757, -10.6587,   6.3870,  ..., -10.8155, -11.0336, -10.9525],\n",
      "         [-11.6594, -11.8086,   8.3429,  ..., -11.6990, -11.8666, -11.6758]],\n",
      "\n",
      "        [[ -9.0790,  -9.5097,  -6.6928,  ...,  -8.7588,  -9.7848,  -9.7276],\n",
      "         [-11.2132, -11.9747,  -5.2313,  ..., -11.0718, -12.0937, -11.4260],\n",
      "         [-12.6877, -13.8239,  -4.5529,  ..., -13.0489, -13.4645, -13.5303],\n",
      "         ...,\n",
      "         [-10.4357, -10.4363,   5.4080,  ..., -10.3116, -10.4077, -10.3848],\n",
      "         [-10.1879, -10.4694,   7.9432,  ..., -10.1420, -10.4463, -10.3166],\n",
      "         [-10.6011, -10.8364,   9.6562,  ..., -10.4821, -11.4788, -11.3154]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.5184,  -9.2337,  -5.6037,  ...,  -8.5975,  -8.6706,  -9.0745],\n",
      "         [-11.3862, -12.6602,  -4.6619,  ..., -11.4046, -12.0355, -11.7168],\n",
      "         [-10.5329, -11.8570,  -5.2764,  ..., -10.6258, -11.5385, -11.1112],\n",
      "         ...,\n",
      "         [-11.5836, -11.6816,   6.4830,  ..., -11.1140, -11.4295, -12.1415],\n",
      "         [-12.3029, -12.8307,   8.4396,  ..., -11.9552, -12.5152, -13.0419],\n",
      "         [-12.4932, -12.7418,   9.1701,  ..., -11.9737, -12.4078, -12.9527]],\n",
      "\n",
      "        [[ -9.0333,  -9.3276,  -6.3612,  ...,  -8.8007,  -9.7983,  -9.6076],\n",
      "         [-11.8504, -12.4800,  -4.9207,  ..., -11.7368, -12.5798, -11.9322],\n",
      "         [-13.1705, -14.0653,  -4.3981,  ..., -13.3881, -13.7166, -13.8232],\n",
      "         ...,\n",
      "         [-12.2719, -12.4142,   6.0548,  ..., -12.0307, -12.4749, -12.2926],\n",
      "         [-12.4150, -12.8373,   7.4087,  ..., -11.9565, -12.4344, -12.3535],\n",
      "         [-12.5749, -12.6866,   9.4097,  ..., -12.1920, -12.4118, -12.3540]],\n",
      "\n",
      "        [[ -8.8247,  -9.6015,  -5.6377,  ...,  -8.7372,  -8.8973,  -9.2449],\n",
      "         [-11.4013, -12.6450,  -4.7738,  ..., -11.3949, -11.9886, -11.4755],\n",
      "         [-11.7339, -12.9721,  -4.9176,  ..., -11.9610, -12.3998, -12.2640],\n",
      "         ...,\n",
      "         [-10.6289, -10.6185,   4.5681,  ..., -10.2876, -10.5494, -10.2936],\n",
      "         [-10.4105, -10.6009,   6.1916,  ...,  -9.9690, -10.1558, -10.0858],\n",
      "         [-11.6180, -11.4782,   8.3944,  ..., -11.0996, -11.2416, -11.0403]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.3279,  -8.9809,  -5.8376,  ...,  -8.2513,  -8.3983,  -8.4741],\n",
      "         [-11.0798, -12.2451,  -4.7861,  ..., -11.2524, -11.6423, -11.0910],\n",
      "         [-11.7063, -12.7451,  -4.4035,  ..., -11.4510, -11.9982, -12.1579],\n",
      "         ...,\n",
      "         [-11.6038, -12.1572,   4.4128,  ..., -11.4863, -11.9529, -11.8586],\n",
      "         [-11.7123, -12.2542,   5.4508,  ..., -11.6653, -12.0873, -12.1616],\n",
      "         [-10.6108, -11.5500,   7.0230,  ..., -10.9980, -10.8561, -11.2287]],\n",
      "\n",
      "        [[ -8.9560,  -9.6813,  -6.1693,  ...,  -8.7549,  -9.5839,  -9.5326],\n",
      "         [-11.7960, -13.0164,  -4.7945,  ..., -11.7427, -12.4409, -11.9410],\n",
      "         [-11.8445, -12.9189,  -4.8096,  ..., -11.9337, -12.5705, -12.4018],\n",
      "         ...,\n",
      "         [-11.5930, -11.8753,   5.2475,  ..., -11.5685, -11.5423, -11.5929],\n",
      "         [-11.2260, -11.4610,   6.2632,  ..., -11.1394, -11.1477, -11.6360],\n",
      "         [-11.0261, -11.2183,   8.6972,  ..., -11.0856, -11.2238, -10.9237]],\n",
      "\n",
      "        [[ -8.7242,  -9.6062,  -6.2094,  ...,  -9.0356,  -9.0872,  -9.3472],\n",
      "         [-11.9843, -13.3057,  -4.8959,  ..., -12.2755, -12.6056, -12.1354],\n",
      "         [-11.9623, -13.3526,  -5.0469,  ..., -12.1559, -12.5091, -12.3165],\n",
      "         ...,\n",
      "         [-10.7191, -11.5458,   6.6133,  ..., -11.0329, -10.8643, -11.0799],\n",
      "         [-11.0721, -11.5493,   8.6363,  ..., -11.4437, -11.4884, -11.2126],\n",
      "         [-11.8574, -11.9969,   9.8817,  ..., -12.0206, -11.5746, -11.7639]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.1064,  -9.8286,  -6.0147,  ...,  -8.9599,  -9.8007,  -9.8262],\n",
      "         [-11.8542, -13.0138,  -4.5687,  ..., -11.9035, -12.6554, -12.1963],\n",
      "         [-13.1841, -14.6151,  -4.1144,  ..., -13.7778, -13.9074, -14.1220],\n",
      "         ...,\n",
      "         [-11.4858, -11.7117,   3.3172,  ..., -11.8220, -11.8858, -11.6489],\n",
      "         [-10.5371, -11.4500,   5.8522,  ..., -11.1209, -11.2313, -10.9623],\n",
      "         [-10.8310, -11.5018,   7.9525,  ..., -11.0639, -11.3307, -11.2326]],\n",
      "\n",
      "        [[ -9.1497,  -9.4827,  -6.3836,  ...,  -8.8968,  -9.8439,  -9.6933],\n",
      "         [-11.8023, -12.5383,  -4.8366,  ..., -11.6084, -12.3847, -11.8549],\n",
      "         [-13.0824, -14.1420,  -4.4687,  ..., -13.4716, -13.7054, -13.8921],\n",
      "         ...,\n",
      "         [-11.9137, -12.1707,   4.7114,  ..., -11.6977, -12.2038, -11.8534],\n",
      "         [-12.0041, -12.2535,   5.9139,  ..., -11.7631, -11.7940, -11.8717],\n",
      "         [-11.6669, -11.8000,   6.4381,  ..., -11.4449, -11.6121, -11.6041]],\n",
      "\n",
      "        [[ -8.9064,  -9.2964,  -6.3318,  ...,  -8.6337,  -9.5560,  -9.5513],\n",
      "         [-11.3732, -12.0880,  -4.9364,  ..., -11.2219, -12.0665, -11.5891],\n",
      "         [-13.0048, -13.8896,  -4.4495,  ..., -13.3135, -13.6264, -13.7335],\n",
      "         ...,\n",
      "         [-11.5646, -11.3916,   4.1217,  ..., -11.3822, -11.7945, -11.7316],\n",
      "         [-11.9042, -12.0127,   6.6897,  ..., -11.3113, -12.1466, -12.2034],\n",
      "         [-11.9536, -11.7673,   7.4804,  ..., -11.5008, -11.8358, -11.8571]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.4511,  -9.7506,  -5.7232,  ...,  -9.3791,  -9.4539,  -9.5685],\n",
      "         [-10.6142, -10.9001,  -4.3959,  ..., -10.2734, -11.0635, -10.4537],\n",
      "         [-10.5665, -11.1473,  -5.6149,  ..., -10.2400, -10.9108, -11.2596],\n",
      "         ...,\n",
      "         [-10.2387, -10.1276,   2.5892,  ..., -10.2150, -10.1403, -10.6876],\n",
      "         [-11.3100, -10.9209,   4.5854,  ..., -10.9444, -10.8085, -11.5666],\n",
      "         [-10.9984, -10.3658,   5.3846,  ..., -11.0840, -10.5397, -10.8468]],\n",
      "\n",
      "        [[ -8.9955,  -9.3352,  -6.2413,  ...,  -8.6108,  -9.5316,  -9.5820],\n",
      "         [-11.4060, -12.1911,  -5.0247,  ..., -11.3390, -12.1492, -11.5092],\n",
      "         [-12.8574, -13.8387,  -4.3595,  ..., -13.2273, -13.3923, -13.6471],\n",
      "         ...,\n",
      "         [-11.9715, -12.4028,   4.1304,  ..., -11.9026, -12.2024, -12.5417],\n",
      "         [-12.2859, -12.7340,   5.5993,  ..., -11.9967, -12.1719, -12.8209],\n",
      "         [-11.7362, -11.9853,   6.7571,  ..., -11.4255, -11.8793, -11.9115]],\n",
      "\n",
      "        [[ -9.4615, -10.3487,  -6.2738,  ...,  -9.3307, -10.0554,  -9.9861],\n",
      "         [-12.1992, -13.4396,  -4.8583,  ..., -12.0943, -12.6098, -12.1880],\n",
      "         [-13.0151, -14.4086,  -4.4361,  ..., -13.5600, -13.5339, -13.8400],\n",
      "         ...,\n",
      "         [-11.5020, -12.0382,   4.2204,  ..., -11.4084, -11.4662, -11.7907],\n",
      "         [-11.5359, -11.7394,   5.4222,  ..., -11.3714, -11.7328, -11.9489],\n",
      "         [-11.8569, -11.9215,   8.1247,  ..., -11.6431, -11.8934, -11.7709]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.7562,  -9.5723,  -6.0541,  ...,  -8.8570,  -8.8457,  -9.0373],\n",
      "         [-11.4195, -12.5382,  -5.0167,  ..., -11.4631, -11.8943, -11.2781],\n",
      "         [-11.3956, -12.6314,  -5.0886,  ..., -11.6541, -12.1800, -12.0416],\n",
      "         ...,\n",
      "         [-10.5188, -11.2593,   4.0729,  ..., -10.8510, -11.6090, -10.8877],\n",
      "         [-10.8083, -10.9530,   4.7771,  ..., -10.8946, -11.1620, -10.7694],\n",
      "         [-11.1577, -11.6985,   6.7430,  ..., -11.0518, -11.3088, -11.2013]],\n",
      "\n",
      "        [[ -9.1980,  -9.6428,  -6.0260,  ...,  -9.0728,  -9.7321,  -9.4486],\n",
      "         [-11.7956, -12.7876,  -4.7781,  ..., -11.7271, -12.4268, -11.6276],\n",
      "         [-13.0420, -14.2640,  -4.1919,  ..., -13.6270, -13.8252, -13.6298],\n",
      "         ...,\n",
      "         [ -9.4696,  -9.8070,   3.3103,  ...,  -9.4663,  -9.7867,  -9.1896],\n",
      "         [ -9.5669, -10.3036,   6.2430,  ...,  -9.9993, -10.1949,  -9.9346],\n",
      "         [-10.4811, -10.7572,   7.9847,  ..., -10.5133, -10.4739, -10.6395]],\n",
      "\n",
      "        [[ -9.1116,  -9.5606,  -5.8602,  ...,  -8.9701,  -9.0455,  -9.5245],\n",
      "         [-11.7042, -12.6493,  -4.8204,  ..., -11.6713, -12.1646, -11.8945],\n",
      "         [-11.8572, -12.8051,  -4.7783,  ..., -12.0692, -12.4707, -12.4476],\n",
      "         ...,\n",
      "         [-11.6145, -11.4531,   2.8868,  ..., -11.6798, -11.2630, -12.0540],\n",
      "         [-11.1247, -11.2532,   5.1482,  ..., -11.1836, -11.3260, -11.6388],\n",
      "         [-10.8245, -11.0987,   6.2578,  ..., -11.2296, -10.9102, -11.0587]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.6097,  -9.2727,  -6.2883,  ...,  -8.6997,  -9.3232,  -9.1399],\n",
      "         [-11.7074, -12.7868,  -5.0813,  ..., -11.6658, -12.5102, -11.7834],\n",
      "         [-12.5692, -13.9352,  -4.4948,  ..., -13.0476, -13.5056, -13.5295],\n",
      "         ...,\n",
      "         [-12.0741, -12.1303,   4.9335,  ..., -11.8065, -12.0958, -12.3230],\n",
      "         [-11.8210, -12.2086,   7.0537,  ..., -11.7865, -11.8054, -12.1933],\n",
      "         [-12.3472, -12.8825,   8.7488,  ..., -12.0828, -12.4754, -12.9024]],\n",
      "\n",
      "        [[ -9.0584,  -9.5179,  -6.3191,  ...,  -8.8272,  -9.5612,  -9.6251],\n",
      "         [-10.8942, -11.5807,  -5.4270,  ..., -10.6026, -11.2231, -10.7504],\n",
      "         [-10.5765, -11.0605,  -4.9813,  ..., -10.0351, -10.9617, -10.8085],\n",
      "         ...,\n",
      "         [-11.6623, -11.6098,   4.1862,  ..., -11.2557, -10.9842, -11.1686],\n",
      "         [-11.5104, -11.3949,   5.6206,  ..., -10.9947, -11.2017, -11.1330],\n",
      "         [-11.1399, -11.1265,   7.3169,  ..., -10.7474, -10.8980, -10.7993]],\n",
      "\n",
      "        [[ -9.2303,  -9.6831,  -6.4238,  ...,  -8.9725,  -9.8845,  -9.8693],\n",
      "         [-10.7590, -11.8602,  -4.6099,  ..., -10.8118, -11.8461, -11.1518],\n",
      "         [-11.8199, -12.8732,  -5.3142,  ..., -11.7366, -12.4766, -12.3895],\n",
      "         ...,\n",
      "         [-11.7053, -12.2913,   3.1294,  ..., -11.8205, -11.9981, -12.0767],\n",
      "         [-11.1148, -11.4583,   3.8186,  ..., -11.1442, -11.5907, -11.5104],\n",
      "         [-11.8731, -12.2057,   5.5363,  ..., -11.5873, -12.1673, -12.1310]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.7824,  -9.2634,  -6.4985,  ...,  -8.8251,  -9.6388,  -9.4402],\n",
      "         [-11.3145, -12.1957,  -5.2608,  ..., -11.4264, -12.2554, -11.4748],\n",
      "         [-13.0232, -14.1690,  -4.5989,  ..., -13.5928, -13.9039, -13.8463],\n",
      "         ...,\n",
      "         [-11.7487, -11.8188,   4.0522,  ..., -11.6799, -12.0224, -11.8239],\n",
      "         [-11.5252, -12.2094,   5.9513,  ..., -11.7229, -11.8354, -12.1447],\n",
      "         [-11.1100, -11.7218,   7.5976,  ..., -11.1639, -11.0506, -11.1833]],\n",
      "\n",
      "        [[ -8.6693,  -9.1355,  -6.4794,  ...,  -8.3488,  -9.4018,  -9.3849],\n",
      "         [-11.4704, -12.3676,  -5.0803,  ..., -11.2178, -12.0698, -11.6352],\n",
      "         [-12.5985, -13.5575,  -4.5913,  ..., -12.9455, -13.1760, -13.3657],\n",
      "         ...,\n",
      "         [-11.5819, -11.8555,   5.7543,  ..., -11.7242, -11.9209, -12.0016],\n",
      "         [-11.4777, -11.7103,   7.1331,  ..., -11.2584, -11.7819, -11.6272],\n",
      "         [-10.6378, -11.1311,   8.9650,  ..., -10.4993, -10.8937, -11.1019]],\n",
      "\n",
      "        [[ -9.0785,  -9.5652,  -5.7626,  ...,  -9.0483,  -9.1287,  -9.2531],\n",
      "         [-10.1516, -10.3535,  -4.6310,  ...,  -9.8316, -10.6807, -10.0273],\n",
      "         [-10.2281, -10.5813,  -5.7646,  ...,  -9.8835, -10.3607, -10.5641],\n",
      "         ...,\n",
      "         [-12.3016, -11.9554,   4.9164,  ..., -12.1263, -11.7857, -11.9132],\n",
      "         [-12.2837, -12.1963,   5.0507,  ..., -12.2381, -11.8858, -12.4488],\n",
      "         [-11.0684, -11.0511,   6.7456,  ..., -11.0651, -11.4177, -11.2909]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.1283,  -9.8104,  -6.0979,  ...,  -9.2201,  -9.7969,  -9.4118],\n",
      "         [-12.0615, -13.2763,  -4.8148,  ..., -12.1139, -13.0610, -12.1887],\n",
      "         [-12.2433, -13.1185,  -4.5576,  ..., -11.9585, -13.0261, -12.6824],\n",
      "         ...,\n",
      "         [-12.4018, -12.9911,   5.9665,  ..., -12.4444, -13.0261, -12.9241],\n",
      "         [-11.6265, -12.2097,   7.0150,  ..., -11.4194, -11.7014, -11.6045],\n",
      "         [-12.0554, -12.8313,   8.8276,  ..., -11.8517, -12.6022, -12.6976]],\n",
      "\n",
      "        [[ -9.6374,  -9.9696,  -6.0415,  ...,  -9.4175,  -9.3331, -10.1070],\n",
      "         [-12.3293, -13.2171,  -4.8141,  ..., -12.3020, -12.6777, -12.4588],\n",
      "         [-12.1564, -13.1302,  -5.1295,  ..., -12.3772, -12.6351, -12.7686],\n",
      "         ...,\n",
      "         [-10.7712, -10.5834,   4.5437,  ..., -10.7712, -10.4623, -10.7231],\n",
      "         [-10.6646, -10.3830,   5.2470,  ..., -10.7825, -10.6027, -10.5475],\n",
      "         [-10.3175, -10.7517,   7.4548,  ..., -10.9254, -10.9011, -10.9859]],\n",
      "\n",
      "        [[ -8.7029,  -9.5818,  -6.2585,  ...,  -8.9515,  -9.2332,  -9.6041],\n",
      "         [-11.8538, -13.2738,  -4.9969,  ..., -12.0825, -12.6713, -12.3554],\n",
      "         [-12.2912, -13.3074,  -4.6956,  ..., -12.1033, -12.5772, -12.8978],\n",
      "         ...,\n",
      "         [-11.0483, -11.2542,   3.9254,  ..., -10.7146, -11.0593, -11.2984],\n",
      "         [-10.9522, -11.4093,   4.8052,  ..., -10.9519, -10.8030, -11.1594],\n",
      "         [-10.4710, -10.9520,   5.3813,  ..., -10.2662, -10.5254, -10.6314]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.7352,  -9.0796,  -6.3153,  ...,  -8.4100,  -9.3478,  -9.4482],\n",
      "         [-11.3104, -12.1494,  -4.9522,  ..., -11.1610, -11.9929, -11.5087],\n",
      "         [-11.8158, -12.5059,  -4.7207,  ..., -11.7155, -12.3244, -12.1691],\n",
      "         ...,\n",
      "         [-12.0274, -12.4008,   6.8094,  ..., -11.8130, -11.8238, -12.2971],\n",
      "         [-12.4648, -12.7245,   8.8527,  ..., -11.9774, -12.5947, -12.6925],\n",
      "         [-12.5662, -12.4107,   9.5408,  ..., -12.3625, -12.3106, -12.3731]],\n",
      "\n",
      "        [[ -8.4507,  -8.6930,  -6.4850,  ...,  -8.1335,  -9.2409,  -9.2033],\n",
      "         [-11.4471, -12.1815,  -5.1539,  ..., -11.1742, -12.2845, -11.6622],\n",
      "         [-12.3723, -13.4276,  -4.4669,  ..., -12.7982, -13.1950, -13.3792],\n",
      "         ...,\n",
      "         [-10.2266, -10.3978,   7.9858,  ..., -10.0475, -10.7847, -10.6900],\n",
      "         [-10.5605, -10.9833,   9.2207,  ..., -10.3465, -11.1479, -11.1963],\n",
      "         [-11.0382, -11.0018,   9.8351,  ..., -10.8230, -11.3570, -11.2138]],\n",
      "\n",
      "        [[ -9.0985,  -9.4804,  -5.7453,  ...,  -8.9829,  -9.1446,  -9.0940],\n",
      "         [-10.1784, -10.5418,  -4.4160,  ...,  -9.8632, -10.7818, -10.1731],\n",
      "         [-10.0896, -10.5350,  -5.3767,  ...,  -9.6834, -10.4596, -10.7502],\n",
      "         ...,\n",
      "         [-11.5379, -11.4731,   5.7553,  ..., -11.4522, -11.5622, -11.6631],\n",
      "         [-11.3087, -11.5428,   8.6056,  ..., -11.3246, -11.6010, -11.8378],\n",
      "         [-11.6307, -11.5753,   8.9660,  ..., -11.5348, -11.5702, -12.1234]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.3101,  -9.4559,  -6.3911,  ...,  -8.9566,  -9.3130,  -9.5774],\n",
      "         [-12.0853, -12.9244,  -4.9340,  ..., -11.9935, -12.5172, -12.1700],\n",
      "         [-12.2208, -12.9559,  -4.8180,  ..., -12.3157, -12.6543, -12.6416],\n",
      "         ...,\n",
      "         [-10.3949, -10.1517,   6.4941,  ..., -10.2870, -10.1306, -10.8113],\n",
      "         [-10.9070, -11.1148,   7.8370,  ..., -10.8387, -10.9401, -11.1344],\n",
      "         [-11.4794, -11.2912,   8.8416,  ..., -11.1444, -11.0861, -11.5279]],\n",
      "\n",
      "        [[ -9.1676,  -9.6320,  -6.2604,  ...,  -9.1183,  -9.0844,  -9.7358],\n",
      "         [-11.2039, -12.2456,  -4.5995,  ..., -11.3778, -11.7155, -11.5631],\n",
      "         [-12.1714, -13.2253,  -5.2192,  ..., -12.2817, -12.4514, -12.6021],\n",
      "         ...,\n",
      "         [-10.5847, -11.2125,   3.6823,  ..., -10.8175, -10.8664, -11.1982],\n",
      "         [-10.4795, -11.1014,   5.2391,  ..., -10.4040, -10.3446, -10.5628],\n",
      "         [-11.1870, -11.1886,   7.2498,  ..., -10.7345, -11.0792, -10.9855]],\n",
      "\n",
      "        [[ -8.8044,  -9.1354,  -6.2775,  ...,  -8.4011,  -9.3647,  -9.3699],\n",
      "         [-11.8022, -12.5807,  -4.9489,  ..., -11.4735, -12.3538, -11.9202],\n",
      "         [-12.8777, -13.7983,  -4.3881,  ..., -13.1527, -13.5411, -13.6290],\n",
      "         ...,\n",
      "         [-12.4815, -12.3236,   6.6628,  ..., -11.8487, -12.1484, -12.6316],\n",
      "         [-11.7469, -12.2683,   9.5471,  ..., -11.2342, -11.4826, -12.1615],\n",
      "         [-12.4002, -12.3650,   9.7290,  ..., -11.7310, -11.9941, -12.5117]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.5405,  -8.8190,  -6.4060,  ...,  -8.4013,  -9.1978,  -9.3608],\n",
      "         [-11.6548, -12.4566,  -4.9698,  ..., -11.4225, -12.1470, -11.8370],\n",
      "         [-12.7660, -13.6591,  -4.6095,  ..., -13.0110, -13.3246, -13.6077],\n",
      "         ...,\n",
      "         [-11.1772, -11.0151,   6.6541,  ..., -10.8260, -11.1033, -11.3726],\n",
      "         [-10.1882, -10.2676,   7.7058,  ...,  -9.8915, -10.1980, -10.5144],\n",
      "         [-10.9433, -10.7450,   9.1245,  ..., -10.6392, -10.8648, -11.2611]],\n",
      "\n",
      "        [[ -9.5511,  -9.9186,  -6.3466,  ...,  -9.3263, -10.1740, -10.2042],\n",
      "         [-11.8959, -12.7586,  -5.0256,  ..., -11.6188, -12.5334, -12.0494],\n",
      "         [-13.0826, -14.1897,  -4.5280,  ..., -13.3990, -13.7115, -14.0344],\n",
      "         ...,\n",
      "         [-12.5059, -12.4853,   5.7326,  ..., -12.0546, -12.5137, -12.5333],\n",
      "         [-12.4514, -12.8727,   7.2248,  ..., -11.9073, -12.4054, -12.6380],\n",
      "         [-11.4616, -11.7989,   7.6862,  ..., -11.0498, -11.5858, -11.5151]],\n",
      "\n",
      "        [[ -9.0800,  -9.6816,  -5.9447,  ...,  -9.2346,  -9.3303,  -9.3946],\n",
      "         [-11.6375, -12.6761,  -4.8417,  ..., -11.6209, -12.1062, -11.6979],\n",
      "         [-11.2561, -12.4322,  -5.1697,  ..., -11.2156, -11.8662, -11.7631],\n",
      "         ...,\n",
      "         [-11.0281, -11.5828,   5.3226,  ..., -11.0524, -11.5436, -11.5350],\n",
      "         [-11.4559, -11.9111,   5.8423,  ..., -11.4117, -11.6272, -11.8046],\n",
      "         [-11.3178, -11.7625,   7.5185,  ..., -11.0658, -11.3872, -10.9456]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.1758,  -9.3435,  -6.6145,  ...,  -8.9785,  -9.6142,  -9.8345],\n",
      "         [-11.7418, -12.3222,  -5.2982,  ..., -11.5836, -12.1252, -11.7334],\n",
      "         [-13.0788, -13.9144,  -4.5825,  ..., -13.5359, -13.4172, -13.8718],\n",
      "         ...,\n",
      "         [-11.6500, -11.3123,   4.7053,  ..., -11.2739, -11.1992, -11.8623],\n",
      "         [-11.7610, -11.7432,   7.0136,  ..., -11.4676, -11.7559, -12.1799],\n",
      "         [-12.0092, -11.8795,   8.4990,  ..., -11.6851, -11.7058, -12.3762]],\n",
      "\n",
      "        [[ -8.8444,  -9.2082,  -6.5233,  ...,  -8.6608,  -9.4857,  -9.3931],\n",
      "         [-11.5081, -12.2728,  -5.0486,  ..., -11.2373, -12.0116, -11.5446],\n",
      "         [-12.5788, -13.4872,  -4.5657,  ..., -12.8249, -13.0951, -13.2058],\n",
      "         ...,\n",
      "         [-11.1994, -11.4273,   5.6497,  ..., -11.4324, -11.5838, -11.2021],\n",
      "         [-11.3317, -11.4279,   7.4332,  ..., -11.1225, -11.5994, -11.4121],\n",
      "         [-10.4407, -10.6395,   8.6840,  ..., -10.0097, -10.4702, -10.2139]],\n",
      "\n",
      "        [[ -9.1584,  -9.5897,  -6.3382,  ...,  -8.9611,  -9.8183,  -9.9922],\n",
      "         [-11.8950, -12.7908,  -4.7808,  ..., -11.7693, -12.5800, -12.2423],\n",
      "         [-13.3353, -14.4394,  -4.2510,  ..., -13.7599, -13.9698, -14.3213],\n",
      "         ...,\n",
      "         [-10.2894, -10.6649,   6.5517,  ..., -10.3731, -11.1177, -10.8018],\n",
      "         [-10.7854, -10.8582,   6.6819,  ..., -10.5896, -11.0150, -10.7975],\n",
      "         [-10.1121, -10.2549,   7.4781,  ..., -10.0291, -10.4026, -10.3041]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.0224,  -9.4344,  -6.2733,  ...,  -8.7439,  -9.6897,  -9.6603],\n",
      "         [-11.7787, -12.5356,  -4.8618,  ..., -11.5821, -12.3025, -11.9140],\n",
      "         [-11.0377, -11.6781,  -5.2550,  ..., -10.7178, -11.4951, -11.1351],\n",
      "         ...,\n",
      "         [-11.6972, -11.3979,   4.8102,  ..., -11.5490, -11.8384, -11.8331],\n",
      "         [-10.8149, -11.0631,   6.2371,  ..., -10.8548, -11.0988, -11.3856],\n",
      "         [-11.4378, -11.3683,   7.7781,  ..., -11.0347, -10.9632, -11.5260]],\n",
      "\n",
      "        [[ -8.6640,  -9.0444,  -6.5025,  ...,  -8.3541,  -9.3257,  -9.1994],\n",
      "         [-11.7330, -12.6182,  -4.9901,  ..., -11.5560, -12.4176, -11.9159],\n",
      "         [-11.1960, -11.9864,  -5.4955,  ..., -10.8635, -11.7009, -11.2477],\n",
      "         ...,\n",
      "         [-11.7311, -12.1013,   4.3419,  ..., -11.6562, -11.9957, -12.1039],\n",
      "         [-11.1471, -11.2957,   5.1766,  ..., -10.9016, -11.3783, -11.4567],\n",
      "         [-11.2126, -11.3422,   7.3153,  ..., -10.8456, -11.3509, -11.4742]],\n",
      "\n",
      "        [[ -9.1579,  -9.4090,  -6.5752,  ...,  -8.8030,  -9.7817,  -9.8517],\n",
      "         [-11.9156, -12.7032,  -5.1401,  ..., -11.5675, -12.5015, -11.9707],\n",
      "         [-13.2101, -14.2225,  -4.6048,  ..., -13.4973, -13.7717, -14.1222],\n",
      "         ...,\n",
      "         [-11.9384, -11.3690,   3.8871,  ..., -11.5831, -11.5690, -11.7750],\n",
      "         [-10.9245, -11.0843,   5.7033,  ..., -10.7570, -10.9489, -11.4353],\n",
      "         [-11.4945, -11.3045,   6.8618,  ..., -11.1270, -10.9918, -11.7839]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.5179,  -8.8770,  -6.6250,  ...,  -8.4138,  -9.1645,  -9.2339],\n",
      "         [-11.2004, -12.0232,  -5.1708,  ..., -11.1398, -11.9887, -11.4186],\n",
      "         [-11.3244, -11.9920,  -5.3090,  ..., -11.0398, -11.7363, -11.3657],\n",
      "         ...,\n",
      "         [-11.9757, -12.2603,   3.9881,  ..., -11.8129, -12.3727, -12.5881],\n",
      "         [-10.9470, -11.1440,   4.7348,  ..., -10.9049, -11.3771, -11.2533],\n",
      "         [-12.3158, -12.3320,   7.1631,  ..., -11.7945, -12.1219, -12.3751]],\n",
      "\n",
      "        [[ -8.7559,  -9.2088,  -6.4149,  ...,  -8.5324,  -9.6956,  -9.4979],\n",
      "         [-11.6073, -12.4642,  -4.9273,  ..., -11.4477, -12.3851, -11.8128],\n",
      "         [-11.6635, -12.3510,  -4.9747,  ..., -11.4185, -12.2941, -11.9220],\n",
      "         ...,\n",
      "         [-10.7854, -10.9379,   2.5885,  ..., -10.7304, -11.0429, -10.8580],\n",
      "         [-10.8676, -11.0980,   4.3944,  ..., -10.8731, -11.3849, -11.0273],\n",
      "         [-11.0866, -11.8421,   6.4887,  ..., -11.1944, -11.1813, -11.7050]],\n",
      "\n",
      "        [[ -8.5971,  -8.8837,  -6.4629,  ...,  -8.3100,  -9.2495,  -9.2550],\n",
      "         [-11.3884, -12.0785,  -5.0570,  ..., -11.0756, -12.0314, -11.5068],\n",
      "         [-12.5483, -13.4595,  -4.5234,  ..., -12.8473, -13.0752, -13.3151],\n",
      "         ...,\n",
      "         [-10.1643, -10.4864,   4.3711,  ..., -10.5840, -10.5875, -10.7484],\n",
      "         [-10.6728, -10.8957,   6.6637,  ..., -10.7367, -10.7607, -11.1470],\n",
      "         [-11.2829, -11.3821,   7.4624,  ..., -11.1444, -11.3991, -11.5520]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.2029,  -9.7435,  -6.2424,  ...,  -8.8912,  -9.8863,  -9.8232],\n",
      "         [-11.3428, -12.2738,  -4.9548,  ..., -11.0906, -12.0361, -11.5751],\n",
      "         [-12.6149, -13.7601,  -4.2489,  ..., -12.9238, -13.3339, -13.4624],\n",
      "         ...,\n",
      "         [-11.8146, -12.2629,   4.9217,  ..., -11.8065, -12.1580, -12.0683],\n",
      "         [-11.2074, -11.2236,   5.7249,  ..., -10.8736, -11.0885, -11.1170],\n",
      "         [-11.8140, -11.9260,   7.5961,  ..., -11.2866, -11.4846, -11.8030]],\n",
      "\n",
      "        [[ -9.2916,  -9.6855,  -6.4164,  ...,  -8.8401,  -9.2972,  -9.5631],\n",
      "         [-11.9752, -12.9088,  -4.9461,  ..., -11.7226, -12.5032, -11.9849],\n",
      "         [-12.6500, -13.7487,  -4.6092,  ..., -13.0930, -13.3197, -13.3967],\n",
      "         ...,\n",
      "         [-10.4725, -10.8316,   2.9946,  ..., -10.6530, -10.8872, -10.4640],\n",
      "         [ -9.6796, -10.0529,   3.2620,  ...,  -9.8263, -10.1087,  -9.6262],\n",
      "         [ -9.3965,  -9.8503,   5.5662,  ...,  -9.2802,  -9.5565,  -9.3364]],\n",
      "\n",
      "        [[ -8.9176,  -9.3459,  -6.3266,  ...,  -8.9621,  -8.9004,  -9.1889],\n",
      "         [-11.5124, -12.4401,  -5.1947,  ..., -11.4323, -11.8972, -11.4381],\n",
      "         [-11.2594, -12.3813,  -5.5221,  ..., -11.3937, -11.7863, -11.7723],\n",
      "         ...,\n",
      "         [-11.3491, -11.3056,   3.1945,  ..., -11.0641, -11.4098, -11.3746],\n",
      "         [-11.2452, -11.2442,   4.7444,  ..., -10.7773, -10.7818, -11.1187],\n",
      "         [-10.8869, -10.6248,   6.8580,  ..., -10.5677, -10.4142, -10.9506]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.8693,  -9.5792,  -6.3263,  ...,  -8.5616,  -9.1727,  -9.4824],\n",
      "         [-10.9779, -12.1021,  -5.3190,  ..., -10.7933, -11.5715, -11.3293],\n",
      "         [-11.0452, -12.0765,  -5.0618,  ..., -10.9593, -11.6850, -11.6660],\n",
      "         ...,\n",
      "         [-11.9241, -12.4788,   5.4084,  ..., -12.1254, -12.4999, -12.6200],\n",
      "         [-11.3081, -12.0511,   6.6068,  ..., -11.4746, -11.7203, -11.7753],\n",
      "         [-11.3747, -11.5073,   7.1387,  ..., -11.4385, -11.3600, -11.8213]],\n",
      "\n",
      "        [[ -8.9771,  -9.4605,  -6.1345,  ...,  -8.9452,  -9.6703,  -9.7614],\n",
      "         [-10.6672, -11.1998,  -5.1659,  ..., -10.2333, -11.3409, -10.8842],\n",
      "         [-11.5384, -12.1275,  -5.0553,  ..., -11.0297, -11.4608, -11.5049],\n",
      "         ...,\n",
      "         [-11.2945, -11.4520,   5.2093,  ..., -11.1225, -10.7887, -11.5250],\n",
      "         [-10.9887, -11.5270,   5.5691,  ..., -11.2512, -10.7804, -11.1339],\n",
      "         [-11.2594, -11.3046,   7.4363,  ..., -11.3128, -11.1583, -11.3291]],\n",
      "\n",
      "        [[ -9.4093,  -9.6646,  -6.3229,  ...,  -8.8518,  -9.5209,  -9.6655],\n",
      "         [-11.9277, -12.7950,  -5.0121,  ..., -11.6454, -12.4038, -12.0068],\n",
      "         [-12.0902, -12.8256,  -5.0538,  ..., -11.9697, -12.4387, -12.3019],\n",
      "         ...,\n",
      "         [-10.4173, -10.4531,   3.3030,  ..., -10.6854, -10.7345, -10.5099],\n",
      "         [-10.2057, -10.2921,   4.8445,  ...,  -9.9759, -10.5851, -10.2835],\n",
      "         [-11.3510, -10.7866,   6.5681,  ..., -10.7972, -11.0545, -11.0450]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -9.1296,  -9.6710,  -6.2510,  ...,  -9.0816,  -9.1092,  -9.4023],\n",
      "         [-11.0610, -12.1509,  -5.1514,  ..., -11.0046, -11.6996, -11.2959],\n",
      "         [-10.7867, -11.9779,  -5.4113,  ..., -10.7668, -11.5559, -11.6492],\n",
      "         ...,\n",
      "         [-11.9000, -12.5271,   6.4634,  ..., -11.8592, -12.1733, -12.7077],\n",
      "         [-12.6237, -13.0078,   6.7817,  ..., -12.2401, -12.2221, -13.1644],\n",
      "         [-12.0061, -12.1918,   9.0419,  ..., -11.5207, -11.8146, -12.2488]],\n",
      "\n",
      "        [[ -8.9041,  -9.2902,  -6.4064,  ...,  -8.8483,  -9.6954,  -9.6554],\n",
      "         [-11.1475, -12.1654,  -5.1140,  ..., -11.0280, -11.9258, -11.3560],\n",
      "         [-12.5857, -13.8129,  -4.5267,  ..., -13.1177, -13.2655, -13.4947],\n",
      "         ...,\n",
      "         [-10.5750, -11.2292,   4.2424,  ..., -10.8820, -11.1546, -10.8886],\n",
      "         [-10.2242, -10.8519,   5.4917,  ..., -10.5579, -10.4884, -10.6222],\n",
      "         [-10.4907, -10.4808,   7.6069,  ..., -10.6847, -10.7094, -10.4576]],\n",
      "\n",
      "        [[ -8.7105,  -9.1388,  -6.4888,  ...,  -8.5660,  -9.6401,  -9.4707],\n",
      "         [-11.1242, -11.9894,  -5.1551,  ..., -10.9803, -11.9375, -11.3814],\n",
      "         [-12.3693, -13.4922,  -4.4703,  ..., -12.8468, -13.1754, -13.1686],\n",
      "         ...,\n",
      "         [ -8.6322,  -9.2143,   2.3919,  ...,  -8.7110,  -9.1559,  -8.9252],\n",
      "         [ -8.6252,  -9.4429,   5.0925,  ...,  -8.7877,  -9.0816,  -9.0528],\n",
      "         [ -9.5806,  -9.9606,   6.8835,  ...,  -9.7366,  -9.8833,  -9.9991]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.3741,  -9.1791,  -6.1602,  ...,  -8.7562,  -8.9748,  -9.1805],\n",
      "         [-11.0604, -12.2787,  -5.0520,  ..., -11.3374, -11.9043, -11.4766],\n",
      "         [-12.5589, -13.9105,  -4.4611,  ..., -13.3425, -13.3649, -13.7587],\n",
      "         ...,\n",
      "         [-10.9752, -10.8978,   5.6250,  ..., -11.1823, -11.2974, -11.1169],\n",
      "         [-10.7747, -11.1116,   6.6202,  ..., -10.9507, -11.0727, -11.1383],\n",
      "         [-11.2014, -11.2012,   8.5503,  ..., -11.3237, -11.2135, -11.5811]],\n",
      "\n",
      "        [[ -8.9937,  -9.4250,  -6.5861,  ...,  -8.5184,  -9.2172,  -9.5305],\n",
      "         [-11.4071, -12.2496,  -5.2049,  ..., -11.1114, -12.0235, -11.4304],\n",
      "         [-11.9705, -13.0601,  -5.3135,  ..., -11.8646, -12.5597, -12.0853],\n",
      "         ...,\n",
      "         [-10.6255, -11.2751,   3.7564,  ..., -10.8597, -11.1092, -10.8149],\n",
      "         [-10.7930, -11.3291,   5.6970,  ..., -10.8588, -10.7820, -10.7054],\n",
      "         [-10.9696, -10.9996,   7.6607,  ..., -10.8282, -11.0080, -10.5817]],\n",
      "\n",
      "        [[ -8.2606,  -8.9142,  -6.2634,  ...,  -8.2640,  -8.5955,  -8.6196],\n",
      "         [-10.7470, -12.0526,  -5.0166,  ..., -10.7713, -11.5049, -10.8943],\n",
      "         [-10.7099, -12.0591,  -5.4312,  ..., -10.8946, -11.6433, -11.4104],\n",
      "         ...,\n",
      "         [-10.4880, -10.8789,   5.5108,  ..., -10.9035, -10.9528, -10.7967],\n",
      "         [-11.2390, -11.4443,   7.6794,  ..., -11.6021, -11.6475, -11.3892],\n",
      "         [-11.2463, -11.4593,   9.1343,  ..., -11.6385, -11.7668, -11.3078]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.7182,  -9.3133,  -6.1231,  ...,  -8.6920,  -9.3628,  -9.5014],\n",
      "         [-11.0511, -12.0631,  -4.9569,  ..., -10.9008, -11.7420, -11.2730],\n",
      "         [-12.1770, -13.3892,  -4.4830,  ..., -12.6246, -12.8074, -13.1827],\n",
      "         ...,\n",
      "         [-10.5681, -11.0256,   3.7342,  ..., -11.0117, -11.1780, -11.1057],\n",
      "         [-10.9088, -11.1671,   4.8461,  ..., -11.1480, -11.2564, -11.4612],\n",
      "         [-11.4828, -11.3673,   7.3400,  ..., -11.5346, -11.7292, -11.6592]],\n",
      "\n",
      "        [[ -8.8662,  -9.1116,  -6.3472,  ...,  -8.5587,  -9.4485,  -9.4126],\n",
      "         [-11.3057, -11.8959,  -5.0299,  ..., -11.0939, -11.7335, -11.3672],\n",
      "         [-12.6172, -13.4212,  -4.6180,  ..., -12.8477, -13.1482, -13.3544],\n",
      "         ...,\n",
      "         [-10.1944, -10.0930,   5.1856,  ...,  -9.9647, -10.5284, -10.1868],\n",
      "         [-10.2220, -10.3071,   7.3064,  ...,  -9.9460, -10.5833, -10.5181],\n",
      "         [-10.3197, -10.2885,   8.6142,  ..., -10.1529, -10.6738, -10.4901]],\n",
      "\n",
      "        [[ -8.6370,  -9.2930,  -5.9737,  ...,  -8.5559,  -8.7549,  -8.8843],\n",
      "         [ -9.7748, -10.0147,  -4.6522,  ...,  -9.4133, -10.2241,  -9.6787],\n",
      "         [ -9.3790,  -9.8131,  -5.7122,  ...,  -8.9785,  -9.7169, -10.0181],\n",
      "         ...,\n",
      "         [-10.7026, -10.5418,   4.5182,  ..., -10.7941, -10.9621, -11.1685],\n",
      "         [-10.7760, -10.9915,   6.4187,  ..., -10.9730, -11.0269, -11.3545],\n",
      "         [-10.3244, -10.4260,   8.0868,  ..., -10.4640, -10.4476, -10.9547]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.4185,  -8.8004,  -6.5724,  ...,  -8.2103,  -9.3041,  -9.1756],\n",
      "         [-10.8732, -11.8592,  -5.4603,  ..., -10.6439, -11.7772, -11.0112],\n",
      "         [-11.9894, -13.1413,  -4.8065,  ..., -12.4576, -12.6545, -12.8586],\n",
      "         ...,\n",
      "         [-10.8786, -11.2633,   5.4163,  ..., -10.3026, -11.1351, -11.2150],\n",
      "         [-11.4729, -11.0882,   7.5311,  ..., -10.6567, -11.3523, -11.4562],\n",
      "         [-11.0452, -11.0645,   9.2816,  ..., -10.7031, -11.4530, -11.4149]],\n",
      "\n",
      "        [[ -8.7519,  -8.9874,  -6.7046,  ...,  -8.5148,  -9.1908,  -9.4582],\n",
      "         [-11.3951, -12.0030,  -5.3118,  ..., -11.1755, -11.8155, -11.4646],\n",
      "         [-11.3371, -12.0202,  -5.1843,  ..., -11.3507, -11.7925, -11.8828],\n",
      "         ...,\n",
      "         [-10.1524, -10.5299,   5.3303,  ..., -10.5748, -10.8595, -11.1423],\n",
      "         [-10.1847, -10.3599,   6.6491,  ..., -10.2201, -10.4091, -10.6597],\n",
      "         [-10.2319, -10.5183,   8.5138,  ..., -10.3513, -10.9072, -10.8266]],\n",
      "\n",
      "        [[ -8.5624,  -9.1297,  -5.9751,  ...,  -8.4342,  -8.2594,  -8.7853],\n",
      "         [-10.8595, -11.8681,  -5.0316,  ..., -10.8242, -11.2644, -10.8679],\n",
      "         [-11.4807, -12.3152,  -4.4765,  ..., -11.2521, -11.7112, -12.0236],\n",
      "         ...,\n",
      "         [-12.3262, -12.4915,   4.3997,  ..., -11.9575, -12.4042, -12.2240],\n",
      "         [-12.0416, -12.1002,   6.5584,  ..., -11.6874, -11.7553, -12.1545],\n",
      "         [-11.2369, -11.5581,   7.8772,  ..., -11.1834, -10.8661, -11.3276]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.4996,  -9.2558,  -6.1689,  ...,  -8.7136,  -8.7620,  -9.1035],\n",
      "         [-10.9253, -12.0405,  -5.1537,  ..., -10.8956, -11.5697, -11.2147],\n",
      "         [-10.7564, -12.0271,  -5.3060,  ..., -11.0349, -11.5880, -11.4759],\n",
      "         ...,\n",
      "         [-12.4799, -12.7564,   4.8652,  ..., -12.3219, -12.5996, -12.4768],\n",
      "         [-11.3986, -11.8856,   7.0170,  ..., -11.9138, -11.6220, -11.8176],\n",
      "         [-11.1624, -11.6394,   8.6074,  ..., -11.2693, -11.1368, -11.3056]],\n",
      "\n",
      "        [[ -8.9057,  -9.3827,  -6.2287,  ...,  -8.8023,  -8.9108,  -9.5976],\n",
      "         [-11.5994, -12.6205,  -5.0162,  ..., -11.4423, -12.0092, -11.7075],\n",
      "         [-11.6835, -12.6547,  -5.1195,  ..., -11.7159, -12.1269, -12.1662],\n",
      "         ...,\n",
      "         [-10.9314, -11.2128,   4.1468,  ..., -10.9778, -11.5853, -11.4794],\n",
      "         [-10.2523, -10.3325,   4.4680,  ..., -10.5036, -10.8729, -10.5692],\n",
      "         [-10.6705, -11.1836,   6.5652,  ..., -11.0913, -11.2340, -11.1683]],\n",
      "\n",
      "        [[ -8.2610,  -9.2057,  -6.4222,  ...,  -8.6926,  -9.0893,  -9.2012],\n",
      "         [-10.7779, -12.2179,  -5.0008,  ..., -11.0919, -11.7490, -11.2292],\n",
      "         [-10.6575, -11.9208,  -5.1833,  ..., -11.0144, -11.6955, -11.5089],\n",
      "         ...,\n",
      "         [-11.6984, -12.0935,   5.1135,  ..., -11.9024, -12.1087, -11.9772],\n",
      "         [-10.5207, -10.8665,   7.5058,  ..., -10.5760, -10.3404, -10.9780],\n",
      "         [-11.2807, -11.7537,   8.4690,  ..., -11.5516, -11.4850, -11.8169]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.6156,  -9.0793,  -6.5579,  ...,  -8.3710,  -9.4387,  -9.3150],\n",
      "         [-10.8690, -11.7859,  -5.3770,  ..., -10.5242, -11.6772, -10.9805],\n",
      "         [-12.1110, -13.1955,  -4.7626,  ..., -12.5632, -12.9815, -12.9378],\n",
      "         ...,\n",
      "         [-10.6396, -10.7417,   4.5575,  ..., -10.4466, -10.8668, -10.7697],\n",
      "         [-10.4748, -11.1573,   6.5963,  ..., -10.7316, -11.0916, -11.0965],\n",
      "         [-10.2324, -10.5300,   8.2082,  ..., -10.4627, -10.5811, -10.5886]],\n",
      "\n",
      "        [[ -8.5729,  -8.8249,  -6.5160,  ...,  -8.4516,  -8.9355,  -8.9509],\n",
      "         [-11.0209, -11.8602,  -5.2681,  ..., -10.9026, -11.5819, -11.1438],\n",
      "         [-12.5907, -13.6520,  -4.4798,  ..., -13.1040, -13.1653, -13.4964],\n",
      "         ...,\n",
      "         [-11.0003, -10.9771,   5.1889,  ..., -11.0308, -10.8644, -11.0939],\n",
      "         [-10.0961, -10.0877,   6.3026,  ..., -10.5060, -10.3444, -10.0149],\n",
      "         [-10.4756, -10.5517,   8.0888,  ..., -10.7165, -10.6512, -10.6299]],\n",
      "\n",
      "        [[ -8.1793,  -8.8517,  -5.9269,  ...,  -8.0314,  -7.9897,  -8.1279],\n",
      "         [ -7.7540,  -8.7139,  -5.2825,  ...,  -7.5368,  -8.4619,  -8.0045],\n",
      "         [ -9.4052,  -9.6563,  -5.4705,  ...,  -9.0044,  -9.2342,  -9.5812],\n",
      "         ...,\n",
      "         [ -9.8917, -10.1495,   3.3601,  ..., -10.0833, -10.0860, -10.5659],\n",
      "         [ -9.1905,  -9.5348,   4.7347,  ...,  -9.4642,  -9.5710,  -9.6326],\n",
      "         [ -9.3602,  -9.8070,   6.0780,  ...,  -9.6258,  -9.4596, -10.0430]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.9726,  -9.7183,  -6.5057,  ...,  -9.2464,  -9.2096,  -9.4066],\n",
      "         [-11.4024, -12.5969,  -5.4687,  ..., -11.3998, -11.9016, -11.4904],\n",
      "         [-11.1678, -12.3401,  -5.9055,  ..., -11.1579, -11.7813, -11.6881],\n",
      "         ...,\n",
      "         [-12.4956, -13.0666,   4.6704,  ..., -12.4063, -12.2570, -13.4135],\n",
      "         [-11.2696, -11.9211,   5.8791,  ..., -11.3499, -11.3727, -11.9950],\n",
      "         [-11.8441, -12.3516,   8.1059,  ..., -11.7274, -11.8947, -12.6743]],\n",
      "\n",
      "        [[ -8.8106,  -9.2653,  -5.8333,  ...,  -8.7365,  -8.7474,  -8.7808],\n",
      "         [ -9.3726,  -9.6080,  -4.3767,  ...,  -8.9817,  -9.6771,  -9.2436],\n",
      "         [ -9.6581, -10.0485,  -5.6585,  ...,  -9.4284,  -9.7817, -10.0781],\n",
      "         ...,\n",
      "         [-11.1544, -11.0438,   2.8012,  ..., -11.4226, -11.1135, -11.4749],\n",
      "         [-10.2312,  -9.9685,   4.0628,  ..., -10.2872, -10.2144, -10.2708],\n",
      "         [-10.1809, -10.4939,   7.4912,  ..., -10.5233, -10.4749, -10.9680]],\n",
      "\n",
      "        [[ -8.9887,  -9.3342,  -6.4569,  ...,  -8.7344,  -9.4760,  -9.5813],\n",
      "         [-11.4744, -12.2392,  -4.9752,  ..., -11.2795, -12.0947, -11.6646],\n",
      "         [-11.2340, -11.8759,  -5.1501,  ..., -11.1557, -11.9219, -11.7254],\n",
      "         ...,\n",
      "         [-11.3239, -11.6167,   4.0129,  ..., -11.2084, -11.7845, -12.0457],\n",
      "         [-10.5794, -10.5566,   5.1994,  ..., -10.4800, -10.8123, -10.8814],\n",
      "         [-10.8956, -11.0189,   6.3926,  ..., -10.7149, -11.2402, -11.1697]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.9180,  -9.3082,  -6.1400,  ...,  -8.7058,  -9.6913,  -9.6523],\n",
      "         [-11.5346, -12.2368,  -4.7561,  ..., -11.2248, -12.1010, -11.7330],\n",
      "         [-12.8896, -13.7216,  -4.3109,  ..., -13.1511, -13.4492, -13.7006],\n",
      "         ...,\n",
      "         [-11.6116, -11.7206,   6.1387,  ..., -11.2419, -11.6549, -11.7055],\n",
      "         [-11.6671, -11.3602,   7.2658,  ..., -11.2432, -11.7210, -11.4834],\n",
      "         [-11.6510, -12.1049,   8.5082,  ..., -11.4411, -12.0364, -11.7680]],\n",
      "\n",
      "        [[ -8.4778,  -8.8653,  -6.4399,  ...,  -8.4700,  -9.0987,  -8.9602],\n",
      "         [-10.9910, -11.7927,  -5.1199,  ..., -10.8453, -11.5892, -11.0576],\n",
      "         [-11.3984, -12.2221,  -5.0312,  ..., -11.5148, -11.9973, -11.7269],\n",
      "         ...,\n",
      "         [ -9.4308,  -9.6638,   5.0445,  ...,  -9.6254,  -9.9192, -10.1845],\n",
      "         [ -9.9962, -10.3613,   6.0653,  ..., -10.1966, -10.3301, -10.5547],\n",
      "         [-10.8610, -10.9859,   7.5993,  ..., -10.4993, -10.9951, -11.1506]],\n",
      "\n",
      "        [[ -9.1237,  -9.8333,  -6.2659,  ...,  -9.0116, -10.0897,  -9.8437],\n",
      "         [-11.3137, -12.2934,  -4.9982,  ..., -11.0960, -12.2560, -11.3890],\n",
      "         [-12.8247, -13.9869,  -4.3430,  ..., -13.1426, -13.7185, -13.6948],\n",
      "         ...,\n",
      "         [-12.6204, -13.3818,   4.9126,  ..., -12.6452, -13.0538, -12.6533],\n",
      "         [-11.9834, -12.6184,   6.0382,  ..., -11.7457, -12.1321, -12.5431],\n",
      "         [-11.5955, -12.2701,   6.8853,  ..., -11.4608, -12.0106, -12.0230]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "train output :  tensor([[[ -8.8892,  -9.7360,  -6.1497,  ...,  -8.7034,  -9.6099,  -9.4962],\n",
      "         [-11.4469, -12.7302,  -4.7610,  ..., -11.3489, -12.3107, -11.6792],\n",
      "         [-12.7740, -14.0417,  -4.2474,  ..., -13.2205, -13.4625, -13.5613],\n",
      "         ...,\n",
      "         [-10.8636, -11.1657,   3.5727,  ..., -11.2126, -11.3594, -11.3496],\n",
      "         [-11.6976, -11.9754,   5.7180,  ..., -11.5111, -11.9304, -11.6606],\n",
      "         [-10.9206, -11.1489,   7.4967,  ..., -10.8745, -11.1621, -10.9690]],\n",
      "\n",
      "        [[ -8.8483,  -9.3802,  -6.4311,  ...,  -8.9801,  -8.7611,  -9.2304],\n",
      "         [-11.2486, -12.3105,  -5.4530,  ..., -11.3640, -11.8092, -11.5451],\n",
      "         [-11.6948, -12.6538,  -4.9340,  ..., -11.6505, -12.1564, -12.3387],\n",
      "         ...,\n",
      "         [-12.1401, -12.3565,   4.8327,  ..., -12.2594, -12.1839, -12.4687],\n",
      "         [-11.2519, -11.9540,   6.6159,  ..., -11.3681, -11.2078, -12.0397],\n",
      "         [-10.9232, -11.0567,   8.3355,  ..., -10.8388, -10.5588, -11.4829]],\n",
      "\n",
      "        [[ -9.1637,  -9.6815,  -5.9838,  ...,  -9.1066,  -8.9284,  -9.3827],\n",
      "         [-10.7140, -11.5174,  -5.3939,  ..., -10.5733, -11.0987, -10.7655],\n",
      "         [-10.4664, -11.2873,  -5.5206,  ..., -10.5637, -11.0972, -11.2078],\n",
      "         ...,\n",
      "         [-10.9588, -11.4019,   3.2664,  ..., -11.0870, -10.9159, -11.3462],\n",
      "         [-10.7471, -10.5725,   4.2870,  ..., -10.3160, -10.5490, -10.6564],\n",
      "         [-10.4416, -10.1005,   5.7403,  ..., -10.0731,  -9.9619, -10.3448]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.4389,  -9.0097,  -6.3839,  ...,  -8.5699,  -8.5960,  -8.7557],\n",
      "         [ -9.7526, -10.5052,  -5.2299,  ..., -10.2643, -10.4001, -10.2370],\n",
      "         [-10.8819, -11.4204,  -5.8934,  ..., -10.7459, -11.3644, -11.0177],\n",
      "         ...,\n",
      "         [-12.0775, -12.4071,   4.9726,  ..., -11.9274, -12.1076, -12.3750],\n",
      "         [-11.2900, -11.6936,   7.3277,  ..., -11.1183, -11.3654, -11.6829],\n",
      "         [-11.1842, -11.6753,   8.3983,  ..., -10.8908, -11.1142, -11.8321]],\n",
      "\n",
      "        [[ -8.2245,  -8.8653,  -5.8663,  ...,  -7.9787,  -8.3112,  -8.5968],\n",
      "         [ -8.8769,  -9.4387,  -4.7374,  ...,  -8.3129,  -9.2531,  -9.1554],\n",
      "         [ -9.5558, -10.2629,  -5.1048,  ...,  -9.4415, -10.1040, -10.0171],\n",
      "         ...,\n",
      "         [-10.2258, -10.1703,   3.9665,  ..., -10.1881, -10.4021, -10.5112],\n",
      "         [ -9.2463,  -9.2080,   5.1048,  ...,  -9.1628,  -9.6318,  -9.3208],\n",
      "         [ -9.6202,  -9.4687,   7.1029,  ...,  -9.4674,  -9.7821,  -9.7546]],\n",
      "\n",
      "        [[ -9.2399,  -9.4892,  -6.1864,  ...,  -9.0861,  -9.8402,  -9.9816],\n",
      "         [-11.4778, -12.2218,  -4.9488,  ..., -11.4745, -12.1009, -11.7578],\n",
      "         [-12.8055, -13.7044,  -4.6275,  ..., -13.1772, -13.2461, -13.6255],\n",
      "         ...,\n",
      "         [-10.6366, -10.5474,   3.4358,  ..., -10.7503, -10.9107, -11.2105],\n",
      "         [-10.5898, -10.5494,   4.3182,  ..., -10.5246, -10.4312, -10.8114],\n",
      "         [-10.4663, -10.5317,   6.7530,  ..., -10.5283, -10.8392, -10.9445]]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, loss_fn, device)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[1;32mIn[71], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Loss 계산\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  훈련 시작\n",
    "NUM_EPOCHS = 5\n",
    "STATEDICT_PATH = 'seq2seq-chatbot-kor.pt'\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    \n",
    "    val_loss = evaluate(model, test_loader, loss_fn, device)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), STATEDICT_PATH)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early Stop\n",
    "    es(loss)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "                   \n",
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "torch.save(model.state_dict(), f'seq2seq-chatbot-kor-{best_loss:.4f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[1132, 1133,   22,   48,    7,   50,   22,  159,   73,   16,  389,  116,\n",
      "           64,   13,   46,   11,   12,   13,   14,  206,  207,  208,   19,    2,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   7,   97,   49,  363,   51,  772,   35,  189,   55,  243,  101,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [1859, 2854, 1011, 1254, 3066,   22,  435,  297,  736,   16, 3099,   22,\n",
      "           73, 2759,   73,   46,  683,   35,    7,   41,  739,   12,   13,  440,\n",
      "          101,    2,    0,    0,    0,    0],\n",
      "        [ 344,  187,    7,  159,   22,   23,   98,  135,   49,  707,   22,  116,\n",
      "          411,  284,  273,  116,  737,   12,   13,  440,  101,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 707,   12,    7,  159,    4,   98,  167,   56, 1851,   64,   13,   14,\n",
      "           15,   16,   17,  171,   19,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   7,   49,   50,   22,  159,   73,   93,   23,   98,  135,   49,   52,\n",
      "         1263,   12,  567,  440,  101,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 600,   56,  682,   16,  344,  187,    7,   97,   22,   23,   98,   97,\n",
      "           22,   43,   99,  100,  101,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   7,  562,  164,   49,  317,   25,   50,   35,  242,  243,  101,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [1760,  187, 1761,   51,  143,   35,  113,  380, 1223, 2075,   22,  258,\n",
      "         1224,  116,   64,   13,  440,  101,    2,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   7,  159,   22,   23,  108, 1388,  116,  641,   22,  206,  207,  208,\n",
      "           19,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "질문   : 식이 요법 을 통해 치매 증상 을 예방 하 고 개선 할 수 있 는 방법 이 있 는지 알려 주 세요 .\n",
      "답변   : 치매 예방 을 위해 건강 한 생활 습관 을 유지 하 는 것 은 매우 중요 합니다 . 기억력 향상 을 위해 일상 적 으로 규칙 적 인 운동\n",
      "예측답변: 치매 예방 을 위해 는 적 인 건강 검진 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위해 는 적 인 운동 과 건강 한\n",
      "==============================\n",
      "질문   : 치매 검진 의 시간 과 비용 은 어느 정도 인가요 ?\n",
      "답변   : 치매 의 진단 은 어떻게 이루 어질 까요 ? 치매 의 진단 은 다음 과 같 은 과정 을 거칩니다 . 치매 를 의심 하 는 증상 이 있\n",
      "예측답변: 알츠하이머병 은 현재 까지 근본 적 인 치료법 은 개발 되 는 약물 입니다 . 현재 까지 근본 적 인 치료법 은 개발 되 는 약물 이 개발 되 는\n",
      "==============================\n",
      "질문   : 할머니 께서 계속 해서 옷 을 입 지 않 고 외출 을 하 려고 하 는 행동 은 치매 와 연관 이 있 을까요 ?\n",
      "답변   : 치매 는 뇌 세포 의 기능 적 문제 로 인해 다양 한 인지 영역 에 문제 가 발생 하 는 질환 입니다 . 치매 환자 의 인지 기능 저하\n",
      "예측답변: 치매 는 인지 기능 의 저하 로 인해 발생 하 는 질환 으로 , 치매 의 원인 은 다양 한 원인 으로 인해 발생 하 는 질환 입니다 .\n",
      "==============================\n",
      "질문   : 노인 성 치매 예방 을 위해 어떤 종류 의 운동 을 할 때 주의 해야 할 점 이 있 을까요 ?\n",
      "답변   : 치매 예방 을 위해서 는 적절 한 운동 과 활동 이 필요 합니다 . 걷 기 나 계단 오르 기 와 같 은 신체 적 인 활동 은 뇌\n",
      "예측답변: 치매 예방 을 위해 일상 생활 에서 건강 한 생활 습관 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위해 는 적 인 운동\n",
      "==============================\n",
      "질문   : 운동 이 치매 예방 에 어떤 결과 를 가져올 수 있 는지 알 고 싶 어요 .\n",
      "답변   : 치매 예방 은 치매 에 대한 사회 적 인식 과 예방 활동 을 강화 하 는 것 이 중요 합니다 . 특히 건강 한 식습관 을 가지 고 스트레스\n",
      "예측답변: 치매 예방 을 위해 는 적 인 건강 검진 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위해 는 적 인 운동 과 건강 한\n",
      "==============================\n",
      "질문   : 치매 의 증상 을 예방 하 기 위해 어떤 종류 의 인지 훈련 이 좋 을까요 ?\n",
      "답변   : 치매 는 노인 들 에게 흔히 발생 하 는 질환 이 며 , 예방 이 중요 합니다 . 치매 예방 을 위해 규칙 적 인 운동 과 건강 한\n",
      "예측답변: 치매 예방 을 위해 는 적 인 건강 검진 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위해 는 적 인 운동 과 건강 한\n",
      "==============================\n",
      "질문   : 어머니 를 모시 고 노인 성 치매 검진 을 위해 어떤 검진 을 받 아야 할까요 ?\n",
      "답변   : 현재 치매 의 약물 치료 를 위한 많 은 연구 들 이 진행 되 고 있 습니다 . 알츠하이머병 치료 에 사용 되 는 약물 로 는 콜린 성\n",
      "예측답변: 알츠하이머병 은 현재 까지 근본 적 인 치료법 은 개발 되 는 약물 입니다 . 현재 까지 근본 적 인 치료법 은 개발 되 는 약물 이 개발 되 는\n",
      "==============================\n",
      "질문   : 치매 라는 질환 의 주요 한 증상 은 무엇 인가요 ?\n",
      "답변   : 치매 의 운동 은 다양 한 장점 을 가지 고 있 습니다 . 치매 환자 들 의 삶 의 질 을 개선 하 기 위해 여러 운동 이 사용\n",
      "예측답변: 치매 환자 에게 운동 은 인지 기능 을 향상 시키 고 인지 기능 을 향상 시키 는 데 도움 이 됩니다 . 운동 은 치매 환자 에게 운동\n",
      "==============================\n",
      "질문   : 흡인 성 폐렴 과 같 은 위험 요소 로부터 어르신 을 어떻게 보호 할 수 있 을까요 ?\n",
      "답변   : 알츠하이머병 은 치매 의 한 종류 로 , 기억력 이 점차 약해 지 는 상태 를 의미 합니다 . 알츠하이머병 은 아밀로이드 베타 라는 물질 이 뇌 에 쌓이\n",
      "예측답변: 치매 는 을 위해 는 한 방법 들 이 있 습니다 . 먼저 , 인지 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 예방 을 위해\n",
      "==============================\n",
      "질문   : 치매 예방 을 위해 신경 써야 할 사항 을 알려 주 세요 .\n",
      "답변   : 치매 예방 을 위해 가장 효과 적 으로 할 수 있 는 방법 은 일상 생활 에서 건강 한 습관 을 가지 는 것 입니다 . 치매 예방 을\n",
      "예측답변: 치매 예방 을 위해 는 적 인 활동 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위해 는 적 인 운동 과 건강 한 식습관\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
